A large test suite with 10 test cases was used by P3, however it suffers from a relatively high number of false positives, and uses an inordinate amount of time for a test suite of this size. P15 uses a very well thought out approach to test its tool, validating patches both manually and through the help of the developers of the vulnerable applications. Its test suite is also extremely large, with 8,640 test cases. However, it is not reported how many vulnerabilities were found in each application, nor how large the apps were, which makes it harder to compare with other methods. The test suite used by P4 is one of the largest in this survey, and contains 6 different test cases of varying size. The paper reports an important amount of detected vulnerabilities, however only less than half of these are repaired, albeit with no false positives.

P10, P11, P12, and P13 all relate to the same tool, WAP, with each paper introducing improvements on the previous one. The tool is thoroughly tested with several huge test suites containing both synthetic and natural code, and a large amount of test cases of varying size. In terms of reliability, the only aspect that may be commented on, is the suspiciously low amount of false positives considering the amount of false positives in the real world, which is a trend we observe throughout the surveyed papers. It is interesting to note that most test suites in this survey are ad-hoc and with natural code: about 70% of test cases are made of open source web applications, and only two papers use formal test suites. Barely 8% of test cases used synthetic code, and most in the form of purposefully vulnerable web applications.

# 4 RQ3
None of the 20 test cases contained within the 5 Java test suites used in the surveyed papers were found, most links having expired and surviving versions of programs being different from the ones used. P15’s 8,640 apps were not found either. Hence, we only analyse the two systematic test suites.

Juliet and VAKB are strikingly different (see Table 4): VAKB dwarfs Juliet in terms of total number of LOC, but contains much less test cases, with 164 of them even being unavailable. The size difference between the smallest and largest test case is also very different: Juliet’s largest test case is about 84 times larger than its smallest, while VABK’s largest test case is more than 5,800 times larger than its smallest. This is explained by the fact that Juliet is entirely composed of synthetic code, while VAKB is entirely composed of natural code. Yet, they complete each-other in their coverage of OWASP Top 10 categories, and being of different types of code, are intended for different types of testing loads. It should be noted that we could not find the repository or fix commit for 164 test cases in VAKB.

"Unreported" indicates that a metric has not been reported by the paper, and "N/A" means that the paper has not tested its approach. The "False positives", when not reported, were calculated by subtracting the number of true positives from the number of generated patches.

The number of LOC per OWASP category for each test suite seems to lack test cases for the other OWASP Top 10 categories. Figure 2 emphasises the massive difference in size between the test suites, but also indicates that they have a very different focus, with Juliet being more focused on A1, and VAKB on A5. It should be noted that A9 does not have any corresponding CWEs, and is hence omitted.

This is further reflected in the number of test cases per OWASP category (Figure 3), where Juliet also seems to emphasize A7. However, it also shows that Juliet heavily emphasises A1 and A7, and VAKB seems to have a better coverage of each OWASP category individually.

# 5 DISCUSSION
# 5 Comparison with related work
Shafiq et al. effectuated a survey like us, although we restricted our scope to APR of vulnerabilities, and to source code modification. This increased focus allowed us to identify 19 approaches that Shafiq et al. had not classified, despite 18 of them being published before their literature review, and being inside their acceptance criteria. Thus, all but one approach we surveyed had not been analyzed or categorised by Shafiq et al. We also performed a deeper analysis of each approach, as we looked at their performance in the context of their test suite, which we also analyzed.

The approach of King et al. is fundamentally different from our approach, as they focused on benchmarking the tools they surveyed on a common test suite, while we compared tools by analyzing their reported performance and test suite. Our approach allowed for more tools to be analysed, with the additional analysis of test suites giving greater insights into how future APR tools should be tested. A good basis of test suites is present in the form of Juliet and VAKB, which could be extended to cover the missing OWASP Top 10 categories. Regardless of the test suite used, it should be easily and durably available for other researchers to analyse and use, as most test suites found in the benchmarking efforts of Durieux et al. gave interesting insight into the potential widespread test suite overfitting in APR, and possible mitigation strategies. Our results seem to indicate a similar tendency in APR of vulnerabilities, seeing that almost all of the test suites used to assess the performance of approaches we analyzed were of low quality and arbitrarily chosen, which might be an indication of test suite overfitting.

# 6 CONCLUSION AND FUTURE WORK
This survey reveals a very uneven coverage of OWASP Top 10 in APR, with categories such as A1 and A7 being the subject of multiple papers, while other categories are not even mentioned. There is room for improvement in this respect, and future work should probably focus on other categories than A1 or A7. It is also interesting to note that many papers rejected from this survey implemented runtime APR, i.e. they did not modify source code. Our work shows that few researchers adequately test their approaches, which corroborates previous research that pointed to a lack of standardised performance assessment. A standard performance measure approach and a well-designed standardised test suite would greatly facilitate comparisons between tools..