To more-in-depth understand the specific app market information that could drive the classification, we computed the information gain  for each of the features in the Market metrics group and ranked these features based on their scores. In Table 5, the top 15 ranked features, along with the related information gain scores are reported. Looking at Table 5, we can observe that the app size is the feature with the highest score. Besides, permissions-related aspects such as (i) the number of permission, and (ii) the presence of specific permissions (e.g. mostly related to storage and networking operations) required by the app represent relevant information for deciding the vulnerability-proneness level of an app. Finally, as further confirmation of what we found in RQ 1 (Section 4) and RQ 2 (Section 4), both the Category of an app and its popularity indicators (i.e., Raters and ...

# Empir Software Eng (2021) 26: 78
Installs) provide valuable information for identifying the risk levels. Since the Size feature represents the best predictor, the resulting model might be biased towards this metric (i.e., the predictions of vulnerability-proneness levels could be mainly based on this feature). Thus, to estimate the extent to which the performance of the ML models is driven by the Size metric, we repeated the ML experiments (in which ML algorithms are fed with Market metrics) by not considering this feature and observed degradations in precision, recall, and F-measure values lower than 5%. This allows us to conclude that the eventual bias introduced by the Size metric is only marginal.

# 5 Threats to Validity
Threats to construct validity concern the relationship between theory and observation. The most important threat that could affect the results of our study is related to possible imprecision/incompleteness in identifying the vulnerabilities and, thus, the vulnerability-proneness levels of apps. It is worth noticing that we rely on a state-of-the-art tool (i.e., AndroBugs) for identifying well-known security flaws. As this is a static analysis tool, it may yield false positives. In addition, in our RQ2 we used the average rating as an indicator of the success of an app. However, user ratings can be subjective and imprecise. To alleviate such an issue, we (i) only considered apps rated by a reliable number of users (i.e., Raters > 500), and (ii) complemented the analysis by also considering the number of installs as an alternative way.

to measure app success. We believe that, with a smaller number of ratings, there is a higher risk that our results may depend on the subjectiveness of the ratings. Indeed, considering only a few tens of ratings, extremely positive or negative ratings given by certain groups of users may have too much impact on the average score . For this reason, the average rating value of an app that is rated by at least 500 users is more likely to take into account the opinions of more heterogeneous users and, thus, better reflects the actual rating. Our study does not consider the temporal dimension associated with the user rating and the number of installs, and this could represent a threat that can affect the validity of our findings. Unfortunately, Google Play does not allow extracting the user ratings and number of installs referring to a specific app version, and only average or cumulative counts are provided. However, prior research  demonstrated that average user rating is quite resilient to version-rating changes, especially for apps having higher numbers of raters. Finally, our measurements on vulnerability proneness are based on the assumption that users need to be aware of the security level of all apps. However, the usage of specific vulnerability-prone apps could be unavoidable for users when real alternatives are not available to replace popular apps (e.g., Facebook, Whatsapp, etc.). This means that it is unclear weather users having more information about the level of vulnerabilities of popular apps can push them to not install or use them. This is something we plan to investigate for future work.

# Threats to conclusion validity
Threats to conclusion validity concern the relationship between treatment and outcome. Appropriate, non-parametric statistical procedures have been adopted to draw our conclusions, since the variables of interest were not well-modeled by normal distributions (as verified through the Shapiro-Wilk normality test (Shapiro and Wilk 1965)). More specifically, we used the Kruskal-Wallis test and post hoc Mann-Whitney pairwise comparisons for investigating the statistically significant differences. Moreover, the magnitude of the observed differences is quantified by using Cliff’s delta effect size measure. For coping with multiple comparisons, the Holm’s correction procedure has been adopted to adjust p-values.

# Threats to internal validity
Threats to internal validity concern factors that can affect our results. A possible source of bias might be related to the thresholds we used when analyzing the data and presenting our results. In particular, to answer our RQ2, we clustered the apps into four levels of rating and three levels of installs. Similarly, to answer our RQ3, we grouped the apps into two vulnerability-proneness level groups. The thresholds to define the average rating categories and the vulnerability-proneness categories were based on the descriptive statistics indicators (i.e., quartiles) of the related distributions for the 1,002 considered apps, while the apps have been assigned to the different levels of installs based on the downloads category assigned by the Google Play store (see Section 3). Different choices might lead to different results. Nevertheless, we obtained similar results when considering different (i) rating —e.g. low, encompassing the apps with an average rating lower than the first quartile, medium comprising the apps with an average rating value between the first and the third quartile, and high, consisting of the apps with an average rating higher than the third quartile– and (ii) downloads groups. To verify weather the adopted independent features (or variables) could correlate among themselves we computed the Kendall correlation between all the pairs of (numeric) features considered in our study. As result, no correlations with either large or medium effect size are observed (τB ≤ 0 for all the pairs of features).

# Threats to external validity
Threats to external validity concern the generalization of the findings. Our analysis involves about 1,000 Android apps sampled from a dataset not specifically designed for security purposes. Thus, it is unclear if our results may generalize to further apps or apps developed for other mobile platforms. However, as discussed in Section 3, our data collection is a
statistically significant sample of a dataset consisting of about 10,000 apps. While our results are in line with previous investigations demonstrating that also very popular apps suffer from insecure communication vulnerabilities (Gajrani et al. 2020; Papageorgiou et al. 2018; Taylor and Martinovic 2017b), it is worth pointing out that some of these vulnerabilities could be non-exploitable (i.e., appearing in dead or legacy code), or present in third-party libraries . To estimate the trustworthiness of our results, as well as the actual exploitability of the signaled vulnerabilities, we randomly selected 20 apps in our dataset. We reverse-engineered (by using dex2jar9) such apps and inspected their source code to understand whether the vulnerable code portions reported by AndroBugs and marked as Critical were actually reachable. As a result of this analysis, we observed that less than 5% of the signaled critical vulnerabilities appeared in (statically detectable) unreachable code. Besides, we performed a dynamic analysis on the same 20 selected apps, following the approach used in previous work (Darvish and Husain 2018). Indeed, to conduct such an analysis, we used the Charles Proxy10 and tried to perform man-in-the-middle (MITM) attacks, by testing the SSL certificate validation, SSL certificate checking, and that all important URLs are SSL protected. Specifically, we opened each target application and tried different functionalities of the application while intercepting the packets on the Charles proxy. The attack succeeded if we were able to read (or decode) the intercepted packets. Despite for all the 20 analyzed apps AndroBugs reported issues with SSL certificates, for four of them (i.e., 20%), we were unable to trigger the attack. Furthermore, we only considered free apps and it is unclear if our conclusions are also valid for paid apps, as more rigorous processes for avoiding security flaws could be adopted when developing paid apps. To further increase the generalizability and reliability of our results, in the future, we plan to replicate our study at a larger scale considering apps from other datasets  and investigating if our findings are still valid when considering paid apps.

# 6 Conclusions and Future Work
Users typically share sensitive information to use mobile apps for accomplishing a lot of everyday life activities. However, recent research demonstrated that the majority of these apps suffer from critical security defects.