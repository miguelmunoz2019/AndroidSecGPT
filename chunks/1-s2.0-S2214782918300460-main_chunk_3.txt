# Internet Interventions 15 (2019) 110–115
Unacceptable scores. Slightly over half of the apps reviewed had no privacy policy at all. Of the apps that did have a policy, they were often only provided after users were asked for information, meaning the apps had collected data before alerting users how that data could be used. The availability of privacy policies varied depending on the type of data collected by the app; apps collecting identifiable data were more likely to have a privacy policy than apps collecting non-identifiable data. This indicates an awareness of the importance of privacy among developers of apps soliciting personal information. Yet, not all apps collecting identifiable data provided policies, and those that did, did not disclose all aspects deemed relevant within our checklist. The availability of privacy policies also varied by platform; apps from iTunes were more likely than Google Play to have a policy, which is likely a reflection of different requirements in app stores. Our results parallel the findings of Rosenfeld et al. (2017), who found that two thirds of apps for dementia included in their review did not have a privacy policy. Seeing this pattern mirrored in our review of depression apps suggests that this may be a recurring pattern in mental health apps as a whole.

Among the privacy policies we did find, many policies were vague and lacked important information, such as details on encryption of data, password protection, and the ability to edit or delete entered information. In addition to being vague, many privacy policies are convoluted. Das et al.' (2018) conducted a readability analysis of privacy policies and determined most are not comprehensible to the general population. This mirrors previous findings that most app privacy policies require college-level literacy . This lack of clarity might limit people's ability to understand the content of privacy policies.

Improving data security standards is not only in the interest of clinicians and consumers, but has a commercial advantage too. Uptake of apps will likely increase if users are more confident that their entered information is secure. Indeed, clinicians report they would use and recommend apps if privacy and security issues could be overcome . Higher standards and increased regulation might improve clinician confidence in such products, thus increasing their comfort in recommending such tools. In Schueller et al.'s (2018) study of consumer interest in mental health apps, 74% (N = 602) of survey respondents said that encryption of data was important or very important to them, and 70% (N = 572) rated the availability of a privacy policy as important or very important. However, only 10% (N = 87) said that privacy and data security concerns would prevent them from using or downloading an app. In relation to the findings from this study, consumers have few available choices of apps with adequate disclosure and quality of their data security and privacy practices, thus making it challenging to incorporate this into decision making.

# 4. Limitations
Our study has several limitations. We excluded apps that did not appear to collect user information. However, we cannot be certain that these apps do not collect background data or information such as location. If so, the data security and privacy policies of these apps should be evaluated with similar rigor. Additionally, while we reviewed app policies, we did not audit data handling practices. Thus, we cannot ascertain that apps which have policies deemed “acceptable” are actually following the practices they outline. It is possible, therefore, that our findings overestimate the quality of privacy and data security of these apps. This is worrisome given that our potentially “best case” scenario was still quite grim. A recent paper which conducted static and dynamic analyses of mobile health apps privacy and data security found that few followed well-established practices and guidelines . In addition, a previously established app certification group, Happtique, faced challenges and shut down after several of the apps that it certified as having acceptable level of privacy and security were demonstrated to not be secure by a group of hackers. Lastly, while we evaluated the presence or absence of key pieces of information in the privacy policies we reviewed, we did not evaluate the comprehensibility of policies. Das et al. (2018) explored this within youth-focused apps, and found policies had high literacy demands. Based on our experience, we suspect evaluating the reading comprehension level required to understand privacy policies within apps designed for adults would produce similar findings.

# 4. Future directions
It is likely that this pattern of lack of transparency around data handling is repeated with mobile apps targeting other mental health conditions. Future research could review the policies of other mental health apps to obtain a broader picture of the state of the field of mental health apps. It is also worth noting that our study used an independent rating tool for privacy policies that draws from consensus from the research literature and expert opinions. Another approach would be to evaluate assessments or privacy policies from potential end users, for example clinicians or patients. One could even directly compare the adoption of apps on the basis of those that include privacy policies or not, or with privacy policies of varying quality. The rating tool itself could also be evaluated based on clinician or patient feedback to determine if it reflects the concerns of these key stakeholder groups. These directions would help align these findings with the needs and interests of those who would use mental health apps in their practice and/or lives.

Formal regulation of data handling procedures within apps will likely remain lax. Many of the mental health apps included in our review would fall within the subset of which the FDA has decided to exercise enforcement discretion. Thus, we strongly encourage developers, and potentially app stores, to raise their standards. If people are to trust apps with their mental health information and hope that those apps might be useful for them, they should have confidence that their information is going to be used in ways that protects their safety, security, and privacy. Recent data security breaches have gained considerable media attention, such as Cambridge Analytica obtaining and misusing the private information of more than 50 million Facebook users, bringing issues of data privacy and security into public conscience. This may result in skepticism or hesitation around the use of digital health tools which collect personal information. The onus is on developers and app stores to be clear about the extent, and limitations, of privacy protections in order to increase public confidence in using tools. There may also be a role for third-party reviewers to help raise the bar.

The current paper focuses on understanding the existing state of privacy policies within mental health apps for depression, but stops short of exploring what would be the desired state of privacy policies for those apps. Such an exploration would require further input from stakeholders as previously noted not just in comprehensiveness and sufficiency of information, but also in terms of its capabilities to be easily read and understood. It would also be worth considering from a regulatory perspective what is required of developers. This is an evolving landscape with new regulations emerging, such as the General Data Protection Regulation (GDPR), which impacts what data can be collected and how that collection needs to be disclosed. Nevertheless, future work could be more aspirational to further help developers determine how to create effective practices for data security and privacy and effectively convey those practices to end users.

# 5. Conclusion
Currently app developers are provided considerable latitude in their data security and privacy practices within health apps and how they explain these practices to users. Some developers are acting responsibly; for example, providing this information to consumers prior to obtaining any user information. However, as we found in the case of
# K. O'Loughlin et al.