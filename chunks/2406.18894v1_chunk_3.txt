- GPT 3 (gpt-35-turbo version Nov. 2023) : It is a powerful language model that has been pre-trained on a large corpus of text data, including code. It has demonstrated performance in various natural language processing (NLP) tasks and has been used for code analysis tasks such as code completion, code search, and code summarization.

- GPT 4 , : It is the newest version of GPT being pre-trained on an even larger corpus of text data, including code. It has demonstrated improved performance over GPT 3 in various NLP tasks and has been used for code analysis, including code review and repair.

# Vasileios Kouliaridis, Georgios Karopoulos, and Georgios Kambourakis
# Current Models Overview
- GPT 4 Turbo (gpt-4-1106): It is a variant of GPT 4, been specifically designed for tasks that require faster inference times, such as code analysis. It has been pre-trained on the same large corpus of text data as GPT 4, optimized for faster performance.

- Llama 2 (Llama-2-70b-chat) : This LLM has been pre-trained on a diverse set of text data, including code. It has demonstrated performance in various NLP tasks, also been exploited for code analysis, including code summarization and code search.

- Zephyr Alpha (zephyr-7b-alpha) : It is pre-trained on a huge corpus of text data from diverse sources, including books, articles, and websites. This model has been fine-tuned with a mix of publicly available and synthetic datasets on top of Mistral LLM. Despite its small size (7B parameters), it potentially shows a performance comparable to several models with a number of parameters in the range of 20-30B.

- Zephyr Beta (zephyr-7b-beta) : This model has been fine-tuned with a mix of publicly available and synthetic datasets on top of Mistral LLM. It is the successor of Zephyr Alpha, therefore considered significantly more powerful than its predecessor. Based on its documentation, it is fast and competent, showing a performance comparable to the best open-source models, having around 70B parameters.

- Nous Hermes Mixtral (nous-hermes-2-mixtral-8x7b-dpo) : It is one of the most powerful open-source models available, comprising a fine-tuned version of Mixtral base model.

- MistralOrca (mistral-7b-openorca) , , : It has been fine-tuned with Open-Orca datasets on top of Mistral LLM. Despite its small size, it outperforms Llama 2 13B, showing a performance comparable to several models with a number of parameters in the range of 20-30B.

- Code Llama : It is a special version of Llama 2, tailored specifically for coding applications. This specialized version has been refined through extensive additional training on code-focused data, with prolonged exposure to relevant datasets. The result is a tool with alleged superior coding proficiency that builds upon the foundation of Llama 2. More specifically, Code Llama can generate code and create explanations about code in response to prompts in both programming and natural language. Its capabilities extend to assisting with code completion and troubleshooting code errors. Furthermore, Code Llama is versatile, supporting a broad array of widely-used programming languages, including Python, C++, Java, PHP, JavaScript, C Sharp, and Bash. In this work, we examine the smallest pre-trained model, namely, the 7B version. In addition, for this LLM, in a separate run, we employed LlamaIndex  to improve the detection capabilities of Code Llama. LlamaIndex is a data framework for LLM-based applications, enhancing them with additional contextual data. This context augmentation technique is called Retrieval-Augmented Generation (RAG) and can be used to address the restrictions of LLMs by giving them access to contextual, current data. For the RAG process, we used the 50% of Vulcorpus, i.e., only the samples that contain code comments regarding the specific vulnerability. Android’s
# Title Suppressed Due to Excessive Length
# 3 Evaluation process
All nine pre-trained LLMs listed in subsection 3, except Code Llama, run on the GPT@JRC platform, a system developed by the European Commission’s Joint Research Centre (JRC). Code Llama was run on a local computer with an M2 processor and 16GB unified memory. Each LLM was fed with Vulcorpus for comparing its performance on identifying potential vulnerabilities and proposing code improvements. To this end, as detailed in Section 4, we use a simple scoring system to present (a) the number of vulnerabilities each LLM was able to detect, and (b) if the LLM proposed valid suggestions for possibly fixing the vulnerability. Both these partial scores have a maximum value of 10/10 per vulnerability category, i.e., one point for each piece of vulnerable code the LLM was able to detect and annotate. It is important to note that the input or question given to each LLM has a major effect on its output. For our study, each LLM was queried as follows: “Check if there are any security issues in the following code; if there are, explain the issue”.

As previously mentioned, the LLMs used in this work are pre-trained. This means that the associated libraries, possibly needed by each code sample but not included in the input, cannot be analyzed. This mostly affects the analysis regarding the M2 vulnerability. Therefore, to evaluate LLMs against M2, instead of Java code, we used 10 libraries with known vulnerabilities as input. These libraries, also included in Vulcorpus for reasons of reproducibility, were published before the training date of each LLM.

At a final stage, as detailed in Section 4, the results of each LLM were compared and crosschecked against those produced by two well-known SAST tools, namely Bearer  and MobSFscan . Bearer is a static application security testing tool, which uses built-in rules covering the OWASP Top 10 and Common Weakness Enumeration (CWE) Top 25. MobSFscan is a static analysis tool that uses MobSF’s  security rules and can find insecure code patterns in Android or iOS source code. Finally, we also assessed the performance of each LLM in detecting privacy-invasive behaviors, using the three samples detailed in subsection 3. The output was rated using three categories: not privacy-invasive, (b) potentially privacy-invasive, and (c) privacy-invasive.

# 4 Results
Tables 1 and 2 recapitulate the results for each LLM. Particularly, each line of Table 1 indicates if the specific model detected the vulnerability (denoted with the letter “D”), and if it explained the situation and provided a valid solution for improving the code (denoted with the letter “I”). Actually, the “I” aspect is
# Vasileios Kouliaridis, Georgios Karopoulos, and Georgios Kambourakis
The letters “D” and “I” stand for the number of vulnerable samples detected and the number of vulnerable samples for which the LLM suggested improvements, respectively. Top scores per vulnerability are in boldface. The asterisk exhibitor stands for Code Llama without RAG.

N: not privacy-invasive, P: potentially privacy-invasive, Y: privacy-invasive.

# Title Suppressed Due to Excessive Length
a key factor in evaluating each LLM (also against each other), as this is the sole indicator of whether the LLM actually “perceives” the security issue.

Overall, with reference to Table 1, the best performers in terms of total vulnerabilities detected, are Code Llama (81/100), GPT 4 (67/100), Nous Hermes Mixtral (62/100), Zephyr Beta (54/100), and Zephyr Alpha (53/100), followed by GPT 4 TURBO (50/100), GPT 3 (42/100), MistralOrca (37/100), and Llama 2 (30/100). On the other hand, the best performers, in terms of total code improvement suggestions, are GPT 4 (83/90), GPT 4 Turbo (66/90), Zephyr Alpha (58/90), Zephyr Beta (56/90), and Nous Hermes Mixtral (56/90), followed by Code Llama (44/90), MistralOrca (38/90), GPT 3 (37/90), and Llama 2 (31/90). Overall, GPT 4 poses as the top performer, considering a composite score of high “D” and high “I”. On the other hand, LLMs like Code Llama, which do identify the correct vulnerability, but fail to provide corrections or suggestions regarding the problematic lines of code may indicate an insufficiently trained model for this type of analysis.

When looking at each vulnerability individually, GPT 4 achieved a perfect score for M1 and M6, MistralOrca for M9, Zephyr alpha for M5 and M10, Zephyr beta for M5 and M9, and Llama 2 and Code Llama for M5. Regarding the rest of the vulnerabilities, namely, M2, M3, M4, M7, and M8, the best performers were GPT 3 (7/10), Zephyr Beta and Code Llama (9/10), Nous Hermes Mixtral (9/10), Code Llama (9/10), and Nous Hermes Mixtral and Code Llama (9/10), respectively. Concerning M2, recall from subsection 3 that it was tested using 10 vulnerable libraries published before the training date of each LLM. Even so, the M2 low detection performance in Table 1 for all the LLMs but GPT-3 may designate that these libraries were not considered during LLM training, so the respective scores can be regarded only as indicative. The same applies to the “I” score for M2, which it is marked as N/A. As discussed in subsection 3, to address these limitations, LLMs used for vulnerability detection can capitalize on context augmentation; this way the LLM is provided with access to contextual, up-to-date data.