One of the scenarios for which this work has been developed is the SBOM, as explained in Section 1. This concept has been well explained by Zahan et al. , where they discuss the importance of SBOM in improving cybersecurity, highlighting its benefits and challenges. The work has been based on the Log4Shell vulnerability: a zero-day remote code execution vulnerability discovered in Apache Log4j that significantly impacted software organizations in December 2021 despite a few months earlier, EO (Executive Order) 14028  recognized SBOMs as a practice for enhancing software supply chain security, and NTIA released a report of minimum points to use SBOM in risk reduction. After the Log4Shell vulnerability, NIST recognized SBOM as one of the official practices organizations must follow to reduce cyber attacks and listed it in the first version of the Secure Software Development Framework. Moreover, the industries following SBOMs were able to identify the Log4Shell vulnerability rapidly and had a more effective response.

In the work, Zahan et al. highlight the benefits of using SBOMs, which include risk management, vulnerability detection, supply chain transparency, proactive management of security risks, and effective response to cyber threats. As part of EO 14028, SBOMs can improve the nation’s cybersecurity, but it still has to be standardized in the industry, and some challenges still must be solved, such as the standardization of data requirements, the enhancement of solid guidelines and the practitioners’ collaboration.

# Risk Algorithm Native Code Vulnerabilities Android
# 4 Methodology and Implementation Details
This section illustrates the methodology used for vulnerability detection in the Native Code of Android applications. First, we describe the creation of the purpose-specific database. Then, we detail the library extraction algorithm we developed to analyze the information from each APK studied. Lastly, the risk assessment algorithm is fully explained.

Given an APK, we extract the native library from its lib directory. As the compiled library is an ELF file, we need specific reverse engineering tools and techniques to extract data for vulnerability detection. Such data is the list of functions and the product name, along with the version to which the library belongs. Once we have this information, we can match this result with a database of known vulnerabilities. The database is purpose-specific, containing for each CVE a field with the affected vulnerable version and function. At the moment of this writing, there is no publicly available database for this purpose, and with this specific structure, a system can easily access and read it.

We need a list of N products to construct the database, which are those whose vulnerabilities we want to look for in the apps. The product matching process is a whitelist approach that assumes that none of the Android app’s developers has changed the library name (keeping the real one employed during compilation time in the NDK). Once we have a match between the library under analysis and the database, we can assign a risk assessment score to the CVE found in the analyzed library. Our purpose is to give a risk value to each library (according to the risk of each CVE found) and, consequently, to each app to provide an alarm to developers and security researchers. The following sections will explain in more detail the methodology.

# 4 Building a custom CVE Database
A CVE database is essential to check if the extracted data from the analyzed library have been declared in a published vulnerability listing. We decided to employ a custom database to reduce the query time to a public online database. Hence, our local database is a dump of data contained in the online selected website of CVEs (e.g., NVD) and with fields organized according to the aim of the research. As explained in Section 2, the essential information about a vulnerability is the product’s name with the version and the vulnerable functions of the library. All this information may be found inside the human-readable description. It does not follow a specific syntax such as "In product P of version v.n there is a vulnerable function F.", so we need to employ Natural Language Processing (NLP) techniques to process the description and extract the valuable data. Specifically, we employed Python nltk library5, adapting it to our use case and syntax.

The product name is easily matched with one of the N selected products, and if one is mentioned inside the description, we know that the CVE has been.

5
F. Author et al.

found in that specific product. This can be further confirmed by searching all vulnerabilities by product name in the public database. The version of the library affected by the CVE is always a number that may come after the word version, the product name, or preceding words with the meaning of after or before. Some examples are described in Table 1: (i) if there is a word with the meaning of before (i.e., before, prior, earlier, etc.), every product version lower than the one found in the description is vulnerable; (ii) if there is a word with the meaning of after (i.e., after, following, successive) every product version higher than the one found in the description is vulnerable; (iii) if no word with the precedents meaning is found, the affected product version is only the one found in the description.

The last data to extract from the description is the function name. We rely our strategy on how programmers typically recognize function names and give names to functions. Then, by looking at some CVE descriptions, we noticed that the function is never declared as, for example, function F, represented uniquely by its name. The function name usually contains specific symbols (i.e. _, ::, (), etc.). Moreover, if the name is made up of more than one word, it is camel-cased. As an example, makeSum is made up of two words: make and Sum. In other cases, the function is not found, which means that the whole product version is vulnerable, but no description of the vulnerable function is provided. This methodology was developed in iterative steps with manual cross-validation to check if the algorithm worked correctly.

# 4 Library Extraction, Analysis and Association
The overall approach, subdivided into library extraction, analysis, and association, is depicted in Figure 1. The presented methodology can be used as it is by vulnerability researchers and adapted for developers to release a secure Android application. Library Extraction is the first step of the analysis part, and, as the name states, it consists of the extraction of the compiled library (ELF file) from the
# Risk Algorithm Native Code Vulnerabilities Android
# Libraries.so files
# Android APK extraction Analysis
# Products
# Library Products linked to APK
Android application. It is done by unzipping .so files from the lib directory of each APK. Indeed, libraries are compiled according to different ABIs and saved inside the application. We can choose to extract libraries only for specific architectures or to analyze libraries for all available ABIs. In this work, we considered all ELF files from armeabi-v7a directory, and if not available, looked for arm64-v8a or x86_64 as they represent the most popular architectures.

Library Analysis is the part where we need to extract from the library the data for vulnerability detection. In particular, we need to know the list of functions in the library and the product name and the version to which the library belongs. This step can be done with different reverse-engineering tools, such as Ghidra and its Python extension for automation. Typically, the product name and its version can be retrieved in the strings section (.rodata section), while the defined functions in the ELF file can be retrieved from the Symbol Table section. In this work, we used pwntools , a popular Python framework for binary analysis and exploitation. Specifically, we employed its ELF module, which allows the analysis of ELF files from which the strings and the list of functions are extracted. The version is taken from the result of the strings Linux utility. At the same time, the functions are found inside the Symbol Table of the ELF without considering .got and .plt sections. A difficulty comes when binaries are stripped where not all function names are available, or the names are unrecognizable.

Library Association is the part where each ELF file is associated with at least one of the selected N products. This is a crucial task: if we do not link each ELF file to its related product, we cannot determine if the ELF file contains vulnerabilities, hence assigning a risk assessment score. Section 3 illustrates that different works used binary similarities techniques or Machine Learning algorithms; however, we decided to apply a simple identification algorithm because we know that every product uses a clear and unique syntax in strings and functions.

6 strings tool in Linux is capable of retrieving all printable sequences of characters.

7 A stripped binary is a binary without some debugging symbols and so with a lack from the .rodata section of an ELF.