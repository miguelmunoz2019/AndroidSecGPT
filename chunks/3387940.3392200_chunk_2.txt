# 3 Research Questions
The following research questions were formulated to address these issues:
- RQ1: How can the automatic repair methods reported in the literature be classified according to OWASP Top 10?
- RQ2: How reliable is each method based on their associated test suite and reported performance?
- RQ3: How well-suited are the Java test suites for testing automatic program repair tools?
# 3 Selection of papers and classification
Based on the inclusion criteria mentioned above, we will select papers by searching through six databases or search engines: Springer Link , Oria , IEEE Xplore , ACM Digital Library , Engineering Village , and Google Scholar . These were chosen based on their prevalence in existing surveys and their accessibility. For each database, we will use 120 search queries crafted from the names of OWASP Top 10 vulnerabilities. For each search, we will only examine the first 100 results, as some search engines produce several thousand results, which are often not relevant to the search itself. This means that we expect the maximum amount of results to be 72,000. Due to this amount, each author will go through half of the databases. For each result, we will manually assess its relevance by reading its title and abstract. Then we will read in their entirety results found to be relevant in the previous step. Finally, results passing the previous step will be cross-examined by both authors. The classification according to OWASP Top 10 will be based on the claimed coverage of the analysed publications.

# 3 Reported performance and reliability analysis
The metrics used in the reported performance of APR tools may vary greatly from one publication to another. However, drawing
# 3 Test suite analysis
For each Java test suite, we will use the following metrics based on the observations made in :
- LOC: the total number of Lines of Code in a test suite, which affects a tool’s time performance and capacity to handle significant loads;
- Number of test cases: ideally, a test suite should contain many test cases, in order to present the most diverse testing environment;
- Size difference of test cases: test suites should have test cases of varying size;
- Type of code: if a test case consists of natural code (i.e. the test case comes from a real full-sized program), it will present a more realistic setting, but it will be more difficult to assess how many vulnerabilities are present in it, and their details (which vulnerability class, what the most appropriate patch is...). This makes assessing a tool’s correctness and completeness more difficult. Conversely, if a test case consists of synthetic code (i.e. the test case was written for the express purpose of being used in a test suite), all details about the vulnerabilities present in the test case will be known. However, the setting will be less realistic, and such test cases also tend to be smaller. An ideal test suite would contain a combination of both types of code;
- Availability: the test suite should be easily and openly available for researchers, in order to promote usage and peer-review;
- Vulnerability classes: the test suite should cover most possible classes of vulnerabilities. In this analysis we will look at which OWASP Top 10 classes each test suite covers;
# 4 RESEARCH RESULTS
# 4 RQ1
The results of our search are presented in Table 1, which shows the number of results for each database or search engine. The total number of results amounts to about half of the expected number of results.

From these 38,095 results, 27 were admitted to the cross-examination phase. During this phase, we found one duplicate, one publication that relied on user interaction to repair its target, and five papers that implemented so-called runtime approaches which do not modify the source code. All these publications were removed, which leaves 20 publications that were inside the scope we had defined. While we unfortunately did not take note of which papers were rejected before the cross-examination, they were largely similar both in nature and in proportion as the papers rejected in the cross-examination phase. These are summarised and classified according to the OWASP Top 10 categories they cover and their
target language in Table 2. What is immediately noticeable is that there are only two target languages, namely Java and PHP, despite the omnipresence of JavaScript in web development. The OWASP Top 10 coverage is summarised in Figure 1, which underscores the strikingly sparse coverage of OWASP Top 10 categories. The pre-dominant categories are very clearly A1 - Injection and A7 - XSS, with 14 and 10 approaches targeting them respectively, followed by A3 - Sensitive Data Exposure with 6 approaches. The other OWASP Top 10 categories are scarcely evaluated, and half of them are not evaluated at all. This may be in part due to the fact that most of the targeted vulnerabilities can be solved with proper input sanitization, which is a repair that is heavily standardised and can often be implemented by inserting a single line of code, and is hence easier to automatize.

# 4 RQ2
The reported performances of the surveyed papers are summarised in Table 3. P8 presented two distinct approaches, which were labeled P8-1 and P8-2. Some papers (such as P11) tested their approach on distinct test suites with different metrics, hence their reported performance is summarised individually for each test suite. We may immediately see that two papers (P19 and P20) did not test their approach, making any analysis of their effectiveness very difficult. Also difficult to analyse are P17 and P14, due to their lack of reported performance metrics, or vagueness in their report ("All" patches were true positives in the case of P17, which, without the amount of generated patches, does not provide much information). P2 suffers from a very small test suite of only 250 estimated LOC, made solely for the purpose of testing this approach, which increases the likelihood of test suite overfitting. It is difficult to determine its aptitude to repair vulnerabilities outside this test suite, a weakness recognised by the authors. The same problem is present in the case of P8-1, with a test suite of only 488 LOC.

Although its test suite is better than P8-1, and its number of generated patches is good, P8-2 does not report whether any of these patches were correct, which weakens its reliability. While having a test suite much larger than P8-1, P1 still suffers from the fact that its test suite is composed of a single test case, which is not enough to emulate the varied environment that is real-world vulnerability repair, and increases the probability of test suite overfitting. P9 presents the same problem, with the added issue of not testing whether the patches break any functionality, although its performance results remain impressive. P18 uses the same general technique (i.e. generating PreparedStatements) as P9, but is tested on a larger test suite composed of 9 test cases, which would indicate a greater reliability. Its biggest weakness is its omission of the time metric. Also generating PreparedStatements, P16 has been tested on an even larger test suite, but containing only 4 test cases. Interestingly, while the test suite is larger, the amount of vulnerabilities detected and patched is much lower in P16 than in P18 or P19, suggesting either that P16 is not as proficient, or that the test suites used by the two aforementioned papers were exceptionally vulnerable.

P5 uses a decently-sized test suite with 5 test cases of varying size, however the approach is non-deterministic and its success rate is not reported. Additionally, the time metric is not reported despite the use of a Genetic Algorithm, a method that may potentially take a substantial amount of time. It is uncertain then whether the reported performance would hold in a different setting. The same test suite as P5 is used by P6, which presents a deterministic method with a slightly better performance, although with a high number of detected, but not repaired, vulnerabilities. P7 uses a similar test suite as P6, but it is difficult to assess its performance due to the use of number of vulnerable files instead of number of vulnerabilities, which makes the report more imprecise.