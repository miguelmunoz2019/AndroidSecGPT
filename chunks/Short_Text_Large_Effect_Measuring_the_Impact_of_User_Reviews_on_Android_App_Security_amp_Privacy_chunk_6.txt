# B. Building the Models
To have a quality model, we need to only include variables that are necessary and can account for as much of the variance in the empirical data as possible. We start with a base model without any independent variables and then subsequently extend it with more predictors. Table III presents the goodness of fit for the relevant steps in building the corresponding models. Since the dependent variable of our analysis is binary—either an app update is SPU or non-SPU—we use logistic regression.

Moreover, to verify that a mixed model suits our data better than a simple base model, we tested the base model without any independent variables against the mixed models. The result is that mixed models fit our data significantly better. In particular, we extend the base model as follows:
- Start with base model with a random effect to account for effects from updates of the same app
- Include variables at user level: SPR ratio, average score
- Include variables at app level: permission mechanism, app category, developer’s reply ratio
- Include interaction between SPR ratio and average score
In each step, we calculated the model fit and used log likelihood model fit comparison to check whether the later model fits our data significantly better than the previous one. For the final model, we chose the one with the best fit that was significantly better in explaining our data than the previous. This is a well established approach for model selection , , ,.

We compared all models according to their corresponding Akaike information criterion (AIC), see Table III, which estimates the relative quality of statistical models for a given set of data. Smaller AIC scores indicate a better fit. Moreover, we also used likelihood-ratio tests, which are evaluated using Chi-squared distribution, to compare the models.

**TABLE III GOODNESS OF FIT FOR THE MODELS PREDICTING SPU. AIC = AKAIKE INFORMATION CRITERION; DF = DEGREE OF FREEDOM; LOG LIK = LOG LIKELIHOOD; PR(&gt;CHISQ) QUANTIFIES STATISTICAL SIGNIFICANCE. STATISTICALLY SIGNIFICANT VARIABLES ARE SHADED.**
# TABLE IV
(odds ratio: 13). This indicates that the more SPR the app developers receive, the more likely they will release SPU. In contrast to SPR ratio, reply ratio has negative impact on SPU (odds ratio: 0), indicating that if developers reply to a review, the less likely the following app updates are security- and privacy-related. When we consider SPR, we see that most of the developers’ replies are Explanation (see Section IV-A3) why such permissions are needed. This is further supported by our regression model. We argue that if permission requests of Android apps are more transparent (e.g., better explanation, request in context), users would understand why such requests are indeed reasonable, hence developers would not need to explain themselves in their replies. Finally, the average score has a negative impact on SPU. More precisely, if an app is receiving high scores (on average), then the next updates are less likely to be related to security/privacy (odds ratio 0).

# VI. DISCUSSION
We discuss shortcomings of our approach and interpret our findings. Then, we highlight future work and a call for action.

# A. Threats to Validity and Future Work
Our approach relies on the ability to map SPR back to app versions in order to measure possible app changes as reaction to user reviews. Similar to related work , we could not retarget the upload dates for all versions of our dataset. In particular, for app versions released before 2012 there exists no reliable third-party source that can be queried for upload dates. As a consequence, we failed to map 629/5,527 SPR (11%) back to app versions and therefore cannot assess the impact of these SPR on the app’s security and privacy.

Further, we use static code analysis to identify security and privacy related changes in app versions (immediately) following an SPR. This empirical evidence is a strong indicator that these changes have been made as a (direct) consequence of the SPR. Reasons for SPU range from following the principle of least privilege to protecting users’ privacy to monetary reasons due to bad ratings and a decline in the number of app installations. For the small number of SPRs to which the developer replied and confirmed the issues, we can directly verify our findings. However, in general, collecting the ground truth would require conducting a developer survey to ask directly for the incentive of these changes. Prior studies ,  have shown that recruiting a reasonable number of developers in Google Play for a survey is challenging without direct infrastructure support of the market (i.e., response rates <1%). We abstained from conducting a survey, as we only have a limited set of 2,583 top apps, with an even smaller number of distinct app developers and, hence, given prior experiences , , a too small expected number of responses.

Another improvement would include adding a sentiment analysis to our binary review classifier (SPR/non-SPR). This could help in understanding ambiguous SPR where users complain about requested permissions but still like the application or when users complain but are explicitly fine with a good explanation of the permission usage.

Lastly, we focused in our study on the top apps in Google Play, for which a higher level of maintenance and developer responsiveness to reviews would be expected. Our results might not apply to the long tail of apps on Play. However, since the top apps account for the bulk of the app downloads on Play , , our results apply to the apps with the highest impact on Android’s user base.

# B. The Effect of SPR
Previous work has not given much attention to the influence of end-users on security and privacy of apps via app reviews (see Section II). Our results show that end-user complaints based on observable evidence (permissions, crashes, or anomalies like unusual battery drain) often lead to app changes that improve security and privacy aspects. In cases where the issues can be attributed to closed-source components (see Section IV-B2), the developers might not even have been aware of these problems without an involved code analysis, e.g., when the library documentations miss important details.

User reviews can also force app developers to react quickly to issues due to the snowball effect. In many cases it is not a single SPR that triggers app changes but a series of SPR by different users (SPR ratio in regression model) or SPR followed by a series of follow-up reviews with low star ratings agreeing with the initial SPR (avg score between app versions). Developers are then forced to react due to a fear of losing reputation (star rating) and user base that typically manifests in significant impact on revenue. As a result, developers either try to quickly resolve the problem by providing a better explanation to end-users or by addressing the issue with an SPU.

Although our results emphasize the positive effect of SPR in general, reviews could be much more informative and effective without the current size limitation of 350 characters for both reviews and replies imposed by Google. Such limits force users to omit important details in reviews and make them use alternative, unrestricted communication channels, such as email (see developer responses in Section IV-A3). This also prevents comprehensive app reviews as we know it from consumer reviews for shops, such as Amazon. Although Google is aware of this problem for years , no improvements have been made to remedy the situation.

# C. The Effect of Runtime Permissions
Permissions are one of the most important security and privacy indicators of apps that can be perceived even by less tech-savvy users. However, the way how permission requests are presented to the user greatly affects their effectiveness (e.g., habituation effects, user understanding, etc., see Section II). The most drastic recent change in Android’s permission system is the switch from install-time to runtime permissions, from which we can also observe a ripple effect onto users’ reviews. Before Android 6, install-time permissions provided a one-time decision possibility without context. Without an explicit connection from permissions to functionality, users have to resort to (frequently missing or incomplete) app descriptions for permission decisions. With the introduction of runtime permissions, permission requests are (typically) shown in context and end-users may decide differently when the same request is displayed on different occasions. Further, developers have the possibility to augment permission requests with information to explain the necessity of a permission in a given situation.