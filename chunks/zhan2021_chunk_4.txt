Fig. (4a) shows the method-level FPR and FNR at different similarity thresholds. We can find when the threshold 6 is around 0, both the FPR and FNR are relatively low. Therefore, we choose 6 = 0 as the MSS threshold where the FPR is less than 1% and FNR is less than 0%, which can achieve a good trade-off. Fig. (4b) shows the TPL-level FPR and FNR at different thresholds. According to the result, we find that when the threshold is gradually close to 0, many false positives appear due to the same TPL with different minor-changed versions. When the threshold is close to 1, the number of false negatives increases. From Fig. 4, we can find FPR and FNR achieve a good trade-off when the threshold is around 0, we thus choose 0 as the threshold 8 of TSS. In summary, we employ 0 = 0 and 8 = 0 for the following experiments.

# B. RQ1: Effectiveness Evaluation
# Experimental Setup
For the effectiveness evaluation, we compare ATVH u n t e r with the state-of-the-art publicly- available TPL detection tools (i.e., LibID, LibScout, OSSP o LICE, and LibPecker) that can specify the used TPL versions by using our ground truth dataset (§ IV-A). We employ three evaluation metrics, i.e., precision, recall, and FI Score to evaluate the detection accuracy at both TPL-level and version-level. TPL-level identification indicates the ability to identify the in-app TPLs correctly (without specifying the versions), and version-level identification indicates the ability to find both the correct TPLs and the correct versions. For example, if a tool reports that it finds “okio-2, okio-2” in an app but the ground truth is “okio-2”, in this situation, for TPL-level, we consider the tool find the correct TPL; for version-level, we consider there are two false positives and one false negative.

# Result
Table I shows the comparison results of ATVH u n t e r and other state-of-the-art tools. Considering the overall performance, we can see ATVH u n t e r outperforms other tools regarding all the metrics; the FI score of ATVH u n t e r at library-level and version-level reached 93% and 88%, respectively. For library-level identification, we can find that all of them can achieve high precision at TPL-level identification but the performance of recall of current state-of-the-art tools is mediocre. In contrast, the recall of ATVH u n t e r is 88%, which is far better than others. For version-level identification, we can find the precision (90%) and recall (87%) of ATVH u n t e r is much higher than that of other tools. Compared with the library-level precision, we can see the precision of each tool at version-level decreases a lot, which means most of them can identify the TPL but they cannot pinpoint the exact versions. We elaborate on the reasons for false positives and false negatives of ATVH u n t e r and other state-of-the-art tools as follows.

# FP Analysis
The reasons for the false positives of ATVH u n t e r can be concluded in three points: (1) reuse of open-source components. We find some TPLs are re-developed based on other TPLs, with only small code changes, if their similarity is larger than the defined threshold, ATVH u n t e r will report the reused ones at the same time, which are false positives. (2) Artifact id or group id changes. We identify a TPL by using its group id, artifact id and version number. However, we find that some old version TPLs has migrated to the new ones, with their group id or artifact id changed, but their code has little difference. Take the TPL file “EventBus” as an example, “org.greenrobot:eventbus”  is the upgraded version of “de.greenrobot:eventbus” . The code of these two TPLs have high similarity but with different group ids. ATVH u n t e r matches both of them and considers they are different TPLs. (3) Different versions with high similarity. The other reason for the false positives of ATVH u n t e r is that some versions of the same TPL have little or no difference in their code. For example, “ACRA_4” only modifies a few statements in a method of “ACRA_4”, and ATVH u n t e r will report them as different TPLs.

# TABLE I: Library and Version Detection Comparison
We find the packages structures of many real-world in-app TPLs are more or less obfuscated, and some TPLs are even without any package structure; current tools cannot handle such cases, leading to false negatives. Besides, it is difficult to use the package structure and package name to ensure the TPL candidates, as demonstrated in SIII-A4. Many different TPLs may have the same package name, and one independent package tree could include several TPLs; therefore, existing tools may generate incorrect code features for these TPLs, which also can lead to false negatives.

LibID uses Dex2jar to decompile apps; it does not always work in all apps. As for the false positives of other tools, the code feature of LibScout (i.e., fuzzy method signature) is too coarse, which discounts the recall of LibID. Besides, LibScout and OSSPoLICE are sensitive to CFG structure modification. Compared with them, our CFG adjacency list is less sensitive to the CFG structure modification. We consider both the syntax and semantic information, and our method adopts the fuzzy hash to generate the TPL fingerprints. Thus, code statements modification can only affect part of the fingerprints, which is more robust to different code obfuscations.

Based on the above analysis, we can find that the strategy of feature selection, extraction, and generation are essential, which can directly affect the performance of the system.

# Conclusion
ATVHunter outperforms state-of-the-art TPL detection tools, achieving 98% precision, 88% recall at library level, and 90% precision, 87% recall at version level.

# FN Analysis
ATVHunter aims to find TPL versions with high precision; thus, we sacrificed part of the recall when we select the similarity threshold. The reasons for false negatives of ATVHunter are as follows: (1) When compiling an app, developers may take some optimizations to reduce the size of their app. The strategy is that the compiler automatically removes some functions of TPLs that are not called by host apps, which causes the in-app TPLs to be different from the original TPLs, leading to false negatives. (2) Some TPLs are integrated into the same package namespace of the host app, which may be deleted at the pre-processing stage, leading to false negatives.

For example, some companies and organizations develop their own Ad SDK, whose package name is the same as that of the host app. However, the code under the package structure of the host app is deleted at the pre-processing stage, i.e., the ad library is also deleted without further consideration, causing false negatives. (3) Another reason is that some apps use rarely-used open-source TPLs hosted on open-source platforms (e.g., Github or Bitbucket) which are not in our TPL database (with over 3 million TPLs), leading to false negatives.

For example, “com.github.DASAR.ShiftColorPicker”, “android-retention-magic-1”, and “android-json-rpc-0” are developed and hosted on Github and not in our dataset; therefore, ATVHunter cannot find this TPL. Since other tools also use the similarity comparison method to find in-app TPLs, this situation may also affect their recall.

# C. RQ2: Efficiency Evaluation
In this section, we investigate the detection time of ATVHunter and compare it with state-of-the-art tools to verify its efficiency. We compare the detection time of ATVHunter with existing tools by employing the dataset collected in § IV-A. All compared tools choose similarity comparison method to find in-app TPLs; thus, the detection time mainly depends on the number of in-app TPLs and the number of TPL features in the database. The detection time is the period cost for finding all TPLs in a test app. Note that the detection time does not include the database construction time.

Table II shows the comparison result of detection time to evaluate the efficiency of each tool. We can see that the efficiency of ATVHunter also outperforms the state-of-the-art tools (66s per app on average). The second one is LibScout, and the average detection time is about 83s. LibID and LibPecker are relatively time-consuming; the average detection time could reach about 16h and 4h per app.

# TABLE II: Comparison Results of Detection Time (per app)
# TABLE III: Comparison on Code Obfuscation Techniques
# Conclusion
Compared with other tools, ATVHUNTER can identify exact TPL-Vs with high efficiency and it takes less time for TPL detection on the ground-truth TPL database. The obfuscation-resilient capability is an important index to measure the performance of a TPL detection tool since obfuscation techniques can discount the detection performance.