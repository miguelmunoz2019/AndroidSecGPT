# Different Security Smells Found in Apps
# Empirical Software Engineering
are less common than projects with only a few contributors. The more people are involved in a project the more the security decreases, especially for large teams. More precisely, we found statistical evidence that only small teams of up to five people are capable of consistently building projects resistant to most security code smells, by using the nonparametric Mann-Whitney U test that does not require the assumption of normal distributions for the dataset. The mean different smell occurrences in the groups “projects with one contributor” and “projects with six contributors” were 1 and 1; the distributions in the two groups differed significantly (Mann-Whitney U = −2, n1 < n2 = 0, P < 0 two-tailed). Similarly, we found that the distributions in the two groups “projects with six to forty contributors” and “projects with more than forty contributors” were diverse (Mann-Whitney U = −2, n1 < n2 = 0, P < 0 two-tailed) with mean different smell occurrences of 1 and 2, respectively.

# 4 App Updates
We investigated the smell occurrences in subsequent app releases. Of the 732 projects, 33 (4%) of them released updates that either resolved or introduced issues. By inspection of source code we noticed that many of the updates targeted new functionality, e.g., addition of new implicit intents to share data with other apps, implementation of new notification mechanisms for receiving events from other apps using implicit pending intents, or registration of new custom schemes to provide further integration of app related web content into the Android system. We believe this is due to developers focusing on new features instead of security.

For the majority of the app updates that introduced new security smells, we found that the dominant cause for decreased security is the accommodation of social interactions and data sharing features in the apps updates. Hence, developers should be particularly cautious when integrating new functionality into an app.

# 4 Evolution
Every new Android version introduces changes that strengthen security. The targeting of outdated Android releases will not only limit the supported feature set to the respective release, but also introduce potential security issues as security fixes are continuously integrated into the OS with each update.

# Empirical Software Engineering
# percentage of all issues found
# 4 Comparison to Existing Android Lint Checks
In order to compare our findings with other issues in the apps, we correlated the results from the existing Android Lint framework with security code smells. We wanted to explore whether frequent reports of specific Android Lint issue categories were also indicative of security issues, or in other words, if security checks by the Android Lint framework agree with our security smells and whether other quality aspects of an app could relate to its security level. We collected all available issue reports for each app and then extracted the occurrences of each detected issue.

We applied the Pearson product-moment correlation coefficient algorithm for each ICC security smell category combination according to the following formula:
Pearson(x, y) = √∑(x − ¯x)(y − ¯y) / √∑(x − ¯x)² ∑(y − ¯y)², where
x is the array of all apps issue occurrences in category ICC security code smells, y is the array of all apps issue occurrences in the respective Android Lint category, and x, ¯y represent the corresponding sample means.

It provides a linear correlation between two vectors represented as a value in the range of −1 (total negative linear correlation) and +1 (total positive linear correlation). The correlation of the Android Lint categories and our ICC smell category in Table 3 reveals several interesting findings: (1) Our ICC security category strongly correlates with the Android Lint security category (+0), which contains checks for a variety of security-related issues such as the use of user names and passwords in strings, improper cryptography parameters, and bypassed certificate checks in WebView components. (2) Another discovery is the minor correlation between the ICC security smells and the Android Lint correctness.

# Empirical Software Engineering
category (+0). This category includes checks for erroneously configured project build parameters, incomplete view layout definitions, and usages of deprecated resources. (3) Furthermore, we assume that usability does not impede security (+0), because issues in usability are closely related to UI mechanics. (4) Finally, minor correlations are shown for performance, accessibility, and internationalization. These three categories have in common that they rely heavily on UI controls and configurations.

To further assess how our tool performs on real world apps against the Android Lint detections, we take the 100 apps with the most and least prevalent ICC security smells and compare them to Android Lint’s analysis results. We expect to see significant similarities in the increase of issues detected as our security smells correlate to Android Lint’s security checks, i.e., the least vulnerable apps should suffer less in both, the Android Lint checks and our security smell detectors. Figure 5 illustrates two plots, each presenting our analysis results for the 100 apps suffering the most and the least from ICC security smells, respectively. The vertical axis represents the condensed mean number of found issues, that is, we conflated all detected ICC security smell issues, regardless of their smell categories, into “ICC Security Smells”. The remaining Android Lint categories on the x-axis are treated accordingly. The crosses represent the mean value of the number of different issues apps are suffering from in each category, and, as we hid any outliers to increase readability, these values can exceed the first quartiles. The least and most affected apps clearly correspond in terms of issue frequency among specific categories, that is, the mean number of issues found in each category is between 29% and 332% higher on behalf of the 100 most vulnerable apps. Besides the ICC Security Smells category with an increase of 219% in issues found, the Android Lint security category experienced an increase of 152%. The Correctness: Message and the Usability: Typography categories of Android Lint achieved, unexpectedly, an increase in issues found of about 332% and 174%, respectively. After manual verification, we discovered that these gains were mostly caused by flawed language dictionary entries used for internationalization, such as missing or misunderstood language dependent string declarations, spelling mistakes, and the use of strings containing three dots instead of the ellipsis character. While the 100 most vulnerable apps appear to prominently incorporate translations for several different languages, the 100 least vulnerable apps rarely make use of these features, hence, they suffer from much fewer issues. The remaining categories encountered an increase of less than 139%.

# Empirical Software Engineering
Interestingly, the internationalization category does not encounter a noticeable increase in issues due to its limited scope, i.e., it only covers five specific flaws regarding insufficient language adaption, and the use of uncommon characters or encodings. We propose that some of these issue detections should be reallocated to other categories, e.g., spelling mistakes should be assigned to internationalization, and vice versa the issue SetTextI18n in the category internationalization that reports any use of methods that potentially fail with number conversions.

# Empirical Software Engineering
# 4 Influence of Project Age and Activity
To explore the effect of recent updates, which we believed would improve app security, we evaluated our ICC category as well as the Android Lint security and correctness categories according to time since the last commit. More precisely, we were interested in the question: Do recent updates improve app security? A related question arises from the age of a project, i.e., are mature projects more secure than recent creations? We investigated these two questions based on available GitHub metadata, and brought the dates into perspective with the reported issues.

# 4 Influence of Code Size
Another popular indicator used in software analysis is the code size, which we measured in thousands of lines of code (kLOC) with the open-source tool cloc. As Android projects consist aside from source code of different configuration, resource and other utility files, we first ran the analysis of adopted software languages (e.g., Java, Kotlin, XML) that required each of those items, before we excluded all elements except the Java code in the main Java source folders for the kLOC measurements. We conjectured that we would see a trend of small teams developing small apps that are less likely to have problems. In contrast, we expect that aging projects are more likely to have smells as they are larger than more recent ones. Figure 7 illustrates the relation between the kLOC and other relevant properties.

In Fig. 7a we categorized projects according to their size on the x-axis, while the left y-axis displays contributors per project, and the right y-axis the number of different categories of security smells found in apps, and the number of different languages used. We see a trend that larger projects rely on more contributors with a minor exception at 20–24 kLOC.