In this section, we describe the highlights of our evaluation (Section 6), along with the three experiments we conduct, and their results. In the first experiment (Section 6), we run μSE on three tools, and record the number of leaks that each tool fails to detect (i.e., the number of uncaught mutants). In the second experiment (Section 6), we perform an in-depth analysis of FlowDroid by applying our systematic manual analysis methodology (Section 4) on the output of μSE for FlowDroid. Finally, our third experiment (Section 6) measures the propagation and prevalence of the flaws found in FlowDroid, in tools from our dataset apart from FlowDroid, and two newer versions of FlowDroid.

These experiments are motivated by the following research questions:
- RQ1 Can μSE find security problems in static analysis tools for Android, and help resolve them to flaws/unsound choices?
- RQ2 Are flaws inherited when a tool is reused (or built upon) by another tool?
- RQ3 Does the semi-automated methodology of μSE allow a feasible analysis (in terms of manual effort)?
- RQ4 Are all flaws unearthed by μSE difficult to resolve, or can some be isolated and patched?
- RQ5 How robust is μSE’s performance?
# 6 Evaluation overview and Highlights
We insert a total of 7,584 data leaks (i.e., mutants) in a set of 7 applications using μSE. 2,026 mutants are verified as executable by the EE, and 83-1,480 are not detected depending on the studied tool. During our analysis, μSE exhibits a maximum one-cost run-time of 92 minutes (RQ5), apart from the time taken by the analyzed tool (e.g., FlowDroid) itself. Further, our in-depth analysis of the output of μSE for FlowDroid discovers 13 unique flaws that are not documented in either the paper or the source code repository (RQ1). Moreover, it takes our analyst, a graduate student with background in Android security, one hour per flaw (in the worst case), due to our systematic analysis methodology, as well as our dynamic filter (Section 4), which filters out over 73% of the seeded non-executable mutants (RQ3). Further, we demonstrate that two newer versions of FlowDroid, as well as the six other tools set apart from FlowDroid (including those that inherit it), are also vulnerable to at least one flaw detected in FlowDroid (RQ2). This is confirmed, with negligible effort, using minimal examples generated during our analysis of FlowDroid (RQ3). Finally, we are able to generate patches for a specific flaw discovered in FlowDroid, and our pull request has been accepted by the tool authors (RQ4).

# 6 Executing μSE
The objective of this experiment is to demonstrate the effectiveness of μSE in filtering out non-executable injected leaks (i.e., mutants), while illustrating that this process results in a reasonable number of leaks for an analyst to manually examine.

Methodology: We create 21 mutated APKs from 7 target applications, with 7,584 leaks among them,
by combining the security operators described in Section 4, with mutation schemes from Section 4. First, we measure the total number of leaks injected across all apps, and then the total number of leaks marked by the EE as non-executable. Note that this number is independent of the tools involved, i.e., the filtering only happens once, and the mutated APKs can then be passed to any number of tools for analysis. The non-executable leaks are then removed. Next, we configure FlowDroid, Argus, and DroidSafe and evaluate each tool with μSE individually, by running them on the mutated apps (with non-executable leaks excluded) and recording the number of leaks not detected by each tool (i.e., the surviving mutants).

Results: μSE injects 7,584 leaks into the Android apps, of which, 5,558 potentially non-executable leaks are filtered out using our EE, leaving only 2,026 leaks confirmed as executable in the mutated apps. By filtering out a large number of potentially non-executable leaks (i.e., over 73%), our dynamic filtering significantly reduces manual effort (RQ3). Table 1 shows the statistics acquired from μSE’s output over FlowDroid, Argus, and DroidSafe. We observe that FlowDroid cannot detect over 48% of the leaks, while Argus cannot detect over 73%. Further, DroidSafe does not detect a non-negligible percentage of leaks (i.e., over 4%), and as these leaks have been confirmed to execute by our EE, it is safe to say that DroidSafe has flaws as well. Note that this experimental result validates our conceptual argument, that security operators designed for a specific goal may apply to tools with that goal. However, given its popularity, we limit our in-depth evaluation to FlowDroid.

Finally, we measure the runtime of the μSE-specific part of the analysis, i.e., up to executing the tool to be evaluated, to be a constant 92 minutes in the worst case, a majority of which (i.e., 99%) is taken up by the EE. Note that the time taken by μSE is a one-time cost, and does not have to be repeated for tools with a similar security goal (RQ5).

# 6 FlowDroid Analysis
This experiment demonstrates an in-depth, manual analysis of FlowDroid, which we choose for two reasons: (1) impact (FlowDroid is cited by 700 papers and numerous other tools depend on it), and (2) potential for change (since FlowDroid is being maintained at the moment, any contributions we can make will have immediate benefits).

# Methodology:
We performed an in-depth analysis using the list of surviving mutants (i.e., undetected leaks) generated by μSE for FlowDroid v2 in the previous experiment. We leveraged the methodology for systematic manual evaluation, described in Section 4, and discovered 13 unique flaws. We confirmed that none of the discovered flaws have been documented before; i.e., in the FlowDroid paper or in their official documentation.

# Results:
We discovered 13 unique flaws, from FlowDroid alone, demonstrating that μSE can be effectively used to find problems that can be resolved to flaws (RQ1). Using the approach from Section 4, the analyst needed less than an hour to isolate a flaw from the set of undetected mutants, in the worst case. In the best case, flaws were found in a matter of minutes, demonstrating that the amount of manual effort required to quickly find flaws using μSE is minimal (RQ3). We give descriptions of the flaws discovered as a result of μSE’s analysis in Table 2.

We have reported these flaws, and are working with the developers to resolve the issues. In fact, we developed patches to correctly implement Fragment support (i.e., flaw 13 in Table 2), which were accepted by developers.

# Flaw Classes:
To gain insight about the practical challenges faced by static analysis tools, and their design flaws, we further categorize the discovered flaws into the following flaw classes:
FC1: Missing Callbacks: The security tool (e.g., FlowDroid) does not recognize some callback method(s), and will not find leaks placed within them. Tools that use lists of APIs or callbacks are susceptible to this problem, as prior work has demonstrated as the generated list of callbacks (1) may not be complete, and (2) or may not be updated as the Android platform evolves. We found both these cases in our analysis of FlowDroid. That is, DialogFragments was added in API 11, i.e., before FlowDroid was released, and NavigationView was added after. These limitations are well-known in the community of researchers at the intersection of program analysis and Android security, and have been documented by prior work . However, μSE helps evaluate the robustness of existing security tools against these flaws, and helps in uncovering these undocumented flaws for the wider security audience. Additionally, some of these flaws may not be resolved even after adding the callback to the list; e.g., PhoneStateListener and SQLiteOpen-
Helper, both added in API 1, are not interfaces, but abstract classes. Therefore, adding them to FlowDroid’s list of callbacks (i.e., AndroidCallbacks.txt) does not resolve the issue.

FC2: Missing Implicit Call: The security tool does not identify leaks within some method that is implicitly called by another method. For instance, FlowDroid does not recognize the path to Runnable.run() when a Runnable is passed into the ExecutorService.submit(Runnable). The response from the developers indicated that this class of flaws was due to an unresolved design challenge in Soot’s  SPARK algorithm, upon which FlowDroid depends. This limitation is also known within the program analysis community . However, the documentation of this gap, thanks to μSE, would certainly help developers and researchers in the wider security community.

FC3: Incorrect Modeling of Anonymous Classes: The security tool misses data leaks expressed within an anonymous class. For example, FlowDroid does not recognize leaks in the onReceive() callback of a dynamically registered BroadcastReceiver, which is implemented within another dynamically registered BroadcastReceiver’s onReceive() callback. It is important to note that finding such complex flaws is only possible due to μSE’s semi-automated mechanism, and may be rather prohibitive for an entirely manual analysis.