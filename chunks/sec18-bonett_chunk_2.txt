The rest of the paper proceeds as follows: Section 2 motivates our approach, and provides a brief background. Section 3 describes the general approach and the design goals. Section 4 and Section 5 describe the design and implementation of μSE, respectively. Section 6 evaluates the effectiveness of μSE, and Section 7 delivers the insights distilled from it. Section 8 describes related work. Section 9 describes limitations. Section 10 concludes.

# 2 Motivation and Background
This work is motivated by the pressing need to help researchers and practitioners identify instances of unsound assumptions or design decisions in their static analysis tools, thereby extending the sound core of their soundy techniques. That is, security tools may already have a core set of sound design decisions (i.e., the sound core), and may claim soundness based on those decisions. While the soundness manifesto  defines the sound core in terms of specific language features, we use the term in a more abstract manner to refer to the design goals of the tool. Systematically identifying unsound decisions may allow researchers to resolve flaws and help extend the sound core of their tools.

Moreover, research papers and tool documentations indeed do not articulate many of the unsound assumptions and design choices that lie outside their sound core, aside from some well-known cases (e.g., choosing not to handle reflection, race conditions), as confirmed by our results (Section 6). There is also a chance that developers of these techniques may be unaware of some implicit assumptions/flaws due to a host of reasons: e.g., because the assumption was inherited from prior research or a certain aspect of Android was not modeled correctly. Therefore, our objective is to discover instances of such hidden assumptions and design flaws that affect the security claims made by tools, document them explicitly, and possibly, help developers and researchers mend existing artifacts.

# 2 Motivating Example
Consider the following motivating example of a prominent static analysis tool, FlowDroid : FlowDroid  is a highly popular static analysis framework for detecting private data leaks in Android apps by performing a data flow analysis. Some of the limitations of FlowDroid are well-known and stated in the paper ; e.g., FlowDroid does not support reflection, like most static analyses for Java. However, through a systematic evaluation of FlowDroid, we discovered a security limitation that is not well-known or accounted for in the paper, and hence affects guarantees provided by the tool’s analysis. We discovered that FlowDroid (i.e., v1, latest as of 10/10/17) does not support “Android fragments” , which are app modules that are widely used in most Android apps (i.e., in more than 90% of the top 240 Android apps per category on Google Play, see Appendix A). This flaw renders any security analysis of general Android apps using FlowDroid unsound, due to the high likelihood of fragment use, even when the app developers may be cooperative and non-malicious.

Further, FlowDroid v2, which was recently released , claims to address fragments, but also failed to detect our exploit. On investigating further, we found that FlowDroid v1 has been extended by at least 13 research tools , none of which acknowledge or address this limitation in modeling fragments. This leads us to conclude that this significant flaw not only persists in FlowDroid, but may have also propagated to the tools that inherit it. We confirm this conjecture for inheritors of FlowDroid that also detect data leaks, and are available in source or binary form (i.e., 2 out of 13), in Section 6.

Finally, we reported the flaws to the authors of FlowDroid, and created two patches to fix it. Our patches were confirmed to work on FlowDroid v2 built from source, and were accepted into FlowDroid’s repository . Thus, we were able to discover and fix an undocumented design flaw that significantly affected FlowDroid’s soundness claims, thereby expanding its sound core. However, we have confirmed that FlowDroid v2  still fails to detect leaks in fragments, and are working with developers to resolve this issue.

Through this example, we demonstrate that unsound assumptions in security-focused static analysis tools for Android are not only detrimental to the validity of their own analysis, but may also inadvertently propagate to future research. Thus, identifying these unsound assumptions is not only beneficial for making the user of the analysis aware of its true limits, but also for the research community in general. As of today, aside from a handful of manually curated testing toolkits (e.g., DroidBench ) with hard-coded (but useful) checks, to the best of our knowledge, there has been no prior effort at methodologically discovering problems related to soundiness in Android static analysis tools and frameworks. This paper is motivated by the need
USENIX Association 27th USENIX Security Symposium 1265
# Mutants
# Static Analysis
App 1 App 2 ...App n Analyze Improved
Uncaught tool t' Mutants
Mutate apps
# Mutation Security
Sound core
SE
to systematically identify and resolve the unsound assumptions in security-related static analysis tools.

# 2 Background on Mutation Analysis
Mutation analysis has a strong foundation in the field of SE, and is typically used as a test adequacy criterion, measuring the effectiveness of a set of test cases . Faulty programs are created by applying transformation rules, called mutation operators to a given program. The larger the number of faulty programs or mutants detected by a test suite, the higher the effectiveness of that particular suite. Since its inception , mutation testing has seen striking advancements related to the design and development of advanced operators. Research related to development of mutation operators has traditionally attempted to adapt operators for a particular target domain, such as the web , data-heavy applications , or GUI-centric applications . Recently, mutation analysis has been applied to measure the effectiveness of test suites for both functional and non-functional requirements of Android apps . This paper builds upon SE concepts of mutation analysis and adapts them to a security context. Our methodology does not simply use the traditional mutation analysis, but rather redefines this methodology to effectively improve security-focused static analysis tools, as we describe in Sections 4 and 8.

# 3 μSE
We propose μSE, a semi-automated framework for systematically evaluating Android static analysis tools that adapts the process of mutation analysis commonly used to evaluate software test suites . That is, we aim to help discover concrete instances of flawed security design decisions made by static analysis tools, by exposing them to methodologically mutated applications. We envision two primary benefits from μSE: short-term benefits related to straightforwardly fixable flaws that may be patched immediately, and long-term benefits related to the continuous documentation of assumptions and flaws, even those that may be hard to resolve. This section provides an overview of μSE (Figure 1) and its design goals.

As shown in Figure 1, we take an Android static analysis tool to be evaluated (e.g., FlowDroid  or MalloDroid ) as input. μSE executes the tool on mutants, i.e., apps to which security operators (i.e., security-related mutation operators) are applied, as per a mutation scheme, which governs the placement of code transformations described by operators in the app (i.e., thus generating mutants). The security operators represent anomalies that the static analysis tools are expected to detect, and hence, are closely tied to the security goal of the tool. The uncaught mutants indicate flaws in the tool, and analyzing them leads to the broader discovery and awareness of the unsound assumptions of the tools, eventually facilitating security-improvements.

# Design Goals
Measuring the security provided by a system is a difficult problem; however, we may be able to better predict failures if the assumptions made by the system are known in advance. Similarly, while soundness may be a distant ideal for security tools, we assert that it should be feasible to articulate the boundaries of a tool’s sound core. Knowing these boundaries would be immensely useful for analysts who use security tools, for developers looking for ways to improve tools, as well as for end users who benefit from the security analyses provided by such tools. To this end, we design μSE to provide an effective foundation for evaluating Android security tools. Our design of μSE is guided by the following goals:
- G1 Contextualized security operators. Android security tools have diverse purposes and may claim various security guarantees. Security operators must be instantiated in a way that is sensitive to the context or purpose (e.g., data leak identification) of the tool being evaluated.

- G2 Android-focused mutation scheme. Android’s security challenges are notably unique, and hence require a diverse array of novel security analyses. Thus, the mutation schemes, i.e., the placement of the target, unwanted behavior in the app, must consider Android’s abstractions and application model for effectiveness.