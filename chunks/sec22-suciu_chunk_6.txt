Our evaluation highlights a qualitative distinction between the problem of predicting functional exploits and that of predicting exploits in the wild. Surprisingly, we observe that social media features are not as useful for predicting functional exploits as they are for exploits in the wild , a finding reinforced by our additional experiments from the technical report , which show that they do not improve upon other categories. This is because tweets tend to only summarize and repeat information from write-ups, and often do not contain sufficient technical information to predict exploit development. Besides, they often incur an additional publication delay over the original write-ups they quote.

Overall, our evaluation highlights the need for noise correction techniques tailored to our problem.

388 31st USENIX Security Symposium USENIX Association
# Precision
Component: NVD & Write-Ups (AUC = 0)
Component: Poc Info & Code (AUC = 0)
Mean ε
All: EE (AUC = 0)
Recall
(a) Performance of EE compared to constituent subsets of features. (b) P evaluated at different points in time.

we compare the performance of EE trained on all feature sets, with that trained on PoCs and vulnerability features alone. We observe that PoC features outperform these from vulnerabilities, while their combination results in a significant performance improvement. The result highlights the two categories complement each other and confirm that PoC features provide additional utility for predicting exploitability. On the other hand, as described in detail in the technical report , we observe no added benefit when incorporating social media features into EE. We therefore exclude them from our final EE feature set.

EE performance improves over time. In order to evaluate the benefits of time-varying exploitability, the precision-recall curves are not sufficient, because they only capture a snapshot of the scores in time. In practice, the EE score would be compared to that of other vulnerabilities disclosed within a short time, based on their most recent scores. Therefore, we introduce a metric P to compute the performance of EE in terms of the expected probability of error over time.

For a given vulnerability i, its score EE(z) computed on date z and its label Di (Di = 1 if i is exploited and 0 otherwise), the error P EE (z, i, S) w.r.t. a set of vulnerabilities S is computed as:
P EE (z, i, S) = { ||{D =0∧EE (z)≥EE(z)| j∈S}|| if Di = 1
||{D =1∧EE (z)≤EE(z)| j∈S}||||S|| if Di = 0
If i is exploited, the metric reflects the number of vulnerabilities in S which are not exploited but are scored higher than i on date z. Conversely, if i is not exploited, P computes the fraction of exploited vulnerabilities in S which are scored lower than it. The metric captures the amount of effort spent prioritizing vulnerabilities with no known exploits. For both cases, a perfect score would be 0.

For each vulnerability, we set S to include all other vulnerabilities disclosed within t days after its disclosure. Figure 6b plots the mean P over the entire dataset, when varying t between 0 and 30, for both exploited and non-exploited vulnerabilities. We observe that on the day of disclosure, EE already provides a high performance for exploited vulnerabilities: on average, only 10% of the non-exploited vulnerabilities disclosed on the same day will be scored higher than an exploited one. However, the score tends to overestimate the exploitability of non-exploited vulnerabilities, resulting in many false positives. This is in line with prior observations that static exploitability estimates available at disclosure have low precision . By following the two curves along the X-axis, we observe the benefits of time-varying features. Over time, the errors made on non-exploited vulnerabilities decrease substantially: while such a vulnerability is expected to be ranked above 44% exploited ones on the day of disclosure, it will be placed above 14% such vulnerabilities 10 days later. The plot also shows that this sharp performance boost for the non-exploited vulnerabilities incurs a smaller increase in error rates for the exploited class. We do not observe great performance improvements after 10 days from disclosure. Overall, we observe that time-varying exploitability contributes to a substantial decrease in the number of false positives, therefore improving the precision of our estimates. To complement our evaluation, the precision-recall trade-offs at various points in time is reported in Appendix A.

# 7 Case Studies
In this section we investigate the practical utility of EE through two case studies.

EE for critical vulnerabilities. To understand how well EE distinguishes important vulnerabilities, we measure its performance on a list of recent ones flagged for prioritized remediation by FireEye . The list was published on December 8 2020, after the corresponding functional exploits were stolen . Our dataset contains 15 of the 16 critical vulnerabilities.

We measure how well our classifier prioritizes these vulnerabilities compared to static baselines, using the P prioritization metric defined in the previous section, which computes the fraction of non exploited vulnerabilities from a set S that are scored higher than the critical ones. For each of the 15 vulnerabilities, we set S to contain all others disclosed within 30 days from it, which represent the most frequent alternatives for prioritization decisions. Table 6 compares the statistics for the baselines, and for P EE computed on the date critical vulnerabilities were disclosed δ, 10 and 30 days later, as well as one day before the prioritization recommendation was published. CVSS scores are published a median of 18 days.

# Time-varying AUC Score
We further consider the possibility that the timestamps in DS3 may be affected by label noise. We evaluate the potential impact of this noise with an approach similar to the one in Section 7. We simulate scenarios where we assume that a percentage of PoCs are already functional, which means that their later exploit-availability dates in DS3 are incorrect. For those vulnerabilities, we update the exploit availability date to reflect the publication date of these PoCs. This provides a conservative estimate, because the mislabeled PoCs could be in an advanced state of development, but not yet fully functional, and the exploit-availability dates could also be set too early. We simulate percentages of late timestamps ranging from 10–90%. Figure 7b plots the performance of EE(δ) in this scenario, averaged over 5 repetitions. We observe that even if 70% of PoCs are considered functional, the classifier outperforms the baselines and maintains an AUC above 0. Interestingly, performance drops after disclosure and is affected the most on predicting exploits published within 12 days. Therefore, the classifier based on disclosure-time artifacts learns features of easily exploitable vulnerabilities, which get published immediately, but does not fully capture the risk of functional PoC that are published early. We mitigate this effect by updating EE with new artifacts daily, after disclosure. Overall, the result suggests that EE may be useful in emergency response scenarios, where it is critical to urgently patch the vulnerabilities that are about to receive functional exploits.

# 8 Related Work
Predicting exploits in the wild. Most of the prior exploit prediction work has been towards the tangential task of predicting exploits in the wild. This has been investigated in our prior study  and Chen et al.  by monitoring Twitter for vulnerability discussions, and Xiao et al.  by using post-disclosure field data about exploitation. Jacobs et al.  used vulnerability prevalence data to improve prediction. Jacobs et al.  proposed EPSS, a scoring system for exploits. Allodi  calculated the likelihood of observing exploits in the wild after they are traded in underground forums.

Vulnerability Exploitability. Allodi and Massacci  investigated the utility of the CVSS scores for capturing the likelihood of attacks in the wild. Prior work by Bozorgi et al.  formulated exploitability estimation as the problem of predicting the existence of PoCs based on vulnerability characteristics. Allodi and Massaci  concluded that the publication of a PoC in ExploitDB is not a good indicator for exploits in the wild. Our work shows that, while their presence might not be a sufficiently accurate indicator, the features within these PoCs are useful for predicting functional exploits. DarkEmbed  uses natural language models trained on private data from underground forum discussions to predict the availability of exploits, but such artifacts are generally published with.

a delay . Instead, EE uses only publicly available artifacts for predicting exploits soon after disclosure; we were unable to obtain these artifacts for comparison upon contacting the authors.

PoCs. PoCs were also investigated in measurements on vulnerability lifecycles. Shahzad et al  performed a measurement of the vulnerability lifecycle, discovering that PoCs are generally released at the time of disclosure. Mu. et al  manually curated and utilized PoCs to trigger the vulnerabilities. FUZE  used PoCs to aid exploit generation for 5 functional exploits.