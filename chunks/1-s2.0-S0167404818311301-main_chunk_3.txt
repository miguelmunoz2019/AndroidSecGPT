From the perspective of APP developing environment, the most-recent configuration file (such as “project.properties” file in eclipse) is generated by “Android Tool”, which supplies a standard configuration for developers when using ProGuard. For optimization, it turns off the option by default (with option “-dontoptimize”). From the perspective of a developer, the customized use of ProGuard can trigger different consequences even if the TPL is processed by the same ProGuard. Again, in the example of
1 Data issued from a modification of ProGuard 5. 2 The default configuration for using ProGuard (proguard-android-optimize.txt) turns off “vertically merged classes”, “horizontally merged classes”, “removed write-only fields”, “privatized fields”, “inlined constant fields”, “arithmetic peephole optimizations”, and “cast peephole optimizations” options, result for these options are not listed in the table. 3 APPs will not turn to be stable until “optimize iterations”. Other data comes from the first iteration of optimization. 4 Optimization is performed based on the results shown in Table 1. The overlapping of shrinkage and optimization tremendously change the APP.

# 3. Our approach and implementation
# 3. System overview
Aside from TPL detection methodology, an architecture is required to ensure the feasibly of TPL detection. In order to deploy TPL checking tool to protect end-user in real time, and promise performance benefits such as low latency and quick response time for customer, we make a profound survey on the bottleneck of real-world TPL detection in advance.

The data for TPL checking is coming from geographically distributed organizations, acquiring from the traffic flow. A high volume data is generated each day by observation. For example, about eighty millions URLs point to APP are discovered each day in CERNET network. The explosive proliferation of APPs for detection requires high bandwidth to transmit and becoming a big computing burden for our cloud. Since the pure cloud computing can’t be competent for TPL detection for the unpredictable latency, bandwidth bottlenecks, an elaborately designed architecture is required for leverage resources for computation, networking, and storage.

Considering the geographically distributed organizations where data is generated have the ability to equip with sufficient computing resource, we propose an edge computing architecture which provide elastic resources at the edge of the network. This computing paradigm collaboratively provide elastic computation, storage and communication for TPL detection. Data generated and mainly analyzed at edge, which reduce the delay of data analytics and decrease the cost of data transmission and storage.

# Computers & Security 80 (2019) 257 – 272
the overall system performance. In practice, edge nodes in our architecture are commonly equipped with low performance servers. To ensure the quick response time even when computation resource on the edge node has been exhausted, we offload the computation task to the back-end cloud.

# 3 Establishing the TPL Corpus
In this subsection, we try to establish the TPL corpus, which is indexed as a hash token, in order to detect both original or mutated (i.e., obfuscated, shrunk and optimized) TPLs. The built-in corpus is resilient against the obfuscation of ProGuard. We leverage “Big Code” to establish the corpus, based on the assumption that a TPL is used by many APPs. Apart from traditional work that uses qualified names as a feature of a TPL, instead, we use the combination of Android SDK API and string hashing as our features. After setting up the tree-based feature, we intentionally probe the most outstanding qualified name, known as “Package Stem”, to identify other versions of the TPL and related hashing. The built-in corpus serves as a benchmark dataset for detecting original or obfuscated TPLs directly. The pipeline of establishing TPL corpus is illustrated in Fig. 3.

Our TPL checking tool is now providing service for CETC, China Unicom, etc. The customer is geographically near to the spot where data is generated, and as Fig. 2 depicted, edge nodes are maintained by these organizations (customers). Meanwhile, some tasks, such as TPL signature exchanging, might benefit from the cloud running in the back-end, and the cloud is owned and maintained by us. Cloud of this architecture is the Hadoop cluster of our Janus platform, which is located in the cloud service provider UCloud (Ucloud Co., 2017). Cloud build TPL corpus and exchange signature between edge. The Janus configuration is shown in Table 4.

Edge nodes are geographically distributed, and logically decentralized in that they are maintained by different organizations. The organizations have the advantage to collect APPs and the localized edge overcome the high bandwidth required for transmitting APPs from different channels to our cloud.

To coordinate different entities in the edge computing architecture and providing a high performance and effective system, we define policies for the system. They are synchronization policy, locking policy, and migration policy. (1) The synchronization policy can reduce resources for computation. For example, a pre-checked APP should not be checked again. In order to achieve this goal, hash or signature for the pre-checked APP should be shared between the edge nodes. The policy enforces data items synchronization at a threshold of 100. That is, when the accumulated new APPs reach 100, the enforced data synchronization is started. (2) The locking policy guarantees the integrity of data. If the edge generated data and synchronized data from other edge arrived at the same time, the unlocked write operation will distort the data collection in the edge node. (3) The migration policy can improve.

# 3 Building the hash-tree package
We use Android SDK APIs  and strings to derive the package-level feature, because Android SDK APIs and strings remain stable even if an APP is obfuscated by ProGuard. The Android package is structured as a hierarchical tree. In the tree, a node is a folder and a leaf is a class file. We apply recursive depth-first search (DFS) algorithm to generate the hash tree, and then generate hash values based on the child hash values, in the way that hash values preserve the structural information of a TPL. Specifically, we do the following:
1. For each leaf i, we generate a vector vi of the functions, where each component encodes an Android SDK API. The length of the vector vi is denoted li = ‖v‖. We then hash vi and output a hash value si1 = SHA − 256(vi); we also hash ordered strings stri and output another hash value si2 = SHA − 256(stri). We then hash the ordered si1 and si2.

2. For each non-leaf node i with child nodes ic, we simply take in all ordered hash values of its child’s nodes sic and output si = SHA − 256(sic). Likewise, the length of the vector vi equals to the length of the vector of its child’s nodes, denoted li = ‖vic‖. We say each hash value si represents the signature of each folder and li represents the number of Android SDK APIs used of its class file. Again, we still preserve the tree-based structure, that is qualified name for each folder p, given TPL.

3. For each signature si of each folder, we try to add up all the APPs associated with s. We then extend the tuple to 〈s, l, p, v, a〉, where ai is the number of APPs associated with s. The value of ai is largely fluctuated by different versions of APPs containing commonly-used code, wherein each APP is characterized by the qualified name and the signature s.

# c o m p u t e r s & s e c u r i t y 80 (2019) 257 – 272
# 3. Tagging the TPL
To accommodate more meta-information into the TPL corpus with minimum human efforts, we tag the TPL as follows:
1. We apply breadth-first search (BFS) algorithm to probe the qualified name. The search depth is determined by the size of pi for each node i. We prioritize the search by sorting ai in descending order, which guarantees us to pass the most popular TPL first.

2. We then probe the “Package Stem” by moving forward until li is changed or the current node i has multiple child nodes. Note that lower-level features or signatures can be polluted by the mixture of TPLs. We de facto probe from the lower-level of the hierarchical tree to the “Package Stem”.

3. We build a dependency graph for the “Package Stem” and use NetworkX  to identify cycles in the PkgDG, attempting to merge “Package Stem”. That is, a TPL is a mixture of completely distinct “Package Stem”.

4. We tag the TPL manually, with the probed “Package Stem”. We store the current representation as 〈s, p, v, d〉 for a given TPL, where di is the semantic description and pi is the ultimate “Package Stem” of the TPL.