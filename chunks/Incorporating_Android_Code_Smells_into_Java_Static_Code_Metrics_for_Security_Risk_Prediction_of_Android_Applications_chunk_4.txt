As explained in Section III-B, to better explore the prediction ability of different classification algorithms, we further examined the ROC curves of each prediction model in Figure 4. There are two kinds of ROC curves for multi-classification problems. In the first case, for each category, we can calculate the probability that the test samples belong to that category. Then, for n categories, we can draw corresponding n ROC curves, and get the average value of n ROC curves. In the second case, for each test sample, the label consists of only 0 or 1, 1 indicating its category, 0 indicating other categories. In order to evaluate ROC curves better, we invoke the sklearn.metrics.roc auc score() function to calculate the AUC value in Python. The first case corresponds to the parameter average= ”macro”, and the second case corresponds to the parameter average= ”micro”. In addition, these Figures also show ROC curves for each category (i.e., low, medium, and high risk level).

As shown in Figure 4, We described the ROC curves corresponding to each risk level and the ROC curves under multi-classification problems. In addition, the area under the ROC curve (AUC) is calculated separately. From Figure 4, we can observe that our proposed techniques have high predictive power for Android applications with high-risk and low-risk levels. The reason might be that the numbers of Android applications at those two levels are relatively larger, which makes the model more fully trained. Another observation is that RF (random forest) and GBDT (gradient boosting decision tree) have better prediction performance compared with other algorithms -- the ROU of RF and GBDT are 0 and 0, respectively (we used the scikit-learn Python library to build RF and GBDT models with default configuration settings in the paper). Both GBDT and RF are classic ensemble models that produce several decision trees at training time. Compared to a single decision tree, such an ensemble method could greatly reduce the over-fit problem caused by a single decision tree –, thus further improved the prediction performance in practice.

# Answer to RQ2:
Android code smells could achieve competitive prediction performance compared to Java static code metrics; and they are complementary to each other in predicting the security of an Android application (By combining these two kinds of features, we could obtain an effective prediction model with the precision of 0, recall of 0, accuracy of 0, and F1-score of 0). Among nine typical machine learning methods, Random Forest (RF) could outperform other classification algorithms by obtaining an AUC of 0.

# C. Results for RQ3
In this paper, we used a gradient boosting machine to identify important features in predicting the security risk level of an Android application. For a boosted tree, an average reduction of Gini impurity5 for each feature is calculated and is further used to represent the importance of the feature. Figure 5 shows the importance of the Java static code metrics and Android code smells in security risk level prediction models. From Figure 5, We can find that 30 features contributed as much as 99% of the cumulative importance while 6 features did not contribute to cumulative importance (these 6 features were inefficient data structure (IDS), no low memory resolver (NLMR), slow loop (SL), debuggable release (DR), unclosed closable (UC) and inefficient SQL query (ISQLQ)). Among those features which contributed to cumulative importance, we can find that metrics including bad violations, lines, member ignoring method (MIM), leaking inner class (LIC) and so
5 https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity
# TABLE IV
# EFFECTIVENESS OF JAVA CODE STATIC METRICS AND ANDROID CODE SMELLS IN PREDICTING THE SECURITY RISK LEVEL OF ANDROID APPLICATIONS
on have a relatively large influence on the performance of prediction models.

Compared to Java static code metrics, we can observe that although the importance ranks of Android code smells are not highest, the absolute values of the importance of code smells are just slightly smaller than those of Java code static metrics. This indicated the competitive importance of these two kinds of features in helping to predict the security risk level of an Android application.

# Answer to RQ3:
Member ignoring method (MIM) and leaking inner class (LIC) have a relatively large influence on Android security risk prediction; developers should pay more attention to avoid these code smells in their application development.

# D. Threats to Validity
Construct Validity. In order to use Android-specific code smells to help predict the security risk level of an application, we should first define what Android code smells are and what code smells to use. As different practitioners may have different opinions about the definitions of Android code smells, in
# The ROC Curve
# (a) Naive Bayes
# (b) k-Nearest Neighbor
# (c) Logistic Regression
# (d) Random Forest
# (e) Decision Tree
# (f) Support Vector Machine
# (g) Gradient Boosting
# (h) Multi-Layer Perceptron
# (i) Convolutional Neural Networks
# Feature Importance
Internal Validity. In this paper, we used two widely-used tools SonarQube and aDoctor to extract Java static code metrics and Android-specific code smells. Some unnoticeable errors introduced by these two tools may to some extent threaten our results. Besides, we used Androrisk to help label an application with a security risk level. To ensure that our labels are reasonable, we manually checked Android files based on the results of Androrisk. However, we were not able to check all Android files and some other unknown factors may also affect the security of an application. We cannot claim that our labels are 100 percent correct.

External Validity. In our study, all applications used are from Github. We cannot guarantee that our conclusions apply.

Authorized licensed use limited to: Pontificia Universidad Javeriana. Downloaded on August 12, 2024 at 03:27:06 UTC from IEEE Xplore. Restrictions apply.

to other OSS platforms or industrial communities. However, considering that our applications are from various domains, and are of different scales, we believe that our study still projected some valuable insights about code smells and security risk of applications.

# V. RELATED WORK
Many studies have been conducted to explore Android vulnerability, including permission mechanisms, system security, application security, and privacy. Gibler et al.  developed a tool called AndroidLeaks to detect security information leakage problems. Chess et al.  proposed an approach to reveal security flaws in source code. Wu et al.  provided a static analyst paradigm to detect Android malware. Kumar et al.  also built a model based on GIST features to detect Android malware. Bose et al.  trained Support Vector Machine (SVM) classifiers to distinguish malicious behaviors from normal behaviors of applications. Shabtai et al.  presented a behavior-based detection framework for Android-powered mobile devices. The authors continuously monitored various features and events from mobile devices and then applied machine learning algorithms to classify data as normal (benign) or abnormal (malicious). In this paper, our focus is on one aspect of Android vulnerability, i.e., the application security problem. We tried to extract useful features from static analysis to predict the security risk level of an Android application.

Androrisk  is most related to our work. Androrisk  is a tool that aims to give risk scores to Android applications using fuzzy logic. Within Androrisk, more sensitive permissions (i.e. access to the location, SMS messages, or payment systems) and functionalities which are more dangerous (i.e. shared libraries, use of cryptographic functions, the reflection API) are assigned with higher risk values. One problem with Androrisk is that it could neither provide accurate risk scores nor provide effective suggestions for developers to reduce security risks of applications, since most applications with low-security scores (measured by Androrisk) by using some sensitive permissions and dangerous functionality are actually not malicious. In this paper, we introduced the Android code smells to compensate for the shortcomings of Androrisk. Android code smells are closely related to the security of an application and could be eliminated by code refactoring. Our focus is to provide code modification suggestions for developers when their application is at high risk, so as to help them develop much safer Android applications.

Existing studies have found that various kinds of software metrics and code smells are related to software quality –. For example, software metrics (including object-oriented (OO) metrics , change metrics , , etc.) have been widely used for fault prediction . And code smells (which were first proposed by Holschuh et al. ) were found to be effective in defect prediction on Java projects. Related to Android applications, Mannan et al.  studied some Android-specific code smells; they found that the distribution of code smells in Android applications were different from that of Java desktop applications. Hecht et al. ,  found that Android code smells occurred more frequently than other code smells. Currently, Android code smells have not been explored in security risk prediction tasks. We are the first to consider exploiting Android code smells to facilitate security risk level prediction of Android applications.