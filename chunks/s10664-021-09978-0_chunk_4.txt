# RQ2
To answer RQ2, we compared the vulnerability-proneness levels of apps having different app success, using a methodology similar to the one adopted by Linares (Vásquez et al. 2013). The dependent variable for this research question is represented by the success of the considered apps. For estimating app success, as in previous studies, we make use of two proxy values—i.e., the average rating and the number of downloads—as they could be easily gathered from the Google Play store webpage. The independent variable is the number of potential security defects exhibited by the different apps.

Concerning the rating, we clustered the apps in four different groups based on their average rating values. To achieve this goal and assign each app to one of the groups, we computed the quartiles of the distribution of average rating values assigned to the apps. In particular, given ra the average user rating, the four sets are: (i) group1 = [apps having ra ≤ 1st Quart.]; (ii) group2 = [apps having 1st Quart. < ra ≤ Median]; (iii) group3 = [apps having Median < ra ≤ 3rd Quart.]; and (iv) group4 = [apps having ra > 3rd Quart.]. Similarly, we clustered the apps in three different groups based on their install class, as Google Play does not provide the exact number of downloads for apps. In particular, the three sets are: (i) set1 = [apps having up to 1,000,000 downloads] (i.e., up to 1M); (ii) set2 = [apps having from 1,000,001 to 50,000,000 downloads] (i.e., up to 50M); and (iii) set3 = [apps having more than 50,000,000 downloads] (i.e., more than 50M). We tested the following null hypothesis:
H02: There are no significant differences between the vulnerability-proneness levels of apps belonging to different success groups.

More specifically, we compared the distributions of the vulnerability-proneness levels associated with the different rating groups, and the vulnerability-proneness levels associated with the different download groups, through the Kruskal-Wallis test and subsequent Mann-Whitney pairwise comparison (with the Holm’s p-value correction procedure and α = 0). In addition, we estimated the magnitude of the statistically significant differences, through the Cliff’s delta. To investigate whether specific results could be observed among the different app categories, we repeated the analysis for each of the app categories in our dataset.

We observed that the apps in our dataset strongly vary in size (i.e., from apps having less than 10 classes to apps having tens of thousands of classes). To account for any potential bias that the difference of app sizes may introduce, beyond evaluating the number of cumulative warnings obtained for each app, for both RQ1 and RQ2 we also proceeded to a normalized analysis. Specifically, for each app, we divided the number of vulnerability warnings signaled by AndroBugs by the number of classes. This allows us to estimate the vulnerability-proneness density of each app.

# RQ3
To answer RQ3 we used app-related information collected from the Google Play store to (i) train a set of machine learning (ML) classifiers and (ii) evaluate the extent to which they were able to identify the vulnerability proneness levels of apps. In particular, to have a balanced dataset, after computing the median value, Mvulns, of the distribution of the vulnerability-proneness values associated with the apps in our dataset, we assigned to each
app the label (i) low, if its vulnerability-proneness level was lower (or equal) than Mvulns, or (ii) high, if its vulnerability-proneness level was higher than Mvulns. These labels represented our ground truth for computing the prediction performance. By relying on the Weka tool6, three different ML algorithms—namely the Naive Bayes, J48, and Random Forest—were trained to predict if the vulnerability-proneness level of an app should be marked as either low or high. The selection of these specific algorithms is not random, as they have been successfully adopted in previous work concerning defect prediction tasks (Giger et al. 2012; Kaur and Kaur 2014).

# Extraction and pre-processing of ML features
To train such ML algorithms, we considered 13 features provided by the app market concerning five different aspects that might be correlated with the vulnerability-proneness of apps. The meaning and rationale of each selected feature is described in Table 2. It is worth noticing that all the features reported in Table 2 can be gathered by simply scraping Google Play. Most of these features are of (i) numeric (i.e., Description, Length, Photos, APK Size, Average Rating, Number of Raters and Number of Permissions), (ii) nominal (i.e., Play Store Category and Number of Installs), or (iii) boolean (i.e., Contains Ads, In App Purchase, and each specific Permission in the list) types. Note that, while the relationships between the individual features in Table 2 and security risks have been extensively studied in prior research (Watanabe et al. 2017; Krutz et al. 2016; Felt et al. 2011a; Yeom and Won 2019; Yang et al. 2017), we leverage findings from these previous studies for selecting significant features to train machine learning algorithms.

# Textual features
However, Name and Description features involve text contents that must be properly preprocessed to be used as features to train the ML models. Previous work demonstrated that natural language texts could be profitably treated (through well-known text analysis techniques aimed at extracting relevant textual features from them) and used for issue/vulnerability classification (or prediction) tasks . In our context, for each considered app, the Name and the Description features have been concatenated. The resulting strings have been then used as an information base to build a textual corpus (i.e., bag of words composing the concatenated text elements), that was pre-processed by applying stop-word removal (using the English Standard Stop-word list) and stemming (i.e., English Snowball Stemmer) (Baeza-Yates et al. 1999). The usage of a stemming approach is not random, since it was successfully leveraged to reduce the number of text features for the ML problems in the mobile context . In future work we plan to investigate the effect of using lemmatization approaches in our results. As result of this process, a set of textual features (i.e., words) with relevance values (i.e., the frequency of the words in the app textual corpus) is associated to each app. In addition to these text features, we also computed, for each app, the n-grams— with [n ∈ [2 − 4]]— appearing in the aforementioned textual corpus to preserve word locality information. The rational of using n-grams is due to the fact that app functionalities are usually described by groups of 2,3 or 4 words rather than single words . Thus, each resulting text feature (i.e., word or n-gram) is weighted using the tf-idf weighting score (Baeza-Yates et al. 1999).

6 https://www.cs.waikato.ac.nz/ml/weka/
# Empir Software Eng (2021) 26: 78
# Empir Software Eng (2021) 26: 78
Static analysis features Previous work (Scandariato and Walden 2012) demonstrated that code metrics can be used to predict Android vulnerabilities. Thus, we also used widely-known static analysis tools to extract a set of code-related metrics, for two main reasons:
- to compare our results with a baseline approach and verify whether machine learning models trained with information extracted from the app store could achieve similar results to the ones obtained by measuring code-related features. If similar results would be achieved with the two approaches, using app store metrics as a proxy for estimating the vulnerability-proneness level of an app would be a clear advantage since there would be no need to download and inspect the app’s code;
- to better understand whether app-related details extracted from the Google store could provide complementary information to the one provided by code-related metrics. In particular, we aim at verifying if the details provided by the app store could be leveraged for improving the results achieved by approaches exclusively based on code-related information.

In particular, as third-party libraries often represent carriers for app vulnerabilities , we used LibRadar  to extract the number of third-party libraries used by each application in our dataset. In addition, we used the apk-parser tool 7 for extracting the following features from each apk files: (i) the minimum API Level required for the application to run (min sdk), (ii) the specific API Level that the application targets (target sdk), (iii) the number of classes (classes), (iv) the number of packages (packages), (v) the number of interfaces (interfaces), (vi) the number of annotated classes (annotations), and (vii) the number of public classes (public classes). While the numbers of classes of different types determine the size of an app, which plays an important role in determining the security of the app (i.e., more classes mean a larger attack surface (Alenezi and Almomani 2018)), the API Levels indicators are strictly connected with the app security.

# Grouping of features
For convenience, the different extracted and pre-processed features have been grouped as follows:
- Market metrics: comprising app market features without considering the app Description, the app Name and the resulting textual features.

- Textual features: comprising all the words and n-grams derived from the text processing steps described above.