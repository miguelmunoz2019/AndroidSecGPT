# 6 THREATS TO VALIDITY
We carefully designed and conducted our study. Nevertheless, some threats to validity may have harmed our study results and discussions. We discuss below some major threats and their respective treatments. We divided into construct, internal, external, and conclusion, well-known categories of validity threats.

# Construct validity
In our study, the first threat to construct validity is related to the choice of the subject applications and metrics. We opt to select 8 applications to favor external validity as we discuss below. The use of Ochiai coefficient for measuring the suspiciousness score may not properly capture how SBFL perform in mobile applications. However, we believe that the Ochiai have promise results since it is among the metrics with the best performance.

Another threat concerns the use of artificial faults from a mutation tool. Although some studies have shown limitations in using artificial defects in experiments comparing SBFL techniques , the use of datasets with real defects also has limitations . Our study has a different scope because we do not intend to compare the performance of the SBFL techniques, but rather to look for evidence of the feasibility of using SBFL in the context of mobile applications.

# Internal validity
We use only three mutation operators. Despite the existence of recent studies discussing specialized mutation operators for mobile applications , we believe that the mutation operators used in this study are a representative subset of the traditional five-operator set . Moreover, traditional operators are associated with a significant proportion of real-world application failures as evidenced by the study of Escobar-Velasquez and others.

# External validity
We conducted our study using 8 open-source Android applications. We believe that these applications are representative of the target population for the experimental study, as they were randomly sampled from GitHub public repositories. To mitigate the impact of the representativeness of the Android applications selected for our study, we choose applications from different categories, sizes, and test suite size (ranging from 525 LOC to 3,674 LOC). Another threat to our study is the quality of the test suite for the selected applications. Our entire analysis depends on the test suite’s ability to detect failures. For instance, the study of Heiden et al.  suggests that the accuracy of SBFL is affected by the number of failed test cases. To mitigate the effect of an incomplete test suite, we limited our analysis to applications with a test suite of at least 500 LOC. Furthermore, we limited our analysis to mobile applications developed in Java or mixing Java and Kotlin. Therefore, we cannot generalize to other programming languages and frameworks such as Flutter and React Native.

# Conclusion validity
These results reflect our perceptions and interpretations of the metrics collected from the applications after execution of the testing strategies. All authors participated in data analysis and discussions of key findings to reduce bias from any one person’s interpretation. Nevertheless, we believe that one would get similar results using other metrics and tools that quantify similar attributes for the same mobile applications.

10 https://github.com/commons-app/apps-android-commons/pull/1791
# Applying Spectrum-Based Fault Localization to Android Applications
# SBES 2023, September 25–29, 2023, Campo Grande, Brazil
# 7 RELATED WORK
Several studies investigate failures in mobile applications on different aspects, such as exception tracking , automatic debugging , and setting-related defects . However, we investigated mobile application failures through the SBFL to better understand resource-related failures in mobile applications.

Su et al.  extensively studied to track failures through unique exceptions from 2,486 open-source and 3,230 commercial Android applications. Also, they conducted an online survey of 135 professional application developers to understand how developers handle exceptions. While they manually investigated failures reported by unique exceptions, we used SBFL to identify suspicious faulty program elements related to a software failure. Through their survey, they observed that developers use tools for fault detection. However, they still have many limitations, such as insufficient bug detection. Our results indicate that SBFL can rank more than 75% of the faulty code in 6 out of 8 applications. In addition, they demonstrate that manually tracking failures is a very costly activity. However, with our exploratory study, it was possible to identify that using SBFL for mobile applications is a viable and effective strategy.

Win et al.  describe that debugging and testing Android applications is more challenging than traditional Java programs. In order to reduce costs with testing and debugging Android applications, their work proposes an automatic debugging technique for Android applications called "Event-aware Precise Dynamic Slicing" (EPDS). This technique is based on "dynamic slicing," which is a technique that reduces the scope of program execution to a relevant subset of instructions. Their experiment evaluated the performance of EPDS compared to other techniques for automatically debugging Android applications. Their results showed that EPDS could significantly reduce program scope compared to the other techniques, making it easier for software developers to identify and correct application errors. While they are concerned with reducing the program’s scope, we focus on observing the interactions of resources and using SBFL. We seek to indicate the part of the code that is more prone to failure. Furthermore, this indicates that parts are more prone to failures and thus indicates parts of the code where the tests should be concentrated. Our results indicate that the SBFL can perceive resource interaction failures and artificially inserted failures.

Sun et al.  proposes an approach to identify and correct defects in Android applications related to system settings, they addressed a problem similar to the one we investigated. In their work, system settings can include screen orientation, screen brightness, and system language, among other settings that can be defined by the user (enabling or disabling) or during the usage of applications. While in our work, we also use resources such as Wi-Fi, Bluetooth, and Location. Their work describes an experiment to evaluate the proposed approach’s effectiveness in identifying defects related to system settings in Android applications. Their results showed that the proposed approach could identify several defects not found by other testing techniques, making it a valuable tool for Android application developers. In our work, we insert artificial faults and use instrumentation to activate and deactivate resources through informed configuration.

Marinho et al.  evaluated five sampling strategies in the context of resource-related failures of mobile applications. They generated and analyzed settings for the selected sampling testing strategies: Random, One-Enabled, One-Disabled, Most-Enabled-Disabled, and Pairwise. They observed that the Random strategy found better results concerning settings with failures. We also did a study to understand failures in mobile applications considering the 14 resources used in their work. In addition, we focused on analyzing SBFL to better understand resource-related failures in mobile applications. Finally, they commented on the challenges and difficulties in testing mobile applications concerning verifying failures arising from resource interactions. For example, they observed some challenges: the need for tooling support, the instrumentation of the test suite, and the time needed to test different settings. In order to overcome these challenges, we use SBFL as a technique that can support and reduce testing costs for mobile applications while considering the resources related to failures.

# 8 CONCLUSION
In this paper, we evaluated the use of SBFL aiming to locate faults in 8 Android applications and evaluate the sensitivity to resource interactions. We used faults seeded from a subset of mutation operators aiming to conduct the experimental study. Moreover, we used the Ochiai coefficient for calculating the suspiciousness score.

As a result, SBFL is able to rank more than 75% of the faulty code in 6 out of 8 applications. We found a major influence of resource settings on the suspiciousness score. That is, for the same failure (i.e., mutant), the ranking of suspicious methods varies depending on the combination of enabled resources (e.g., Wi-Fi and GPS). Therefore, we believe that SBFL is a promising technique that should be used in further studies to characterize the faults behind resource interaction failures.

As future work, we suggest the expansion of the experimental study to include applications implemented in other languages and frameworks, such as Kotlin, Flutter and React Native. Moreover, we can investigate specific mutation operators of mobile applications . Another direction is the use of fault localization families  and empirical studies of the SBFL using real faults.

# AVAILABILITY OF ARTIFACTS
We make our data publicly available for further investigations on a GitHub repository 11.

# SBES 2023, September 25–29, 2023, Campo Grande, Brazil.