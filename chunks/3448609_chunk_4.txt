Therefore, the primary goal of any security system is to enforce its model. For Android operating in a multitude of environments (see above for the threat model), this implies an approach that does not immediately fail when a single assumption is violated or a single implementation bug is found, even if the device is not up to date. Defense in depth is characterized by rendering individual vulnerabilities more difficult or impossible to exploit, and increasing the number of vulnerabilities required for an attacker to achieve their goals. We primarily adopt four common security strategies to prevent adversaries from bypassing the security model: isolation and containment (Section 4), exploit mitigation (Section 4), integrity (Section 4), and patching/updates (Section 4).

# Safe by design/default.

Components should be safe by design. That is, the default use of an operating system component or service should always protect security and privacy assumptions, potentially at the cost of blocking some use cases. This principle applies to modules, APIs, communication channels, and generally to interfaces of all kinds. When variants of such interfaces are offered for more flexibility (e.g., a second interface method with more parameters to override default behavior), these should be hard to abuse, either unintentionally or intentionally. Note that this architectural principle targets developers, which includes device manufacturers, but implicitly includes users in how security is designed and presented in user interfaces. Android targets a wide range of developers and intentionally keeps barriers to entry low for app development. Making it hard to abuse APIs not only guards against malicious adversaries, but also mitigates genuine errors resulting, e.g., from incomplete knowledge of an interface definition or caused by developers lacking experience in secure system design. As in the defense in depth approach, there is no single solution to making a system safe by design. Instead, this is considered a guiding principle for defining new interfaces and refining—or, when necessary, deprecating and removing—existing ones. For guarding user data, the basic strategies for supporting safety by default are: enforced consent (Section 4), user authentication (Section 4), and by-default encryption at rest (Section 4) and in transit (Section 4).

# 4 Enforcing Meaningful Consent
Methods of giving meaningful consent vary greatly between actors, as well as potential issues and constraints.

ACM Transactions on Privacy and Security, Vol. 24, No. 3, Article 19. Publication date: April 2021.

# The Android Platform Security Model
We use two examples to better describe the consent parties:
- Sharing data from one app to another requires:
- user consent through the user selecting a target app in the share dialog;
- developer consent of the source app by initiating the share with the data (e.g., image) they want to allow out of their app;
- developer consent of the target app by accepting the shared data; and
- platform consent by arbitrating the data access between different components and ensuring that the target app cannot access any other data than the explicitly shared item through the same link, which forms a temporary trust relationship between two apps.

- Changing mobile network operator (MNO) configuration option requires:
- user consent by selecting the options in a settings dialog;
- (MNO app) developer consent by implementing options to change these configuration items, potentially querying policy on backend systems; and
- platform consent by verifying, e.g., policies based on country regulations and ensuring that settings do not impact platform or network stability.

Actors consenting to any action must be empowered to base their decision on information about the action and its implications and must have meaningful ways to grant or deny this consent. This applies to both users and developers, although very different technical means of enforcing (lack of) consent apply. Consent is required not only from the actor that created a data item but from all involved actors. Consent decisions should be enforced and not self-policed, which can happen at runtime (often, but not always, through platform mediation) or build respectively distribution time (e.g., developers including or not including code in particular app versions).

# 4 Developer(s)
Unlike traditional desktop operating systems, Android ensures that the developer consents to actions on their app or their app’s data. This prevents large classes of abusive behavior where unrelated apps inject code into or access/leak data from other applications on a user’s device.

Consent for developers, unlike the user, is given via the code they sign and the system executes, uploading the app to an app store and agreeing to the associated terms of service, and obeying other relevant policies (such as CDD for code by an OEM in the system image). For example, an app can consent to the user sharing its data by providing a respective mechanism, e.g., based on OS sharing methods such as built-in implicit Intent resolution chooser dialogs . Another example is debugging: As assigned virtual memory content is controlled by the app, debugging from an external process is only allowed if an app consents to it (specifically through the debuggable flag in the app manifest). By uploading an app to the relevant app store, developers also provide the consent for this app to be installed on devices that fetch from that store under appropriate pre-conditions (e.g., after successful payment).

Meaningful consent then is ensuring that APIs and their behaviors are clear and the developer understands how their application is interacting with or providing data to other components. Additionally, we assume that developers of varying skill levels may not have a complete understanding of security nuances, and as a result APIs must also be safe by default and difficult to incorrectly use to avoid accidental security regressions. One example of a lesson learned in these regards is that early Android apps occasionally used meant-to-be-internal APIs for unsupported purposes and often in an insecure way. Android 9 introduced a major change by only supporting access to APIs explicitly listed as external (https://developer.android.com/reference/packages) and putting restrictions on others . Developer support was added, e.g., in the form of specific log messages to point out internal API usage for debuggable versions of apps. This has two main...

ACM Transactions on Privacy and Security, Vol. 24, No. 3, Article 19. Publication date: April 2021.

19:10 R. Mayrhofer et al.

benefits: (a) the attack surface is reduced, both toward the platform and apps that may rely on undefined and therefore changing internal behavior, and (b) refactoring of internal platform interfaces and components from one version to another is enabled with fewer app compatibility constraints.

To ensure that it is the app developer and not another party that is consenting, applications are signed by the developer. This prevents third parties—including the app store—from replacing or removing code or resources to change the app’s intended behavior. However, the app signing key is trusted implicitly upon first installation, so replacing or modifying apps in transit when a user first installs them (e.g., when initially side-loading apps) is currently out of scope of the platform security model. Previous Android versions relied on a single developer certificate that was trusted on initial install of an app and therefore made it impossible to change the underlying private key, e.g., in the case of the key having become insecure . Starting with Android 9, independently developed key rotation functionality was added with APK Signature Scheme v3  to support delegating the ability to sign to a new key by using a key that was previously granted this ability by the app using so-called proof-of-rotation structs. 9
These two examples (controlled access to internal Android platform components and developer signing key rotation) highlight that handling multi-party consent in a complex ecosystem is challenging even from the point of a single party: Some developers may wish for maximum flexibility (access to all internal components and arbitrarily complex key handling), but the majority tends to be overwhelmed by the complexity. As the ecosystem develops, changes are therefore necessary to react to lessons learned. In these examples, platform changes largely enabled backwards compatibility without changing (no impact when key rotation is not used by a developer) or breaking (most apps do not rely on internal APIs) existing apps. When changes for developers are necessary, these need to be deployed over a longer period to allow adaptation, typically with warnings in one Android release and enforced restrictions only in the next one.

# 4 The Platform.

While the platform, like the developer, consents via code signing, the goals are quite different: The platform acts to ensure that the system functions as intended. This includes enforcing regulatory or contractual requirements (e.g., communication in cell-based networks) as well as taking an opinionated stance on what kinds of behaviors are acceptable (e.g., mitigating apps from applying deceptive behavior toward users). Platform consent is enforced via Verified Boot (see below for details) protecting the system images from modification, internal compartmentalization and isolation between components, as well as platform applications using the platform signing key and associated permissions, much like applications.