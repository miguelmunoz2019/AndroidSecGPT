6. Local vs. Remote: Solutions are available as executables or as sources from which executables can be built. These solutions are installed and executed locally by app developers. Solutions are also available remotely as web services (or via web portals) maintained by solution developers. App developers use these services by submitting the APKs of their apps for analysis and later accessing the analysis reports. Unlike local solutions, app developers are not privy to what happens to their apps when using remote solutions.

# 4 Tools Selection
# 4 Shallow Selection
To select tools for this evaluation, we first screened the considered 64 solutions by reading their documentation and any available resources. We rejected five solutions because they lacked proper documentation, e.g., no documentation, lack of instructions to build and use tools. This elimination was necessary to eliminate human bias resulting from the effort involved in discovering how to build and use a solution, e.g., DroidLegacy [Deshotels et al., 2014], BlueSeal [Shen et al., 2014]. We rejected AppGuard [Backes SRT GmbH, 2014] because its documentation was not in English. We rejected six solutions such as Aquifer [Nadkarni et al., 2016], Aurasium [Xu et al., 2012], and FlaskDroid [Bugiel et al., 2013] as they were not intended to detect vulnerabilities, e.g., intended to enforce security policy.

Of the remaining 52 solutions, we selected solutions based on the first four dimensions mentioned in Section 4.

In terms of tools vs. frameworks, we were interested in solutions that could readily detect vulnerabilities with minimal adaptation, i.e., use it off-the-shelf instead of having to build an extension to detect a specific vulnerability. The rationale was to eliminate human bias and errors involved in identifying, creating, and using the appropriate adaptations. Also, we wanted to mimic a simple developer workflow: procure/build the tool based on APIs it tackles and the APIs used in an app, follow its documentation, and apply it to the app. Consequently, we rejected 16 tools that only enabled security analysis, e.g., Drozer [MWR Labs, 2012], ConDroid [Anand et al., 2012]. When a framework provided pre-packaged extensions to detect vulnerabilities, we selected such frameworks and treated each such extension as a distinct tool, e.g., we selected Amandroid framework as it comes with seven pre-packaged vulnerability detection extensions (i.e., data leakage (Amandroid1), intent injection (Amandroid2), comm leakage (Amandroid3), password tracking (Amandroid4), OAuth tracking (Amandroid5), SSL misuse (Amandroid6), and crypto misuse (Amandroid7)) that can each be used as a separate off-the-shelf tool (Amandroidx) [Wei et al., 2014].

In terms of free vs. commercial, we rejected AppRay as it was a commercial solution [App-Ray, 2015]. While AppCritique was a commercial solution, a feature-limited version of it was available for free. We decided to evaluate the free version and did not reject AppCritique.

In terms of maintained vs. unmaintained, we focused on selecting only maintained tools. So, we rejected AndroWarn and ScanDroid tools as they were not updated after 2013 [Debize, 2012, Fuchs et al., 2009]. In a similar vein, since our focus was on currently supported Android API levels, we rejected TaintDroid as it supported only API levels 18 or below [Enck et al., 2010].

In terms of vulnerability detection and malicious behavior detection, we selected only vulnerability detection tools and rejected six malicious behavior detection tools.

Next, we focused on tools that could be applied as is without extensive configuration (or inputs). The rationale was to eliminate human bias and errors involved in identifying and using appropriate configurations. So, we rejected tools that required additional inputs to detect vulnerabilities. Specifically, we rejected Sparta as it required analyzed apps to be annotated with security types [Ernst et al., 2014].

Finally, we focused on the applicability of tools to Ghera benchmarks. We considered only tools that claimed to detect vulnerabilities stemming from APIs covered by at least one category in Ghera benchmarks. For such tools, based on our knowledge of Ghera benchmarks and shallow exploration of the tools, we assessed if the tools were indeed applicable to the benchmarks in the covered categories. The shallow exploration included checking if the APIs used in Ghera benchmarks were mentioned in any lists of APIs bundled with tools, e.g., the list of the information source and sink APIs bundled with HornDroid and FlowDroid. In this regard, we rejected 2 tools (and 1 extension): a) PScout [Au et al., 2012] focused on vulnerabilities related to over/under use of permissions and the only permission related benchmark in Ghera was not related to over/under use of permissions and b) LetterBomb [Garcia et al., 2017] and Amandroid’s OAuth tracking extension (Amandroid5) as they were not applicable to any Ghera benchmark.

# 4 Deep Selection
Of the remaining 23 tools, for tools that could be executed locally, we downloaded the latest official release of the tools, e.g., Amandroid.

If an official release was not available, then we downloaded the most recent version of the tool (executable or source code) from the master branch of its repository, e.g., AndroBugs. We then followed the tool’s documentation to build and set up the tool. If we encountered issues during this phase, then we tried to fix them; specifically, when issues were caused by dependency on older versions of other tools (e.g., HornDroid failed against real-world apps as it was using an older version of apktool, a decompiler for Android apps), incorrect documentation (e.g., documented path to the DIALDroid executable was incorrect), and incomplete documentation (e.g., IccTA’s documentation did not mention the versions of required dependencies [Li et al., 2015]). Our fixes were limited to being able to execute the tools and not to affect the behavior of the tool.

In the rest of this manuscript, for brevity, we say “a tool is applicable to a benchmark” if the tool is designed or claims to detect the vulnerability captured by the benchmark. Likewise, “a tool is applicable to a set of benchmarks” if the tool is applicable to at least one benchmark in the set. While Ghera did not have benchmarks that were applicable to some of the rejected tools at the time of this evaluation, it currently has such benchmarks that can be used in subsequent iterations of this evaluation.

We stopped trying to fix an issue and rejected a tool if we could not figure out a fix by interpreting the error messages and by exploring existing publicly available bug reports. This policy resulted in rejecting DidFail [Klieber et al., 2014].

Of the remaining tools, we tested 18 local tools using the benign apps from randomly selected Ghera benchmarks I1, I2, W8, and W910 and randomly selected apps Offer Up, Instagram, Microsoft Outlook, and My Fitness Pal’s Calorie Counter from Google Play store. We execute each tool with each of the above apps as input on a 16 core Linux machine with 64GB RAM and with 15 minute time out period. If a tool failed to execute successfully on all of these apps, then we rejected the tool. Explicitly, we rejected IccTA and SMV Hunter because they failed to process the test apps by throwing exceptions [Li et al., 2015, Sounthiraraj et al., 2014]. We rejected CuckooDroid and DroidSafe because they ran out of time or memory while processing the test apps [Gordon et al., 2015, Cuckoo, 2015].

For nine tools that were available only remotely, we tested them by submitting the above test apps for analysis. If a tool’s web service was unavailable, failed to process all of the test apps, or did not provide feedback within 30–60 minutes, then we rejected it. Consequently, we rejected four remote tools, e.g., TraceDroid [van der Veen and Rossow, 2013].

# 4 Selected Tools
# 4 Experiment
Every selected vulnerability detection tool (including pre-packaged extensions treated as tools) was applied in its default configuration to the benign app and the secure app separately of every applicable Ghera benchmark (given in column 9 in Table 5). To control for the influence of API level on the performance of tools, we used APKs that were built with minimum API level of 19 and target API level of 25, i.e., these APKs can be installed and executed with every API level from 19 thru 25.

We executed the tools on a 16 core Linux machine with 64GB RAM and with 15-minutes time out. For each execution, we recorded the execution time and any output reports, error traces, and stack traces. We then examined the output to determine the verdict and its veracity pertaining to the vulnerability.

# 4 Ensuring Fairness
Consider only supported versions of Android: FixDroid was not evaluated on secure apps in Ghera because Android Studio version 3 was required to build the secure apps in Ghera and FixDroid was available as a plugin only to Android Studio version 2. Further, since we added benchmarks C4, I13, I15, and S4 after Ghera migrated to Android Studio version 3, FixDroid was not evaluated on these benchmarks; hence, we evaluated FixDroid on only 38 Ghera benchmarks.