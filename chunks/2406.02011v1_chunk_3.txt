Although the early versions of the CVSS score have not been designed as a metric for risk estimation, over the years, the metric evolved to provide a reliable measure to evaluate the impact of vulnerabilities and exploits. One of the seminal works that pointed out the weaknesses of the previous versions of CVSS was the paper by Allodi and Massacci , who in 2014 criticised the usage of pure CVSS base score without considering the presence of exploits in the wild for a given vulnerability. The authors proposed a novel way to include known attacks by merging CVEs published in NVD and exploits released on exploit-db (repository of computer software exploits and exploitable vulnerabilities), eits (black markets exploits), and sym (vulnerabilities exploited in the wild). By employing this methodology, they could assess the limitations of the CVSS score first version and reduce the risk sensitivity according to the known exploits of 45%. The third version of CVSS (v3, the one used in this work for the experimental part) is more consistent. A brand new version has been released during this writing: CVSS v4 reinforces the concept of CVSS as not just a mere base score, as it considers threats, environments, attack requirements, user.

# 8 F. Author et al.

interactions and other metrics focused on a more real CVSS value, as suggested by Allodi and Massacci.

Other works addressed vulnerability detection. For example, Alves et al.  studied the correlation between software metrics and software vulnerabilities. The authors claim that metrics exist to identify bad software, which is also harder to verify and maintain, with unnoticed or inadvertently introduced vulnerabilities. The authors compiled 5750 vulnerabilities from Linux Kernel, Mozilla, Xen Hypervisor, httpd, and glibc. Analyzing 2875 security patches, they distinguished vulnerable and safe functions. The results emphasize early vulnerability management and the need for developers to use multiple metrics for predicting code vulnerabilities.

Even Madeiros et al.  addressed this topic with a study on software metrics useful to detect security vulnerabilities in software development. They analyzed various software metrics, such as complexity and coupling metrics, as well as other structural quality indicators, and identified patterns and correlations indicating the presence of security vulnerabilities. The authors established a correlation between specific project-level metrics and the number of vulnerabilities present in the software systems. They also found a specific group of discriminative metrics different across the software systems but present in all of them and valuable to distinguish between vulnerable and non-vulnerable code. The software metrics were identified using a genetics algorithm and a random forest classifier.

Instead, in , a method called MVP (Matching Vulnerabilities and Patches) has been presented to detect vulnerabilities using patch-enhanced vulnerability signatures with low false positive and false negative rates. This methodology can distinguish between already patched vulnerabilities and generate accurate vulnerability and patch signatures to improve vulnerability detection accuracy.

Du et al. developed LEOPARD , a framework in a lightweight approach to help security experts detect potentially vulnerable functions in a code base without prior knowledge of the known vulnerabilities. Leopard combines complexity and vulnerability metrics to identify potentially vulnerable functions, providing a more comprehensive vulnerability assessment. The vulnerability is detected at all levels of complexity without missing the low-complex ones. For this purpose, the authors used a binning-and-ranking approach, where functions are grouped into bins based on complexity metrics and then ranked within each bin using vulnerability metrics. The framework covers a substantial portion of vulnerable functions identified while only a fraction is flagged as potentially vulnerable, outperforming machine learning and static analysis methods.

Another interesting problem regarding vulnerability detection is reachability, i.e., analyzing whether or not a vulnerable function is called in the app during its execution. Borzachiello et al. proposed DroidReach , a tool to detect reachable APIs using heuristic and symbolic execution. They were able to represent all possible paths a function may take within the Inter-procedural Control-Flow Graph (ICFG), whose aim is to encode all paths starting from an application entry point. Due to the complex methodology introduced in their work and the high computational complexity, we did not implement it in our.

# Risk Algorithm Native Code Vulnerabilities Android
work but considered this case in the risk methodology. Instead, we highlighted the imported library issue in our work as they did.

Recently, Ruggia et al.  developed a new methodology to reverse engineer Android apps, focusing on identifying suspicious patterns related to native components. They used suspicious tags to train a Machine Learning algorithm for binary classification. In particular, they developed a static tool that analyzes the code blocks responsible for suspicious behaviours in detail. This work demonstrates the use of native code in malicious Android applications so that the analysis is more complex and the maliciousness is better achieved.

One of the first studies on Android Native Code exploits was proposed in 2013 by Fedler et al. . They introduced different techniques to provide various levels of protection against all known local root exploits without affecting the user experience. Their mitigation reduced the exploitability of Android devices. In those years, very few Android applications used Native Code, and their approach was unsuccessful in exploiting and targeting flaws in the Dalvik Virtual Machine. Nowadays, more applications use Native Code, and Android architecture has changed. It is also worth noting that even popular tools for Android APKs vulnerability detection, such as MobSF  and Qark , and SEBAS-TiAn  do not look for vulnerabilities in the Native Code, even if are good vulnerabilities detection tools.

Fuzzing is another technique to detect vulnerabilities, and recently, it has also been used in Android Native Code. One of the most popular fuzzers is AFL++, which has been adapted in Frida mode  to interact and fuzz Android applications: detect the vulnerability in the C/C++ code and also check the interaction with the Java/Kotlin code. Other tools have been released, such as Android-AFL  and Libfuzzer . All these tools are resource-consuming and sometimes could be more efficient in detecting. Different works  are focusing on fuzzing Android Native components, but, as far as we know at the moment of writing this paper, none of them focuses on fuzzing Android application Native Code.

Android CVE analysis has been studied by Brant et al. in  with a focus on the Android security bulletin within the last six years from 2022. According to them, to have more secure Android systems, security bulletin updates must be designed with specific tests and improve code coverage of patched files. Only 13% of security bulletin updates contain fixed test files for that particular update, and among these, only 42% has full patch coverage. Even if the percentage is still low, this is an interesting result, meaning that the community is beginning to address Android security and vulnerability detection.

LibRadar  demonstrated that a whitelist approach is inefficient because package names can be modified in many ways. For this reason, they released a detection tool based on stable code features that are obfuscation-resilient, such as APIs, which are also obfuscation-resilient. We tried to use their approach, but at the time of this writing, the tool was no more accessible. Another interesting approach is the one proposed by Li et al. , which identifies libraries according to reference and inheritance relations between Java classes, methods,
# 10
F. Author et al.

and other app metadata. Notably, a Native Library cannot be identified only according to Java interaction and inheritance, but specific C code syntaxes must be considered.

Other works such as GoingNative , NativeGuard  and AppCage  focused only on the isolation and secure sandboxing of Native Code in Android applications by running the app in a protected but unrealistic environment. These methodologies are fundamental but cannot be considered in a real-world scenario where customized sandboxes are rarely employed. On the contrary, Ndroid  and DroidNative  focused on data flow between Java and Native Code and Native Code control flow patterns, also for malware detection. Finally, AdDetect  is a framework for advertisement library detection using semantic analysis with machine learning and hierarchical clustering techniques. The interaction between Native Code and Java Code is critical and must be considered, even if we are limited to C code vulnerability detection in our work.

As Section 1 mentions, most of these works feature high time and space computational complexity. For example, using Machine Learning approaches, an extensive dataset of samples is needed to train the model correctly, and the fine-tuning of the model can introduce additional complexities. On the other hand, the methodologies above in the literature have a good accuracy in the results. Instead, the methodology we propose in this paper needs very few resources; it is fast to execute even with a large-scale dataset and does not need any dataset on which to train the Machine Learning algorithms.