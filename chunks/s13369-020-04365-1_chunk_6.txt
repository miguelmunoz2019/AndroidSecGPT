We compare the results of our approach with state-of-the-art approach . We tried to run PAPRIKA tool for comparison of our results but we cannot succeed to execute the
# Arabian Journal for Science and Engineering (2020) 45:3289–3315
MIM Member Ignoring Method, NLMR No Low Memory Resolver, IGS Internal Getter/Setter, IDFP Inefficient Data Format and Parser, IDS Inefficient Data Structure, LT Leaking Thread, SL Slow Loop, PD Public Data, DW Durable WakeLock, RAM Rigid Alarm Manager, UC Unclosed Closable, LIC Leaking Inner Class, DR Debuggable Release, SV Static Views, SC Static Context, SB Static Bitmap, CV Collection of Views, CB Collection of Bitmaps, DD Dropped Data, Unt Untouchable, UFO Uncontrolled Focus Order, NDUI Not Descriptive UI, NL Nested Layout, SCC Set Config Changes, OP Overdrawn Pixel, PA Presented Approach
Foga Easy Sound Recorder AmazeFileManager LeafPic Doctors SANAD Evolve Basketball
0 50 100 150 200 250 300 350
tool due to its configuration issues. The authors of PAPRIKA were requested for help but they said that the tool is still under development and upgradation. The manual comparison of results for each smell was very time-consuming and daunting task. We tried our best to compile manual results with great care. There is a wide disparity in the results of our approach as compared to approach . The reasons of disparity are different detection criteria applied by ADOC- TOR in the case of different Android code bad smells. The entry of “N/A” in Table 5 means that the tool is not able to detect that particular code bad smell and comparison is not possible.

# Arabian Journal for Science and Engineering (2020) 45:3289–3315
The results extracted from each selected Android application are presented in Table 5. Table 5 presents a comprehensive comparison of detected results extracted from ADOCTOR  and our approach on the same applications. It is visible from the results that “Member Ignoring Method” code smell has maximum occurrences in all selected applications. This information is very valuable for researchers working on refactoring of Android code smells. We also discover that our approach extracted more instances of this particular smell from all applications except “SANA” application as compared to ADOCTOR . The reason for this variation in results is due to different detection criteria applied by our approach. The approach adopted by ADOCTOR detects only one instance of “Member Ignoring Method” smell in each class but our approach detects all instances of this smell from one class.

It can be observed that the major difference in results is in the case of “Leaking Inner Class,” “No Low Memory Resolver,” “Internal Getter Setters,” “Slow for Loop” and “Unclosed Closable and Leaking Inner Class.” We analyzed the following reasons for the variations in the results of these bad smells extracted by our approach and ADOCTOR as given in Table 6.

The variation in results of other bad smells is due to the reason of weak detection strategy implemented by ADOCTOR . The ADOCTOR detects most of the bad smells by just comparing the text of the whole class with the definition of a bad smell. For example, ADOCTOR detects a “Leaking thread” bad smell by just searching a presence of text “run()” and absence of text “stop().” The ADOCTOR has no capability to find the call run() for an object of Thread class which is a true definition of Leaking thread bad smell. Similarly, the detection strategy for other bad smells implemented in ADOCTOR is very weak and volatile that leads to a large number of false positives in detected results. We extracted 114 instances of “Debuggable Release” bad smell from Evolve Basketball App using ADOCTOR tool. There can be only one instance of Debuggable Release code smell in one application according to the nature of this smell. This is a bug in ADOCTOR tool.

Due to the wide variation in the results of our approach as compared to ADOCTOR, we further analyzed the shared instances of bad smells. For this purpose, we compared the result of our approach from all examined applications on each smell with ADOCTOR. Table 7 presents the shared instances of bad smells extracted by both approaches. These shared instances are extracted automatically by comparing the results of both approaches. We implemented a separate class in our tool for this comparison because the manual comparison of shared instances was a very time-consuming task.

We can see from Table 7 that a number of bad smells have shared instances detected by both approaches but it is still not clear that which instances are exactly shared. In order to view extract shared instances, we went one step further to analyze shared instances of bad smells extracted by both approaches. Tables 8 and 9 present these exact shared instances with complete path of source code for Easy Sound Record and SAND applications, respectively. In order to curtail space, we mention only one shared instance of bad smells that are extracted more than once from same Java class. For example, 9 instances of “Internal Getter/Setter” bad smell are extracted by our approach from FileViewerAdapter.java class but we mention only one instance in Table 8. The rest of 8 instances are extracted by our approach from same class but they are not extracted by ADOCTOR.

We evaluated the accuracy of our approach by using well-established metrics such as precision, recall and F-measure given below:
Precision = TP∕(TP + FP)
Recall = TP∕(TP + FN)
F-Measure = 2 ∗ (Precision ∗ Recall)∕(Precision + Recall)
TP represents the number of true positive instances of smells detected by our approach and FP represents the number of wrongly identified instances of code smells. FN is the number of correct instances of smells not identified by our approach. There may be a trade-off between precision and recall metrics, and therefore, we measure the accuracy of our approach by using harmonic means of precision and recall known as F-measure. We concede that our approach has false positive and false negative instances detected in the case of few code smells as indicated in Table 10. For example, our approach is unable to detect “Member Ignoring Method” bad smell when the method has a local variable or parameters with the same name as the field within the body of the method. In such scenario, our approach assumes that the method is actually accessing and using the class field and it cannot differentiate between the local variable and the class field with the same name. As a result, our approach
# Arabian Journal for Science and Engineering (2020) 45:3289–3315
MIM Member Ignoring Method, SL Slow Loop, IGS Internal Getter/Setter, LIC Leaking Inner Class, NLMR No Low Memory Resolver, IDS Inefficient Data Structure, LT Leaking Thread, DW Durable WakeLock, RAM Rigid Alarm Manager, UC Unclosed Closable, DR Debuggable Release
# Arabian Journal for Science and Engineering (2020) 45:3289–3315
SS Shared Smells, PA Presented Approach, MIM Member Ignoring Method, NLMR No Low Memory Resolver, IGS Internal Getter/Setter, IDFP Inefficient Data Format and Parser, IDS Inefficient Data Structure, LT Leaking Thread, SL Slow Loop, PD Public Data, DW Durable WakeLock, RAM Rigid Alarm Manager, UC Unclosed Closable, LIC Leaking Inner Class, DR Debuggable Release
# Arabian Journal for Science and Engineering (2020) 45:3289–3315
# Arabian Journal for Science and Engineering (2020) 45:3289–3315
cannot detect “Member Ignoring Method” bad smell from examined applications and we get false negative result. Similarly, we get false positive result while detecting same smell when a method does not directly access the field. In contrary, the method accesses a field indirectly using getter method or any other method holding the value of a field.

One of the key challenges for measuring the accuracy of our approach is the unavailability of standard benchmark systems on the results of selected 25 Android smells. We selected three open source and one industrial application for measuring the accuracy of our approach. The manual analysis of source code was conducted by both authors and two master students for calculation of false negatives. Similarly, false positives are determined through comparison of detected instances with manual analysis of source code. It was a very time-consuming and daunting task. The authors cross-checked results of master students to ensure reliability in results to the best level.

In Table 10, precision, recall and F-measure for four applications, namely Easy Sound Recorder, LeafPic, SAND and AmazeFileManager, are presented. We calculated precision, recall and F-Measure metrics for these four applications. There are a few false positives and false negatives as shown in Table 10. The average precision, recall and F-Measure for four applications are presented in Fig. 4 and Table 10.