ICC is the primary communication channel within and across Android apps, hence has been considered a major attack surface in Android. However, our observations on ICC use during app executions indicate its continuous decrease in both benign apps and malware (Finding 4). A potential consequence of this trend is that securing ICC as the major means of security apps  is likely to have a diminishing return. Meanwhile, app testing focusing on covering ICCs might suffer from relatively low overall coverage. The finding that both newer benign apps and newer malware tended to exercise less ICCs than before implies that major attack surface might have been shifted to other points of the apps.

# H. Cai, X. Fu and A. Hamou-Lhadj
# Information and Software Technology 122 (2020) 106291
On the other hand, malware had noticeably higher frequency of exercising ICCs than benign apps did, suggesting ICCs might not be entirely dismissed as an attack surface in security defense in the near future. In addition, ICC-based malware detector should pay more attention to the possible manipulation of built-in apps by malware, given the steadily substantial portion of ICCs being external ones. However, since there were no consistent and substantial differences between benign app and malware in terms of data payloads in ICCs (Finding 5), whether the ICCs carry data or not might not be a good indicator of malicious behaviors. Meanwhile, the observation that data-carrying ICCs delivering the data less often via standard data fields can be utilized as a promising differentiator between benign apps and malware that has not yet been leveraged for malware detection. Thus, features characterizing such preferences (e.g., the percentage of ICCs that carry standard data over all exercised ICCs) can be part of the feature embeddings for apps in dynamic malware detector. Note that these ICC-based features are relative statistics (i.e., percentages), clearly different from those used in prior work that are based on concrete ICC Intent fields (e.g., ). Also, prior work in this line typically uses static approaches (e.g., parsing the Intent information from apps’ manifest files and/or source code), as opposed to our ICC-based features being purely dynamic.

# 6. On callbacks
Analysis of callbacks in Android apps has proven to be highly expensive . However, our dynamic evolution study revealed that callbacks were not very frequently invoked, neither in benign apps nor in malware (Finding 6). Thus, it would be cost-beneficial to fully track callback data/control flows for fine-grained security analyses and app testing. On the other hand, our finding that malware had increasing (while benign apps had decreasing) use of callbacks, especially significantly less frequent invocation of event handlers than in benign apps, suggest potential additional means for differentiating malware from benign apps. Thus, relevant features (e.g., the frequency of event-handling callbacks relative to total method calls) would strengthen the performance of a dynamic malware detector. Meanwhile, with fewer callback executions in benign apps, precise taint analysis is expected to scale better to newer benign apps than to old ones, given that callback analysis was known to be a major performance barrier in such analyses.

The steady dominance of callbacks for managing Activities among all lifecycle callbacks (Finding 7) implies the lasting merits of focusing on Activity callbacks in callback control flow analysis . However, lifecycle callback distribution over varied categories may not be useful for detecting malware, given the consistently similar ranking between the two groups over time. Therefore, relevant features characterizing this distribution should be avoided in the design of a dynamic malware detector. However, a much lower percentage of invocation of Content Provider callbacks could point to malware behaviors. Another good indicator of malware is the great closeness of ranks among all event-handler categories, and/or significantly higher ranks of callbacks for handling View and System status events (Finding 8).

Importantly, as we discuss the security implications of our empirical findings, there are two caveats to keep in mind. First, how effective the features learned from the study would be for sustainable malware analysis depends on how stable the evolutionary patterns in malware behaviors and their differences from benign behaviors will be in the future. Only when the patterns are reasonably stable, would the features effectively contribute to the long-span capabilities of the techniques built on those features. Second, the motivation of this paper is not to identify features once for all that can work for future effective security defense solutions forever, but rather to explore and demonstrate how to discover such features. Realistically, these features, albeit relatively more sustainable (than, for instance, features extracted from a set of apps in a time-agonistic manner), would expectedly become outdated after some time. Yet, the methodology might still be valid and applicable for discovering other, more discriminatory features for future newer malware and benign apps.

# 7. Threats to validity
# 7. Threats to internal validity
We used our toolkit to trace the sample apps studied and compute the dynamic measures that define the execution structure we focus on in this study. While it handles reflective calls and calls via exceptional control flows, this toolkit does not monitor native calls or calls buried in dynamically loaded code. Thus, the method calls and ICCs we analyzed could be incomplete. Moreover, although we have applied a non-trivial coverage threshold when selecting sample apps to make sure the app executions we studied reasonably represent a major portion of common app behaviors, the random test inputs from Monkey we used might have missed representative app execution paths. As a result, our characterization might have not always captured the typical execution structure of some sample apps, which could be more concerning for malware samples since malware is known to be able to hide their behaviors especially when being executed on an emulator. With a small set of malware, we performed our comparative study with a real device versus the emulator and did not see significant differences in terms of our dynamic measures.

Notably, our study focuses on structural characteristics of malware (and benign apps), rather than explicit security traits (e.g., access of sensitive data, known malicious instances/sequences of calls to suspicious functions, etc.). The metrics used in our characterization have shown to be able to capture the behavioral differences between malware and benign apps , despite the known evasiveness of malware and the impediments of varied obfuscation schemes adopted in both malware and benign apps. This implies that our metrics can implicitly capture the essence of app behaviors. Yet, to more convincingly reduce this threat, a more extensive verification would be required.

# 7. Threats to external validity
While we purposely managed to choose a sizable dataset for benign apps and malware for each year, our yearly benchmark suite size is small relative to the entire app population of each year. More importantly, it is not clear how our chosen benchmarks are representative of respective populations. Consequently, our evolution study results may not be generalizable to other apps, nor to the entire app population per year. Thus, the presented observations and findings are best interpretable with respect to the particular apps we chose and studied. Particularly, an additional threat to external validity comes from the uneven distribution of our yearly benign-app and malware set. We used substantially more samples of certain years (e.g., benign apps of year 2014) than others. Our rationale was that the total app population in the real-world more likely than not varies in size year by year as well, thus choosing the same numbers of samples for different years may not respect the reality. Meanwhile, this poses a validity threat because we do not know the accurate sheer totals of apps in each year, thus the unevenness of the distribution of our yearly datasets may not represent the plausible unevenness of the distribution of actual app population sizes across years in reality.

# 7. Threats to construct validity
Given our focus on evolutionary patterns rather than absolute values of dynamic measures, we derived our findings mainly on an average-case basis—we compared benign apps and malware in terms of the average values of the chosen dynamic measures. Although the standard deviations (not reported in paper) were generally small relative to the mean values, more thorough statistical analyses would be more desirable to corroborate our conclusions. In particular, the behaviors of malware (with respect to certain metrics) may have considerably different.

# H. Cai, X. Fu and A. Hamou-Lhadj
# Information and Software Technology 122 (2020) 106291
distribution from those of benign apps (with respect to the same metrics) while still having very similar summary statistics (e.g., the average metric values). Thus, our current study might have missed some behavioral differences between the two app groups. More in-depth data analysis (including that of data distribution) could have revealed even more interesting and important findings. These further examinations are indeed part of immediate future work following this study.