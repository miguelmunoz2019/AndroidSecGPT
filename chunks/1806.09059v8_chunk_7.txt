Observation 6: Based on the classification of the verdicts, 4 out of 14 tools detected none of the vulnerabilities captured in Ghera (“0” in the TP column in Table 5) considering all extensions of Amandroid as one tool. Even in case of tools that detected some of the vulnerabilities captured in Ghera, none of the tools individually detected more than 15 out of the 42 vulnerabilities; see the numbers in the TP column and the number of N’s under various categories in Table 5. This number suggests that in isolation, the current tools are very limited in their ability to detect known vulnerabilities captured in Ghera.

Observation 7: For 11 out of 14 tools, the number of false negatives was greater than 70% of the number of true negatives; see FN and TN columns in Table 5. This proximity between the number of false negatives and true negatives indicates a significant limitation in the tools' effectiveness.

# Benchmarks
Negatives and the number of true negatives suggests two possibilities: most tools prefer to report only valid vulnerabilities (i.e., be conservative) and most tools can only detect specific manifestations of vulnerabilities. Both these possibilities limit the effectiveness of tools in assisting developers to build secure apps.

Observation 8: Tools make claims about their ability to detect specific vulnerabilities or class of vulnerabilities. So, we examined such claims. For example, while both COVERT and DIALDroid claimed to detect vulnerabilities related to communicating apps, neither detected such vulnerabilities in any of the 33 Ghera benchmarks that contained a benign app and a malicious app. Also, while MalloDroid focuses solely on SSL/TLS related vulnerabilities, it did not detect any of the SSL vulnerabilities present in Ghera benchmarks. We observed similar failures with FixDroid. See numbers in # Applicable Benchmarks and TP columns for COVERT, DIALDroid, FixDroid, and MalloDroid in Table 5. These failures suggest that there is a gap between the claimed capabilities and the observed capabilities of tools that could lead to vulnerabilities in apps.

Observation 9: Different tools use different kinds of analysis under the hood to perform security analysis. Tools such as QARK, Marvin-SA, and AndroBugs rely on shallow analysis (e.g., searching for code smells/patterns) while tools such as Amandroid, FlowDroid, and HornDroid rely on deep analysis (e.g., data flow analysis); see H/E column in Table 2. Combining this information with the verdicts provided by the tools (see TP and FN columns in Table 5) provides the number of vulnerabilities (not) detected by shallow analysis and deep analysis across various categories as listed in Table 6. From this data, we observe tools that rely on deep analysis report fewer true positives and more false negatives than tools that rely on shallow analysis. We also observe tools that relied on shallow analysis detected all of the vulnerabilities detected by tools that relied on deep analysis.

Further, among the evaluated tools, most academic tools relied on deep analysis while most non-academic tools relied on shallow analysis; see H/E columns in Table 2 and tools marked with * in Table 5.

Open Questions 3 & 4: A possible reason for the poor performance of deep analysis tools could be they often depend on extra information about the analyzed app (e.g., a custom list of sources and sinks to be used in data flow analysis), and we did not provide such extra information in our evaluation. However, JAADAS was equally effective in both fast (intra-procedural analysis only) (JAADASH) and full (both intra- and inter-procedural analyses) (JAADASE) modes, i.e., true positives, false negatives, and true negatives remained unchanged across modes. Also, FixDroid was more effective than other deep analysis tools even while operating within an IDE; it was the fifth best tool in terms of the number of true positives. Clearly, in this evaluation, shallow analysis tools seem to outperform deep analysis tools. This observation raises two related questions: 3) are Android app security analysis tools that rely on deep analysis effective in detecting vulnerabilities in general? and 4) are the deep analysis techniques used in these tools well suited in general to detect vulnerabilities in Android apps? These questions are pertinent because Ghera benchmarks capture known vulnerabilities and the benchmarks are small/lean in complexity, features, and size (i.e., less than 1000 lines of developer created Java and XML files), and yet deep analysis tools failed to detect the vulnerabilities in these benchmarks.

Observation 10: Switching the focus to vulnerabilities, every vulnerability captured by Permission and System benchmarks were detected by some tool. However, no tool detected any of the two vulnerabilities.

captured by Networking benchmarks. Further, no tool detected 12 of 42 known vulnerabilities captured in Ghera (false negatives); see # Undetected row in Table 5. In other words, using all tools together is not sufficient to detect the known vulnerabilities captured in Ghera.

In line with the observation made in Section 4 based on Table 3 – most of the vulnerabilities captured in Ghera were discovered before 2016, most of the vulnerabilities (9 out of 12) not detected by any of the evaluated tools were discovered before 2016; see Table 7.

# Open Questions 5 & 6
Of the 42 vulnerabilities, 30 vulnerabilities were detected by 14 tools with no or minimal configuration, which is collectively impressive. Further, two questions are worth exploring: 5) with reasonable configuration effort, can the evaluated tools be configured to detect the undetected vulnerabilities? and 6) would the situation improve if vulnerability detection tools rejected during tools selection are also used to detect vulnerabilities?
# Observation 11
Of the 14 tools, 8 tools reported vulnerabilities that were not the focus of Ghera benchmarks; see Other column in Table 5. Upon manual examination of these benchmarks, we found none of the reported vulnerabilities in the benchmarks. Hence, with regards to vulnerabilities not captured in Ghera benchmarks, tools exhibit a high false positive rate.

# Observation 12
To understand the above observations, we considered the relevant APIs and security-related APIs from Ghera representativeness experiment described in Section 3. We compared the sets of relevant APIs used in benign apps (601 APIs) and secure apps (602 APIs). We found that 587 APIs were common to both sets while 14 and 15 APIs were unique to benign apps and secure apps, respectively. When we compared security-related APIs used in benign apps (117 APIs) and secure apps (108 APIs), 108 were common to both sets, and nine were unique to benign apps. These numbers suggest that the benign apps (real positives) and the secure apps (real negatives) are similar in terms of the APIs they use and different in terms of how they use APIs, i.e., different arguments/flags, control flow context. Therefore, tools should consider aspects beyond the presence of APIs to successfully identify the presence of vulnerabilities captured in Ghera.

# Observation 13
We partitioned the set of 601 relevant APIs and the set of 117 security-related APIs used in benign apps into three sets: 1) common APIs that appear in both true positive benign apps (flagged as vulnerable) and false negative benign apps (flagged as secure), 2) TP-only APIs that appear only in true positive benign apps, and 3) FN-only APIs that appear only in false negative benign apps. The sizes of these sets in order in each partition were 440, 108, and 53, and 60, 39, and 18, respectively. For both relevant and security-related APIs, the ratio of the number of TP-only APIs and the number of FN-only APIs is similar to the ratio of the number of true positives and the number of false negatives, i.e., 108/53 ≈ 39/18 ≈ 30/12. This relation suggests, to be more effective in detecting vulnerabilities captured in Ghera, tools should be extended to consider FN-only APIs. Since vulnerabilities will likely depend on the combination of FN-only APIs and common APIs, such extensions should also consider common APIs.

# Observation 14
We compared the sets of relevant APIs used in the 12 false negative benign apps (flagged as secure) and all of the secure apps (real negatives). While 491 APIs were common to both sets, only 2 APIs were unique to benign apps. In the case of security-related APIs, 77 APIs were common while only 1 API was unique to secure apps. Consequently, in terms of APIs, false negative benign apps are very similar to secure apps. Consequently, tools need to be more discerning to correctly identify benign apps in Ghera as vulnerable (i.e., reduce the number of false negatives) without incorrectly identifying secure apps as vulnerable (i.e., increase the number of false positives).

Besides drawing observations from raw numbers, we also drew observations based on evaluation measures. While precision and recall are commonly used evaluation measures, they are biased — “they ignore performance in correctly handling negative cases, they propagate the underlying marginal prevalences (real labels) and biases (predicted labels), and they fail to take account the chance level performance” by Powers . So, we used informedness and markedness, which are unbiased variants of recall and precision, respectively, as evaluation measures.