# Deep crawling to scrape inner-links
Since landing pages and inner pages of government domains collected in the previous step may contain links to other government sites, we perform a deep crawl to scrape links in the HTML page source, up to a depth of 4 levels. For this purpose, we use Hakrawler , that can find links in page source and the associated JavaScript files of crawled URLs. We randomize the URLs fed to Hakrawler to avoid generating a large amount of traffic to any particular web server hosting government sites. Hakrawler crawls only the web content hosted on government domains/sub-domains — i.e., it does not crawl any external websites (e.g., social media sites). For all links collected up to a depth of 4 levels, we filter out the following: links to common file extensions (e.g., docx, pdf, xls); links to social media websites; non-responsive links using curl ; domains not ending with known suffixes of government domains. After filtering, we obtained 15,214,100 URLs from 121,846 unique government domains. For each domain’s landing page, we use curl to save the page source, which is later used to extract Google Play store URLs of government apps.

# Complementing websites from Singanamalla et al.

We add 135,416 government websites from Singanamalla et al.  (collected using a different methodology including crowd sourcing via Amazon MTurk). After eliminating the overlaps, we had a total of 231,449 government websites, and we finally used 150,244 websites as the rest were unreachable (for various reasons, including unresponsive or unreachable servers). The top-10 countries with the highest number of websites in our dataset have a cumulative of 60% (90,047/150,244) — e.g., US (22,506, 15%), China (12,583, 8%), Bangladesh (12,258, 8%). We observe that 8% (12,873/150,244) domains of government websites make it into the Tranco  top-1M websites (cf. 9% in ). We manually verify our government website dataset (with a limited sample size of 100, selected randomly) to ensure false positives are eliminated. We summarize the regions and website counts in Appendix A.

# Government Android apps
Government apps do not follow a common package naming convention. Therefore, we look for URLs relating to Google Play store (i.e., https://play.google.com) in the page source of government URLs saved for each country. However, not all such Google Play store URLs point to government apps (some third-party apps are also linked). We run each Google Play URL with the curl command to fetch developer email, developer website and privacy policy website URLs. We label a Google Play URL as a government app URL in the following cases: (i) the developer email, or developer website/privacy policy URL contains .gov.; (ii) the developer website/privacy policy URL appears in the list of our Play store URLs (a total of 1566), we attempt to download the app using gplaydl . A significant number of Android apps failed to download as they are region-locked. In the end, we collected 1166 government Android apps from 71 countries. The top-10 countries with the highest number of government Android apps in our dataset have a cumulative of 641 (out of 1166, 55%) apps — e.g., India (95, 8%), Australia (92, 7%), Indonesia (91, 7%).

# 3 Measurement of trackers on govt. sites
We configure the OpenWPM  web privacy measurement framework to launch 15 parallel browser instances in headless mode. To simulate the first visit to a website, we clear the browser profile after each URL visit. We use two Azure VMs running Ubuntu server 18 LTS, 4vCPUs, 16GB RAM, 30GB SSD, and a physical machine running Ubuntu 18 LTS, Intel Core i7-9700K, 8GB RAM, 1TB HDD for our OpenWPM measurements between Nov. 5–9, 2020. A total of 150,244 websites were successfully crawled by OpenWPM (out of 231,449), and the rest (81,205) were unreachable during our crawl (e.g., website no longer exists, SSL/TLS errors, name resolution failure, disconnection by the remote-end, timeout).

The instrumented tracking metrics from OpenWPM which include HTTP request/response of both the landing page and associated third party scripts, third party cookies, fingerprinting API calls, call stack information of web requests, and DNS resolution information are saved to a SQLite database. The saved information in OpenWPM contains both stateful (i.e., scripts and cookies) and stateless (fingerprinting) forms of metrics. We then check the saved tracking scripts and cookies for third party domains; i.e., domains of scripts/cookies that do not match the domain of the government site that they are on. We also study the known tracking scripts to find techniques used for other purposes such as session replaying and web analytics (which also could directly aid user tracking).

In order to find the correlation between privacy regulations (i.e., GDPR , CCPA ) and tracking, we separately run OpenWPM with 444 California government websites (from a VPN in California), and 1942 European Union government websites (from a VPN in the Netherlands). Note that our initial OpenWPM measurements are not done using VPNs. Our OpenWPM automation does not interact with crawled government websites, e.g., to accept or reject the cookie banners on EU sites. Therefore, our automation does not accept cookie banners on sites crawled.

# 3 Malicious govt. and tracking domains
We scan domains of all known tracking scripts/cookies in government domains (150,244), and government domains with VirusTotal to check if any of these domains are labelled as malicious. Note that, at least in some cases, VirusTotal engines may misclassify or delay in updating domain categorization labels . Therefore, to improve our labelling, we also automatically collect and use domain categories (e.g., phishing, malicious, spam, and advertisements, as assigned by different anti-virus engines), and community comments in VirusTotal  (sometimes with links to detailed analysis).

We used the VirusTotal API to extract community comments — see https://developers.virustotal.com/v3/reference#comments. To analyze the community comments on government websites. Then for each of the government Google malicious behaviour, we matched them with pre-determined keywords (e.g., phishing).

# Et tu, Brute? Privacy Analysis of Government Websites and Mobile Apps
# 3 Android apps analysis
Tracking SDK detection. We use Mobile Security Framework (MobSF ) to find tracking SDKs embedded in government apps (via static analysis). We load each app to the MobSF server, scan it using the MobSF REST API, and download the JSON formatted results, which include known tracking SDKs, and strings with sensitive data and dangerous permissions  (e.g., camera, contacts, microphone, SMS, storage and location) used by the apps. We then use LiteRadar to find the purpose of the included tracking SDKs (e.g., Development Aid, Mobile Analytics). Finally, we store these results in a local database for our analysis.

Misconfigured Firebase database. Many Android apps, including government apps, use Google Firebase  (a widely used data store for mobile apps) to manage their backend infrastructure. However, due to possible misconfiguration, Android apps connected to Firebase database can be vulnerable (see e.g., ). Exposed data from Firebase vulnerabilities includes personally identifiable information (PII), private health information and plain text passwords . Firebase scanner  is used to find Firebase vulnerabilities of an app (if exists). We run the Firebase scanner  on each APK file, which identifies the vulnerable Firebase URLs; we then download the exposed data from the Firebase datastore URL5 and check for apparent sensitive and PII items, including: user/admin identifiers, passwords, email addresses, phone numbers. However, for ethical/legal considerations, we do not validate the leaked information (e.g., login to an app using the leaked user/admin credentials). Then we remove the downloaded Firebase datastore. We also promptly notify the developers of affected apps.

Dynamic analysis. We use a rooted Samsung S5 neo mobile phone with Android 7. We restrict only newly installed apps to proxy the traffic via mitmproxy  using ProxyDroid , to avoid collecting traffic from system and other apps. A mitmproxy root certificate is installed on the phone. We also installed mitmproxy on a separate desktop machine to collect and decrypt HTTPS traffic. Both the desktop machine and phone are connected to the same Wi-Fi network. We use adb  to automate the installation, launch, and uninstallation of the apps. We also use Monkey  with 5000 events (e.g., touch, slide, swipe, click) for each app. The network traffic is captured and stored in pcap files. We use the captured network traffic to determine sensitive information (e.g., device identifiers sent to trackers, leaked hardcoded user/admin credentials and API keys) sent to external entities. We close mitmproxy and uninstall that government app before moving to the next app.

Malicious domains and apps. We scan the APK files of 1166 government Android apps with VirusTotal. We also scan domains included in apps (as found in the network traffic) with VirusTotal. the privacy of users. In addition, we did not retain any data from exposed Firebase databases. We also reached out to the internal Research Ethics Unit of our University, and explained our experiments. They approved our methodology without requiring a full ethics evaluation. We also kept them informed about our findings and contact attempts with app developers.