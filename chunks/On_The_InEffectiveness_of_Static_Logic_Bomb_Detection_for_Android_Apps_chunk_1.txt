# On The (In)Effectiveness of Static Logic Bomb Detection for Android Apps
# Jordan Samhi and Alexandre Bartel
# Abstract
Android is present in more than 85% of mobile devices, making it a prime target for malware. Malicious code is becoming increasingly sophisticated and relies on logic bombs to hide itself from dynamic analysis. In this article, we perform a large scale study of TSOPEN, our open-source implementation of the state-of-the-art static logic bomb scanner TRIGGERSCOPE, on more than 500k Android applications. Results indicate that the approach scales. Moreover, we investigate the discrepancies and show that the approach can reach a very low false-positive rate, 0%, but at a particular cost, e.g., removing 90% of sensitive methods. Therefore, it might not be realistic to rely on such an approach to automatically detect all logic bombs in large datasets. However, it could be used to speed up the location of malicious code, for instance, while reverse engineering applications. We also present TRIGDB a database of 68 Android applications containing trigger-based behavior as a ground-truth to the research community.

# Index Terms
Logic bombs, trigger analysis, static analysis, android applications security
# 1 INTRODUCTION
ANDROID is the most popular mobile operating system with more than 85% of the market share in 2020 , which undeniably makes it a target of choice for attackers. Fortunately, Google set up different solutions to secure access for applications in their Google Play. It ranges from fully-automated programs using state-of-the-art technologies (e.g., Google Play Protect ) to manual reviews of randomly selected applications. The predominant opinion is that the Google Play market is considered relatively malware-free. However, as automated techniques are not entirely reliable and manually reviewing every submitted application is not possible, they continuously improve their solutions’ precision and continue to analyze already-present-in-store applications.

Consequently, the main challenge for attackers is to build malicious applications that remain under the radar of automated techniques. For this purpose, they can obfuscate the code to make the analysis more difficult. Example of obfuscation includes code manipulation techniques , use of dynamic code loading  or use of the Java reflection API . Attackers can also use other techniques such as packing  which relies on encryption to hide their malicious code. In this paper, we focus on one type of evasion technique based on logic bombs. A logic bomb is code logic which executes malicious code only when particular conditions are met.

A classic example would be malicious code triggered only if the application is not running in a sandboxed environment or after a hard-coded date, making it invisible for dynamic analyses. This behavior shows how simple code logic can defeat most dynamic analyses leading to undetected malicious applications.

In the last decade, researchers have developed multiple tools to help detecting logic bombs , , . Most of them are either not fully automated, not generic or have a low recall. However, one approach, TRIGGERSCOPE  stands out because it is fully automated and has a false positive rate close to 0%. In this paper, we try to replicate TRIGGERSCOPE and perform a large-scale study to show that the approach scales. Furthermore, we identify specific parameters that have a direct impact on the false positive rate.

As TRIGGERSCOPE is not publicly available and the authors cannot share the tool, we implement their approach as an open-source version called TSOPEN. Although TSOPEN has been implemented by faithfully following the details of the approach given in TRIGGERSCOPE paper, we did not use the same programming language, i.e., C++. We used Java to reuse publicly available and well tested state-of-the-art solutions. Indeed, our solution relies on the so-called Soot framework  to convert the Java bytecode into an intermediate representation called Jimple  and to automatically construct control flow graphs. Also, to model the Android framework, the life-cycle of each component and the inter-component communication, TSOPEN relies on algorithms from FlowDroid.

We use TSOPEN to conduct a large-scale analysis to see if such a static approach is scalable. We ran TSOPEN over a set of 508122 applications from a well-known database of Android applications named ANDROZOO . This experiment shows that the approach is scalable but yields a high false-positive rate. Hence, because of this high false-positive rate, the approach might not be suitable to detect all logic bombs automatically. More than 99651 applications were
This work is licensed under a Creative Commons Attribution 4 License. For more information, see https://creativecommons.org/licenses/by/4/
# SAMHI AND BARTEL: ON THE (IN)EFFECTIVENESS OF STATIC LOGIC BOMB DETECTION FOR ANDROID APPS
flagged with 522300 triggers supposedly malicious, yielding a false-positive rate of more than 17%. Since we obtain a false-positive rate which is much higher than in the literature, we investigate the discrepancies. We construct multiple datasets to consider the concept drift effect, which could affect the results shown by Jordany et al. . Moreover, we also investigate multiple aspects of the implementation, such as the list of sensitive methods, the call-graph construction algorithm or the time-out threshold. Furthermore, we applied additional two filters not mentioned in the literature: (1) Purely symbolic values removal and (2) Different package name removal. Results indicate that to get close to a false positive rate of 0%, either aggressive filters should be put in place or a short list of sensitive methods should be used. In both cases, the impact on the false-negative rate is considerable. This means that if the approach is usable in practice with a low false-positive rate, it might miss many applications containing logic bombs.

We have sent the paper to the TRIGGER SCOPE’s authors, who gave us positive feedback and did not see any significant issue regarding our approach nor on TSO PEN’s design.

# In summary, we present the following contributions:
- We implement TSO PEN, the first open-source version of the state-of-the-art approach for detecting logic bombs and show that this approach might not be appropriate for automatically detecting logic bombs because it yields too many false-positives.

- We conduct a large-scale analysis over a set of more than 500000 Android applications. While the approach is theoretically not scalable because it relies on NP-hard algorithms, we find that, in practice, 80% of the applications can be analyzed.

- We conduct multiple experiments on the approach’s parameters to see the impact on the false positive rate and identify that a low false-positive rate can be reached but at the expense, for instance, of missing a large number of sensitive methods.

- We experimentally show that TRIGGERSCOPE’s approach might not be usable in a realistic setting to detect logic bombs with the information given in the original paper. We empirically show that using TRIGGERSCOPE’s approach, trigger analysis is not sufficient to detect logic bombs.

- We publicly release a database of Android apps containing logic bombs as a ground-truth for future research.

We make available our implementation of TSO PEN and TRIG DB with datasets to reproduce our experimental results: https://github.com/JordanSamhi/TSOpen
The remainder of the paper is organized as follows. First, a motivation example is given in Section 2 in order to clarify the reason we are studying logic bombs. Afterward comes the overview of TRIGGERSCOPE in Section 3, in which we give an overview of its structure. We evaluate TSOPEN in Section 4. Subsequently, in Section 5, we discuss the limitations of our work and the reference paper . Section 6 relates similar state-of-the-art works in the context of detecting anti-reverse-engineering practices. Finally, in Section 8 we present the direction in which we will continue our research.

# 2 MOTIVATION
We consider two motivating examples. The first one is an application that seems legit but embeds a time-bomb that is triggered at a specific date: let’s say two weeks after the installation date. This would mean that the malicious code will remain silent for some time before being triggered. Listing 1 is a concrete example of such a logic bomb. It is located at line 4 within the onStart() method, and it triggers the malicious code when a user opens the application and the date is reached.

We can see that with a minimum of effort, a malware developer can bypass most of the dynamic analyses which study the behavior of applications by monitoring them. Petsas & al.  give interesting results regarding simple solutions to bypass state-of-the-art dynamic analyses like Andrubis  and CopperDroid . The idea is to keep the malicious code dormant during dynamic analyses by hiding it behind unexplored branches. Nevertheless, recent works show that dynamic analyses improve code coverage during the execution of an application by forcing path exploration. They do so by instrumenting the application to modify the control flow. However, these approaches face several limitations, e.g., GRODDDROID  does not force all branches but only those that guard malicious code which they have to detect beforehand, reducing the problem to detecting malicious code. Likewise, X-FORCE  does not force all branches and can force unfeasible paths, making it unsound for analyses.