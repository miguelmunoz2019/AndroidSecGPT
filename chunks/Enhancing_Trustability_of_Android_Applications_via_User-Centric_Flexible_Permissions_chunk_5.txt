A second threat is represented by the fact that, in this experiment we consider only one kind of hardware machine. This choice is mainly guided by budget constraints related to both time and available resources. Notwithstanding, the used hardware is consumer-grade, hence we believe that collected measures are representative of performances that can be obtained on ordinary hardware. Moreover, it is important to note that the AFP Instrumenter is deployed in the AFP Server, whose hardware and software performance can be far higher than the machine we used for this experiment and can be easily scaled up if deployed in a virtualized/containerized environment.

Results. Collected measurements are presented in Fig. 8. For each app, we performed a comparison of both the CPU load and memory consumption for its two versions (i.e., original or instrumented) by using the Mann-Whitney test  with a = 0, one-tailed. In all cases, we obtained a p-value much larger than a, thus allowing us to confirm that there is low difference in the medians of CPU and memory consumption, with and without app instrumentation.

Discussion. From Fig. 8 it is evident that both CPU load and memory consumption of the original and instrumented versions of each app are comparable, as confirmed by statistical tests. The results of this experiment give a positive indication about the performance of AFP-based apps.

It is important to note that the focus is not on the formal systematic assessment of the precision of the app instrumentation (i.e., we do not have a formal proof that instrumented apps do not crash in some corner cases); nevertheless, we performed a manual assessment of stability of the 7 apps in experiment 2 by performing a set of evaluation runs, observing that the instrumented app conformed to the expected behavior. To recap: Performance of AFP-enabled apps are comparable to those of regular apps.

Threats to Validity. One common risk to validity of the experiment is the threat that adopted feature-component mappings and execution scenarios may not be representative of real app usage. To mitigate this threat, as a preliminary step, selected apps were examined by (i) analysing apps description in the Google Play store, (ii) manually inspecting their source code, and (iii) performing a set of preliminary runs while observing app behavior. Two different researchers were involved in the definition of both artifacts: first they were proposed by one and then verified to be representative by a second one.

A second threat is represented by the limited amount of apps involved in the experiment (7), a very small minority.

12. https://android.googlesource.com/platform/sdk/+/6db5720/monkeyrunner/src/com/android/monkeyrunner/recorder/MonkeyRecorder.java
13. http://developer.android.com/studio/test/monkeyrunner
14. http://developer.android.com/studio/command-line/adb.html
# SCOCCIA ET AL.: ENHANCING TRUSTABILITY OF ANDROID APPLICATIONS VIA USER-CENTRIC FLEXIBLE PERMISSIONS
2041
# 5 Experiment 3: Usability and Acceptance of AFP by Developers
Design. The goal of this experiment is to evaluate the usability and acceptance of AFP by developers. For this purpose, we conducted an on-line study, involving Android developers, in which we asked them to build the feature-component mappings for the apps they developed. We focused on this aspect as it is the main effort required to developers to make their app AFP-compliant (all the other steps are automated). This experiment is composed of three main phases:
1. Recruiting: we posted an announcement on pertinent on-line discussion groups (i.e., Android developer forums, mailing lists) to enlist developers willing to take part in the study. Each developer was asked to provide (at least) one link to an app published on the Google Play store, on which (s)he worked (either alone or in a team).

2. Mapping: the participants who provided a working link in the previous step were then invited to access a web-based app containing: (i) the definitions of feature- and level-based permissions, and (ii) the AFP web editor for the feature-component mappings. The participants were instructed to create the feature-component mappings for one of their previously-linked apps, and we collected the mappings defined by developers. We kept track of the time required by each participant for creating the feature-component mappings.

3. Evaluation: after completing the mapping, participants were asked to complete an on-line evaluation questionnaire about AFP. In order to encourage developers in participating to the experiment, we informed them that, during the mapping step, they did not have to manually input the activities of their Android app, as we had preemptively loaded them in the web editor after downloading and analyzing the manifest file of each app collected in the recruiting step.

Results. A total of 11 developers completed both the mapping definition and the evaluation questionnaire, providing us with twelve mappings in total (one developer performed the mapping construction for 2 apps). Developers participating in the experiment are also quite heterogeneous, both in terms of experience, number of developed apps, and size of organization. Specifically, participants have an average of 3 years of Android development experience (standard deviation = 2) and their majority (5) developed between 2 and 5 Android apps during their career, followed by 2 developers who developed more than ten. For what concerns organizations, the majority of developers work in small organizations (i.e., with 2 to 10 members), but we have also participants working in organizations with a number of members between 10 and 50. Finally, six developers declared to be Satisfied with current Android permissions, three declared to be Unsatisfied and two are Unsure.

Structure of the Evaluation Questionnaire Used for Developers
Authorized licensed use limited to: Pontificia Universidad Javeriana. Downloaded on August 13, 2024 at 04:26:33 UTC from IEEE Xplore. Restrictions apply.

# IEEE TRANSACTIONS ON SOFTWARE ENGINEERING, VOL. 47, NO. 10, OCTOBER 2021
# Apps Developed by Participants of Experiment 3
mappings, developers took an average of 482 seconds, i.e., 8 minutes (median = 136s, min = 21s, max = 1809s, SD = 660s). Even in the worst case, the time required by the participants to create the feature-activities mapping is close to half an hour. We consider such amount of time acceptable, considering that the definition of such mapping is conducted only once for an app.

Results of the usability part of the evaluation questionnaire are presented in Fig. 10, where each column of the heatmap represents the distribution of answers for each of the ten statements that comprise SUS. The procedure described in the SUS guidelines  was applied to normalize answers to each statement. For most statements, answers are clustered towards the middle of the scale for.

Authorized licensed use limited to: Pontificia Universidad Javeriana. Downloaded on August 13, 2024 at 04:26:33 UTC from IEEE Xplore. Restrictions apply.

# SCOCCIA ET AL.: ENHANCING TRUSTABILITY OF ANDROID APPLICATIONS VIA USER-CENTRIC FLEXIBLE PERMISSIONS
the majority of statements, revealing an acceptable usability. DOP Further confirmation is found in the obtained mean SUS score (49) that, according to prior research , is to be evaluated as Ok. Exceptions are statements s3 and s4. Most participants are in agreement with the former, that reads “I thought the system was easy to use”, confirming that the process was not overly difficult. At the same time developers felt in agreement with the latter: “I think that I would need the support of a technical person to be able to use this system”. Such results, and the fact that developers are not always familiar with permission-related technical aspects , encourages us to investigate in the future on techniques to assist in (or even to automate) the definition of feature-component mappings. Summarizing:
Developers positively evaluated the usability of the AFP approach. Improvements can be made on making the definition of the feature-component mappings more straightforward from a technical perspective.

# Threats to Validity
Possible threats to the validity and points of improvement for experiment 3 are as follows. The number of participants in the experiment (11) represents a very small minority of mobile app developers in the real world, hence results might not generalize. This threat is mitigated by the fact that developers who participated in the experiment have varied years of experience, nationalities and work in organizations of different size. A second threat is represented by the limited number of apps for which developers created the mappings. Thus, the results of our study might potentially not generalize to other apps. This threat is mitigated by the fact that submitted apps have very different purposes, provide different features, and have been developed by different developers. Finally, as the study has been conducted on-line, we had no way to ascertain that participants fully understood the task they were asked to complete. We mitigated this potential threat by directly asking in the final questionnaire whether developers had additional comments or doubts to clarify with respect to AFP.