# Using natural language processing:
Past research has successfully mined software artifacts and connected them with the app descriptions regarding security and privacy aspects. For instance, Gorla et al. used the applications’ descriptions to examine whether or not the description matches the applications’ behavior. The authors proposed Chabada, a solution to cluster apps by their topics based on their description, and to identify outliers, i.e., apps whose behavior deviates from the usage of permission protected APIs within each cluster. Further, Pandita et al. proposed Whyper and Qu et al. proposed AutoCog, two systems that also mine Android application descriptions and then use natural language processing (NLP) to automatically bridge the semantic gap between what applications do and what users expect them to do from their description. All of them, Chabada, Whyper, and AutoCog, work on app descriptions written by developers. On the other hand, our study focuses on reviews written by users, which are usually authored on smart phones, and hence often contain typos and do not necessarily follow grammatical structures.

# Processing app reviews:
App reviews play an important role for the success of an app. They are the primary channel through which developers receive feedback about their applications, such as how users perceive their apps, which features users are requesting, or which aspect of the apps users favor. By default, this channel is public and available to current and potential future users. However, inspecting such reviews is a challenging task for developers as apps receive a high number of reviews every day. Prior work by Pagano and Maalej found that iOS apps receive on average about 22 reviews per day and popular apps, such as Facebook, receive magnitudes more reviews. Moreover, reviews are not easy to automatically analyze given their unstructured forms. Existing work by Chen et al. has shown that only about one third of the user reviews are actually informative to developers. Different prior works have focused on automatically identifying useful user reviews for developers. Palomba et al. proposed ChangeAdvisor to support app developers in classifying feedback useful for app maintenance. ChangeAdvisor combines NLP, text analysis, and sentiment analysis to automatically classify app reviews written by end users. Fu et al. proposed WisCom, a tool that analyzes user comments and ratings in mobile app markets. WisCom uses regression models and latent dirichlet allocation models to analyze the comments’ topics. It is able to discover inconsistencies in reviews and determine why users dislike a given app. However, none of these works focuses on the connection between app reviews and the application’s security and privacy evolution.

# App security evolution
Calciati et al.  studied how the permissions requested by apps evolve across different app versions. Their results show that apps tend to request an increasing number of permissions in their evolution and many newly requested permissions are initially an over-privilege of the app (i.e., a direct violation of the least privilege principle). Violation of least-privilege by app developers is unfortunately a long-standing problem, first identified by Porter Felt et al. . Given the central role of permissions for data protection on Android, past research has also investigated how users should be confronted with permission requests, most noticeably early studies by Porter Felt et al. ,  that investigated users’ concerns connected to permission protected resources and that gave different recommendations, respectively, which are partially reflected in a recent paradigm shift of Android’s design from install-time to runtime permission delegation. More disruptive proposals try to eliminate the explicit role of the user for permission granting, e.g., through user-driven access control as proposed by Roesner et al.  or the use of machine learning as proposed by Wijesekera et al.  and Olejnik et al. . Most recently, different works pointed out the risks of third party libraries, in particular of advertisement libraries , , ,  and of vulnerable libraries , . However, to the best of our knowledge, we are the first to study the connection between user reviews and Android application security and privacy evolution.

# III. M ETHODOLOGY
In this paper, we automatically identify security- and privacy-related reviews (SPR) and map SPR to security- and privacy-related updates (SPU) of the corresponding applications. Figure 1 gives an overview of our methodology. We collect the dataset for our analysis with a custom built crawler, which mines Android applications and their version history as well as the apps’ reviews from Google Play. After having collected the apps and their reviews, a classifier identifies SPR. Once we have the set of SPR, we establish correlations between SPR of apps and the security and privacy relevant changes within the corresponding apps’ release history (S&P Mapping). In the following sections, we will describe the different steps of our methodology in details.

# A. App and Review Crawler
# 1) Mining user reviews:
The collection of the user reviews from Google Play consists of two steps: collecting the reviews’ text as well as their scores, and then pre-processing the text for later classification. We built a crawler to collect Android application reviews from Google Play. As previous studies ,  have shown that only a small fraction of free applications on Play accounts for the bulk of the application downloads—a so called superstar market—we focus our collection of applications on those apps that are most popular among the users of Google Play. Therefore, our crawler collects all Android applications that have at least 50,000,000 downloads, which results in 2,583 distinct applications as of July 2017 when we collected our dataset. It might seem that 2,583 apps is a very small number of applications in comparison to other market studies on Android, but it has to be considered that we also crawl each app’s version history and their corresponding reviews. Thus, we trade a large-scale cross-sectional study, as favored in most other studies on Play, for a longitudinal study of apps that allows us to analyze the evolution and influence of SPR on app security and privacy. Since downloading each app’s version history easily amplifies the required time for data collection and analysis , we chose to limit our data collection, both app version histories and reviews, to apps that have at least 50,000,000 downloads. We explain the technical realization of our app collection further down. We only crawl reviews that were written in English by selecting the Play web interface language code accordingly. Besides the review text with its rating score, we also gather developer responses (if available). Our dataset as of September 2017 contains 4,547,493 reviews. We will elaborate on how we compiled this list of reviews later on when explaining our training dataset for our review classifier (see Section III-B).

# 2) Crawling app history:
Studying security and privacy relevant changes of applications (SPU) and their connections with user reviews requires building an app repository with historical information about apps, i.e., including all versions of a particular app, which allows analysis of an app’s evolution. To this end, we adopt the approach of Backes et al.  that used an undocumented market API to query Google Play for older versions of an app. In the following, we will explain how we obtained the complete history of the top 2,583 apps in Play from September 2017, resulting in a repository of 62,838 distinct app versions (i.e., on average 24 versions per app in our collection).

# a) History collection:
The Play API allows us to query for app versions using the app’s packagename and version code. However, there is no option to list the available versions for a given app. Thus, building the history of an app requires probing for existing version codes. The version code is an integer number that must be monotonically increased with every app update, but there is no official numbering scheme enforced. Although the majority of app developers simply increases the version code by one, related work  has shown that some developers use special date patterns, such as YYYYMMDDVV where VV is the revision-per-day. Since exhaustively probing for the existing version codes of each app is very time consuming, we set the threshold for version codes.

# TABLE I
# S ECURITY- AND PRIVACY- RELEVANT KEYWORDS
the distribution of 790 apps for which we miss the long tail of upload dates. For about 70% of these apps, less than 30% of the whole app history is missing.

that we test to a maximum of 40k. This gives a coverage of 82% for the apps in our data set, i.e., for 2,126 out of 2,583 apps this threshold is higher than their highest version code on Play. Figure 2 illustrates the relative cumulative frequency distribution of the maximum version codes in our data set.