# Timeout
The stopping criteria of the GA is set as 500 generations. Note that the tuning parameters — the population size, the mutation probabilities, the value selection probabilities, and the stopping criteria — we used above are decided based on our preliminary assessment of the test generation algorithm, which we ran on a set of randomly selected apps. We found that higher probabilities of mutating a chromosome (>30%) and lower probabilities of selecting a value from seeded ones for mutation (<50%) usually results in the loss of good solutions. On the other hand, the test generation was not very effective when we used much lower probabilities of mutating a chromosome (e.g., 10%) and much higher probabilities of selecting seeded values (e.g., 90%). Some of the fields in intent messages, such as $action, $category, $pathPattern, and $key are not mutated at all because it would only result in ill-formed intent messages that would be rejected by the AUT. Overall, this ensures that the population is evolved towards better generations.

# 8 Evaluation
In this section we evaluate PREV and compare with two state-of-the-art static analysis-based techniques — Covert and IccTA — which can detect permission re-delegation vulnerabilities
Our goal is to detect as many vulnerabilities as possible at an affordable cost. Therefore, our main evaluation criteria are precision and cost. In addition, we also investigate the recall and report the results.

More specifically, the following research questions are investigated:
- RQ1 (Precision): Is PREV precise at detecting permission re-delegation vulnerabilities in Android apps?
- RQ2 (Cost): Is the cost (in terms of analysis time) of using our approach affordable in practice?
- RQ3 (Recall): Does PREV miss permission re-delegation vulnerabilities?
- RQ4 (Comparison): Does PREV perform better than other tools that can be used to detect permission re-delegation vulnerabilities?
- RQ5 (Robustness): Is PREV robust against the inclusion of anomalies in the training set?
- RQ6 (Threshold): What is the impact of other threshold values on vulnerability detection accuracy of PREV?
Our evaluation was conducted on a machine equipped with an Intel Core i7 2 GHz processor, 16 GB RAM, running Apple Mac OS X 10. Our tool is instrumented to log the analysis time.

# 8 Subject Apps
Our subject apps include a total of 1,258 real world apps from the official Google Play store Since our approach works on compiled apps, the availability of source code is not a requirement. Nevertheless, 595 of our subject apps are open source projects, which offer us the possibility to inspect the source code, determine the correctness of vulnerability reports generated by the tools, and analyze the causes of vulnerabilities. The following explains our selection process of subject apps:
First, we obtained a list of app names from the directory of AndroZoo,18 an app crawling research project that lists app names from many official and unofficial app repositories. We then randomly sampled the names from this list. Additional app names are also taken from a repository that collects open source Android apps, namely F-Droid Among those sampled apps, we picked those that are also available on the Google Play store, to ensure that our subject apps are real world apps.

We then filtered out those apps that are too popular (more than 1 million downloads), to increase the chance of selecting apps that are interesting for our experiment, i.e., apps.

We did not compare with test generation-based approaches because we could not find an adequate or available tool that might be suitable for detecting permission re-delegation vulnerabilities
Note that our tool PREV was trained on 11,796 official apps (see Section 5). Those apps are not considered in the selection of subject apps.

AndroZoo
F-Droid
Empirical Software Engineering (2020) 25:5084–5136 5111
with a good chance of containing vulnerabilities. Popular apps, distributed by well-reputed companies, are probably already subject to intensive security review.

To be able to apply our analysis, we additionally require that app description is in English, that contains at least 10 words so that natural language processing and topic discovery can be performed.

Eventually, there remained 1,258 apps — 663 closed source and 595 open source — from the official app store together with their descriptions. For open source apps, we acquired source code to conduct manual verification later.

The list of training apps and subject apps along with the implementation of PREV is publicly available
# 8 Metrics
To answer our research questions, we report results in terms of these metrics:
- Number of true positives (TP): Number of real vulnerable apps correctly reported as vulnerable;
- Number of false positives (FP): Number of vulnerable apps incorrectly reported as vulnerable (false alarms);
- Number of false negatives (FN): Number of vulnerable apps that are missed (not reported by the tool);
- Analysis time: The time (measured in minutes) taken by the tool to analyze a subject app;
- Number of contaminated apps: Number of training apps that contain permission re-delegation vulnerabilities;
- Threshold: the value used to flag outlier apps.

To answer RQ1 and RQ4, we quantify the precision of the tool based on true positives and false positives (Precision=TP/(TP+FP)). More specifically, when a tool reports a vulnerability, when source code is available, we manually inspect the part associated with the reported vulnerability. When source code is not available, we resort to the test case generated by the tool and observe the actual runtime behavior exercised by the test case. We then determine if the report is a true positive or a false positive. Note that since Covert and IccTA do not generate test cases, we evaluated them only based on open source apps so that we can verify their vulnerability reports. We answer RQ2 by using the analysis time to quantify the cost of using the tool. To answer RQ3 and RQ4, we measure the recall of the tool based on true positives and false negatives (Recall=TP/(TP+FN)).

The challenge here is to establish the false negatives, we would need to thoroughly inspect the source code of the subject apps, and determine if they are vulnerable or absolutely safe. This would require an overwhelming effort. Therefore, instead of conducting a security review of all the subject apps to label them as safe/vulnerable, we conduct a controlled experiment in which we apply two mutation operators to inject security faults that reflect realistic permission re-delegation vulnerabilities into a set of randomly selected apps. This provides us a benchmark for evaluating the recall, where all the apps in this benchmark are vulnerable by construction.

We answer RQ5 by including a set of contaminated apps in the training set and evaluating whether PREV can still detect the same permission re-delegation vulnerabilities as before.

20 https://biniamf.github.io/PREV/
# Empirical Software Engineering (2020) 25:5084–5136
# 8 RQ 1 : Precision
We ran our tool on the 1,258 subject apps. Each app under test (AUT) was subject to outlier detection and test case generation steps shown in Fig. 6.

Given the available clusters (Section 5), we map each AUT to the cluster with most similar app descriptions. We then obtain the permission re-delegation model inferred on the corresponding cluster. Next, we ran API reachability analysis on the AUT, to identify those privileged APIs that are reachable from public entry points. Out of all the 1,258 apps, 401 apps contain reachable privileged APIs.