The above measures define the execution structure that we characterize in our evolution study of Android app behaviors. With this definition, we approach our overarching question through the following three specific research questions.

- RQ1: How does malware exercise its functionalities in varied functionality scopes differently from benign apps?
- RQ2: How does malware execute different types of components differently from benign apps?
- RQ3: How does malware use callbacks of various categories differently from benign apps?
Note that these three questions are essentially three integral parts of our central (umbrella) question that guides the study in this paper‚Äîhow does malware behave differently from benign apps in terms of code-level execution structures? We look into run-time behaviors of malware and benign apps as embodied by the execution structures of apps from three aspects: functionality scope, communication among app components, and callbacks. Accordingly, each of the three questions focuses on one of these closely related aspects of app behaviors. Thus, the three questions are tightly connected, addressing the umbrella question consistently.

# 4. Methodology
This section describes our experimental methodology, including datasets used and study procedure followed.

# H. Cai, X. Fu and A. Hamou-Lhadj
# Information and Software Technology 122 (2020) 106291
# Number of benchmarks used in our study.

# 4. Benchmarks
To investigate how execution structure in benign and malicious Android apps evolve, we collected real-world app samples from varied sources throughout the past eight years (2010‚Äì17). Table 1 gives an overview of the app samples used in our study. Most of the samples were obtained from AndroZoo , except for that malware of years 2013 through 2016 was from VirusShare  and benign apps of year 2017 were from Google Play  to diversify the data sources for possibly better sample representativeness. The year of each sample was determined based on the DEX date and versionCode extracted from the app‚Äôs APK . For each year, we begun with a larger pool of samples and discarded those that did not meet our two selection criteria: (1) the app is dynamically analyzable‚Äîapps corrupted or with missing assets were dismissed, so were those that cannot be instrumented or launched, (2) exercising the app with random inputs generated by the Monkey tool  for 10 minutes did not cover at least 60% user code of the app (in terms of line coverage).

# 4. Execution profiling
For our dynamic study, applying a coverage criteria is necessary. We set the 60% threshold in an attempt to cover a majority of the app such that the app executions we analyzed in the study represent a reasonable portion of the common operational profile of the app. It is important to note that this representativeness is essential: since we aim to understand the behavioral differences of malware from benign apps, exercising the benign and malicious behaviours is a key to the validity of our study results. On the other hand, we could not automatically check if these behaviors (especially those of malware) are sufficiently exercised‚Äîautomatically capturing and validating the exhibition of malicious behaviors is still an open research problem in general . Nevertheless, a relatively high code line coverage as our threshold enforces should give enough confidence about the behaviors we need to characterize being covered.

A few research prototypes of automated input generation techniques exist, which may reach the threshold coverage faster or higher coverage in 10 minutes. Our choice of using Monkey over research prototypes  was made primarily due to the much greater robustness and usability of this industry-strength tool and its relative small shortage of coverage compared to the prototypes. Also, main relevant research prototypes only support relatively old Android (e.g., Dynodroid  and Sapienz  work for Android 4 or earlier versions), with which many of our samples cannot be installed/executed. Two recent studies further justified the use of Monkey.

The numbers of Table 1 were the actual numbers of apps used in our study, after applying the two selection criteria. In total, we analyzed the execution of 30,634 apps, including 15,451 benign apps and 15,183 malware. We did not intentionally select an equal number of apps for all the eight years to respect the uneven distribution of the total app populations over the years. We also had removed redundant apps within and across years whenever applicable, such that only unique samples were considered and any two of our yearly benign/malware datasets are disjoint. Each sample was confirmed as malware or benign using the VirusTotal  service‚Äîthe app was considered malware if at least ten of the anti-virus tools on VirusTotal identified it as so, otherwise it was considered benign. For all the samples eventually used in our study, the profiling with random inputs had line coverage ranging from 60% to 100% (mean 74%, standard deviation 11%).

To profile each benign and malware sample, we performed purely application-level instrumentation to trace method calls and ICC Intents. Our scope of tracing includes all method and ICC calls, including those made through exceptional control flows and reflection. The 10-minute per-app trace was produced by running the instrumented APK on a Google Nexus One emulator with the Android SDK 6 (API level 23), 2G RAM, and 1G SD storage, with inputs fed by Monkey. The emulator itself ran on a Ubuntu 15 host with 8G memory and a dual-core 2GHz processor. We utilized our Android app characterization toolkit  for these profiling tasks. In the end, 30,634 traces, each for a sample, formed the basis of our evolution study. To avoid possible side effects of inconsistent emulator settings, we started the installation and execution of each app in a fresh clean environment of the emulator (with respect to built-in apps, user data, and system settings, etc.).

# 4. Characterization
To characterize the execution structure of apps and their evolution over the eight-year span, we computed the dynamic measures (Section 3) for each app separately from its trace. More specifically, we first build a dynamic call graph from the trace, where each node represents a method/ICC call and each edge represents a dynamic call which is annotated with the frequency (i.e., number of instances) of that call. For an ICC call, the corresponding node contains additionally the Intent field values. Based on this call graph, our dynamic measures were mostly computed as a percentage (of certain kind of calls over the calls in a larger class). The only exception was for callback categorization, for which we rank the categories for each app according to the percentage of callback invocations belonging to each category and report for each category the mean rank across all benchmarks in a dataset. The categories of lifecycle callbacks were decided through a class hierarchy analysis, and those of the event handlers were recognized in reference to the callback interface categorization we developed earlier. More implementation details can be found in.

# 4. Metrics and measurements
It is important to note that, given the goal of our study, we focus on evolutionary trends of apps‚Äô execution structure rather than the absolute values of our measures‚Äîthese absolute values would vary with different samples studied. To compare malware with benign apps, we adopted an average-case analysis. Thus, with respect to each measure, we typically compute the average over all apps in the benign or malware group of a particular year, and then compare the two groups in terms of the averages.

Beyond these average-case comparisons, we also computed the statistical significance of differences between benign-app and malware groups with respect to each structural trait of apps (e.g., percentage of external implicit ICC) involved in our studies. We used the paired Wilcoxon signed-rank tests  to assess the significance at the 0 confidence level (i.e., ùõº = 0). To understand the magnitude of those differences, we further computed corresponding effect sizes in terms of Cliff‚Äòs Delta  (in a paired setting with ùõº = 0). In both analyses, the two groups compared were the average-case metric values of the two app groups across the eight years (i.e., each group has eight values).

# H. Cai, X. Fu and A. Hamou-Lhadj
# Information and Software Technology 122 (2020) 106291
Both analyses are nonparametric, making no assumption about the normality of the distribution of underlying data points. For each app trait (i.e., dynamic measure used in the characterization), we typically compare benign apps against malware regarding that trait first in a plot, followed by the Wilcoxon p values (noted as p) and Cliff‚Äôs Delta (noted as Œî) presented in a table. Given a Cliff‚Äôs Delta value d, we interpret the effect size as follows : effect size is negligible if |d| ‚â§ 0, small if 0 < |d| ‚â§ 0, medium if 0 < |d| ‚â§ 0, and large if |d| > 0.

# 5. Major findings
In this section, we present and discuss our study results, as guided by the three main research questions.