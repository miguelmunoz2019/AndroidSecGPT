# 4 Vulnerability Detection Tools
Every Ghera benchmark is associated with exactly one unique vulnerability v, and its benign app exhibits v while its secure app does not exhibit v. So, for a tool, for each applicable benchmark, we classified the tool’s verdict for the benign app as either true positive (i.e., v was detected in the benign app) or false negative (i.e., v was not detected in the benign app). We classified the tool’s verdict for the secure app as either true negative (i.e., v was not detected in a secure app) or false positive (i.e., v was detected in a secure app). Columns TP, FN, and TN in Table 5 report true positives, false negatives, and true negatives, respectively. False positives are not reported in the table as none of the tools except DevKnox (observe the D for DevKnox under System benchmarks in Table 5) and data leakage extension of Amandroid (observe the {1} for Amandroid 1 under Storage benchmarks in Table 5) provided false positive verdicts. Reported verdicts do not include cases in which a tool failed to process apps.

# Observation 5
Most of the tools (10 out of 14) were applicable to every Ghera benchmark; see # Applicable Benchmarks column in Table 5. Except for MalloDroid, the rest of the tools were applicable to 24 or more Ghera benchmarks. This observation is also true of Amandroid if the results of its pre-packaged extensions are considered together.

# Observation 6
Based on the classification of the verdicts, 4 out of 14 tools detected none of the vulnerabilities captured in Ghera (“0” in the TP column in Table 5) considering all extensions of Amandroid as one tool. Even in case of tools that detected some of the vulnerabilities captured in Ghera, none of the tools individually detected more than 15 out of the 42 vulnerabilities; see the numbers in the TP column and the number of N’s under various categories in Table 5. This number suggests that in isolation, the current tools are very limited in their ability to detect known vulnerabilities captured in Ghera.

# Observation 7
For 11 out of 14 tools, the number of false negatives was greater than 70% of the number of true negatives; see FN and TN columns in Table 5. This proximity between the number of false negatives and the number of true negatives suggests two possibilities: most tools prefer to report only valid vulnerabilities (i.e., be conservative) and most tools can only detect specific manifestations of vulnerabilities. Both these possibilities limit the effectiveness of tools in assisting developers to build secure apps.

# Observation 8
Tools make claims about their ability to detect specific vulnerabilities or class of vulnerabilities. So, we examined such claims. For example, while both COVERT and DIALDroid claimed to detect vulnerabilities related to communicating apps, neither detected such vulnerabilities in any of the 33 Ghera benchmarks that contained a benign app.

AndroBugs, Marvin-SA, and MobSF were the exceptions. We considered all variations of a tool as one tool, e.g., JAADAS. We did not count FixDroid as we did not evaluate it on secure apps in Ghera.

198 Empirical Software Engineering (2020) 25:178–219
and a malicious app. Also, while MalloDroid focuses solely on SSL/TLS related vulnerabilities, it did not detect any of the SSL vulnerabilities present in Ghera benchmarks. We observed similar failures with FixDroid. See numbers in # Applicable Benchmarks and TP columns for COVERT, DIALDroid, FixDroid, and MalloDroid in Table 5. These failures suggest that there is a gap between the claimed capabilities and the observed capabilities of tools that could lead to vulnerabilities in apps.

# Observation 9
Different tools use different kinds of analysis under the hood to perform security analysis. Tools such as QARK, Marvin-SA, and AndroBugs rely on shallow analysis (e.g., searching for code smells/patterns) while tools such as Amandroid, FlowDroid, and HornDroid rely on deep analysis (e.g., data flow analysis); see H/E column in Table 2. Combining this information with the verdicts provided by the tools (see TP and FN columns in Table 5) provides the number of vulnerabilities (not) detected by shallow analysis and deep analysis across various categories as listed in Table 6. From this data, we observe tools that rely on deep analysis report fewer true positives and more false negatives than tools that rely on shallow analysis. We also observe tools that relied on shallow analysis detected all of the vulnerabilities detected by tools that relied on deep analysis.

Further, among the evaluated tools, most academic tools relied on deep analysis while most non-academic tools relied on shallow analysis; see H/E columns in Table 2 and tools marked with * in Table 5.

# Open Questions 3 & 4
A possible reason for the poor performance of deep analysis tools could be they often depend on extra information about the analyzed app (e.g., a custom list of sources and sinks to be used in data flow analysis), and we did not provide such extra information in our evaluation. However, JAADAS was equally effective in both fast (intra-procedural analysis only) (J AADAS H) and full (both intra- and inter-procedural analyses) (J AADAS E) modes, i.e., true positives, false negatives, and true negatives remained unchanged across modes. Also, FixDroid was more effective than other deep analysis tools even while operating within an IDE; it was the fifth best tool in terms of the number of true positives. Clearly, in this evaluation, shallow analysis tools seem to outperform deep analysis tools. This observation raises two related questions: 3) are Android app security analysis tools that rely on deep analysis effective in detecting vulnerabilities in general? and 4) are the deep analysis techniques used in these tools well suited in general to detect vulnerabilities in Android apps? These questions are pertinent because Ghera benchmarks capture known vulnerabilities and the benchmarks are small/lean in complexity, features, and size (i.e., less than 1000 lines of developer created Java and XML files), and yet deep analysis tools failed to detect the vulnerabilities in these benchmarks.

# Observation 10
Switching the focus to vulnerabilities, every vulnerability captured by Permission and System benchmarks were detected by some tool. However, no tool detected any of the two vulnerabilities captured by Networking benchmarks. Further, no tool detected 12 of 42 known vulnerabilities captured in Ghera (false negatives); see # Undetected row in Table 5. In other words, using all tools together is not sufficient to detect the known vulnerabilities captured in Ghera.

In line with the observation made in Section 4 based on Table 3 – most of the vulnerabilities captured in Ghera were discovered before 2016, most of the vulnerabilities (9 out of 12) not detected by any of the evaluated tools were discovered before 2016; see Table 7.

# Empirical Software Engineering (2020) 25:178–219
# Empirical Software Engineering (2020) 25:178–219
The number of benchmarks in each category is mentioned in parentheses. In each category, empty cell denotes the tool is inapplicable to any of the benchmarks, D denotes the tool flagged both benign and secure apps as vulnerable in every benchmark, N denotes the tool flagged both benign and secure apps as not vulnerable in every benchmark, and X denotes the tool failed to process any of the benchmarks. H/I/J/K denotes the tool was inapplicable to H benchmarks, flagged benign app as vulnerable and secure app as not vulnerable in I benchmarks, flagged both benign and secure app as not vulnerable in J benchmarks, and reported non-existent vulnerabilities in benign or secure apps in K benchmarks. Along with D and N, I and J (in bold) contribute to TP and FN, respectively. The number of benchmarks that a tool failed to process is mentioned in square brackets. The number of benchmarks in which both benign and secure apps were flagged as vulnerable is mentioned in curly braces. “-” denotes not applicable cases. “*” identifies non-academic tools. “?” denotes unknown number.

# Empirical Software Engineering (2020) 25:178–219
Breakdown of vulnerabilities detected by shallow and deep analyses, and not detected by either analyses
# Open Questions 5 & 6
Of the 42 vulnerabilities, 30 vulnerabilities were detected by 14 tools with no or minimal configuration, which is collectively impressive. Further, two questions are worth exploring: 5) with reasonable configuration effort, can the evaluated tools be configured to detect the undetected vulnerabilities? and 6) would the situation improve if vulnerability detection tools rejected during tools selection are also used to detect vulnerabilities?
# Observation 11
Of the 14 tools, 8 tools reported vulnerabilities that were not the focus of Ghera benchmarks; see Other column in Table 5. Upon manual examination of these benchmarks, we found none of the reported vulnerabilities in the benchmarks. Hence, with regards to vulnerabilities not captured in Ghera benchmarks, tools exhibit a high false positive rate.