# The RICO mobile apps dataset & Sampling strategy
To guarantee a non-biased selection of the data, we relied on information collected in the RICO mobile apps dataset , which contains metadata information of more than 9k Android apps spanning 27 different app categories. Although there are other (and larger) datasets of apps (e.g., Androzoo Allix et al. 2016), we decided to consider the RICO dataset for 3 main motivations:
- Real crowd-workers successfully interacted with each app contained in this dataset; this reduces the likelihood of considering toy apps ;
- About 30% of the apps in this dataset have been removed from the Google Play; this allows to reduce the likelihood of considering apps that have been already judged as low-quality, risky, or malicious by the app market itself ;
– all the apps in this dataset were released in 2017, at the latest. As vulnerabilities may survive a long time after the first release of an app , apps with years of development behind them are more likely to implement well-established processes for mitigating security threats.

Since several apps (i.e., about 30%) appearing in the RICO dataset are no longer available for download on the Google Play store, we selected a statistically significant sample from this large dataset, to ensure a reliable and fair representativeness of the data collection (i.e., a confidence level of 95% and a confidence interval less than 3). In particular, we performed a stratified random sampling of apps present in the RICO dataset, by selecting apps belonging to different app categories. The number of apps downloaded for each category, along with the ranges of average star ratings assigned to these apps are shown in Table 1. The categories considered in our dataset are a subset of all Google Play categories.

To have a sample with more balanced data concerning the app ratings, we applied the following steps to select the apps to include in our dataset:
1. We scraped Google Play to collect the updated average rating values associated with the apps in the RICO dataset that are still available on Google Play.

# 2.

For each app category C, we computed the median of average ratings values, Mi, assigned to apps of the RICO dataset belonging to C.

For each app category Ci, a half of the Ci apps included in our sample have an average rating value below or equal to Mi, and the remaining half of the Ci apps in our sample have an average rating value above Mi.

For instance, for the Medical category we selected 33 different apps (as reported in Table 1), 16 of such apps have an average rating value below or equal to 4, while the remaining apps have an average rating value higher than 4. It is worth noticing that in our sample we only considered apps having at least 500 raters, this to reduce the risk that our results may depend on very subjective or small amount of ratings. Consistently to previous work , we adopted this threshold (i.e., 500 raters) to target popular apps.

According to recent statistics2 there are 2 million apps available for download on the Google Play Store. Thus, our dataset is also a representative sample (i.e., a confidence level of 99% and a margin of error of 4%) of all apps present on the Google app market.

# Data Collection
Once selected the apps to download, all APK files were downloaded from Google Play. In particular, we implemented a web crawler able to automatically scrape Google Play with the aim of gathering updated information about the apps from the store and collecting the APK files of the selected apps.

Along with the APKs we collected all the information provided by the Google Play store related to each app. More specifically, we gathered the following metadata: (i) the app’s name, (ii) the app’s description, (iii) the app’s Play store category, (iv) the app’s size, (v) the photos/screenshots appearing in the app’s market webpage, (vi) the app’s average rating, (vii) the number of users who rated the app, (viii) range of the number of installs, (ix) the list of permissions the app requires, and (x) monetization-related information (i.e., whether the app contains ads and/or offers in-app purchases). All the APK files, and the related metadata were collected during December 2019.

The downloaded apps were scanned through AndroBugs3, with the aim of detecting potential security defects related to each of the considered apps. Among the several vulnerability scanning tools available, we selected AndroBugs as, differently from other vulnerability scanners, it tries to simulate the operation of an app considering the paths or means through which such weaknesses would be exploited, instead of just scanning the code for weak spots (Darvish and Husain 2018). Besides checking for many known common vulnerabilities in the Android apps, AndroBugs also inspects the code against (i) missing best practices, and (ii) dangerous shell commands (e.g., su). In particular, AndroBugs is a state-of-the-art well-known vulnerability scanner for mobile applications that was first presented at the BlackHat security conference, and has been used in several previous studies investigating vulnerabilities occurring in mobile apps . This static detection tool was also successfully used to find vulnerabilities and other critical security issues in Android apps developed by several big players, such as: Facebook, eBay, Twitter, etc..

All the reports generated by AndroBugs have been parsed to enumerate, for each app, the detected vulnerabilities. Listing 1 illustrates an excerpt from an AndroBugs report. To each detected vulnerability a severity score (e.g. Critical in Listing 1) and a type (e.g. Runtime Command Checking in Listing 1) is assigned. It is worth noticing that we only collected
2 https://buildfire.com/app-statistics - accessed on February 2021.

3 https://github.com/AndroBugs/AndroBugs Framework
# Empir Software Eng (2021) 26: 78
# Listing 1  Excerpt from an AndroBugs report
vulnerabilities which are marked as Critical, Warning, or Notice by AndroBugs, as vulnerabilities with a Info categorization indicate that the specific issue was not found on the specific apk . Some of the potential security weaknesses that the AndroBugs static analyzer is able to detect are related to: (i) SSL connections, implementation and certificate validation, (ii) WebView- and Fragment-related vulnerabilities, (iii) implicit intents, (iv) data storage, (v) KeyStore protection, (vi) Android Manifest settings, and (vii) entry points for command injection. The full list of vulnerabilities checked is reported in our online appendix
Replication package We make available in our replication package5 all the raw data used to answer our research research questions.

# 3 Research method
To answer our RQ1 we compared the vulnerability warnings detected in the apps belonging to the different Google Play store’s categories. In particular, to characterize the vulnerability-proneness level of an app category, we group all the apps in our sampled dataset (see Section 3) that belong to that specific category and compute the vulnerability-proneness levels related of these apps. Then, to establish whether there are categories exhibiting higher/lower vulnerability-proneness levels, we compared the vulnerability proneness levels associated with each category. Besides, to check whether the observed differences are statistically significant, we make use of the Kruskal-Wallis test (Kruskal and Wallis 1952) and subsequent Mann-Whitney pairwise comparison  (with the Holm’s p-value correction procedure  and α = 0), for identifying the specific pairs whose differences have statistical evidence. The Kruskal-Wallis test is a rank-based nonparametric test that is used to determine if statistically significant differences between more than two groups of an independent variable can be observed. However, since this test does not allow identifying the specific groups for which the differences have statistical evidence, we perform a posthoc analysis (using Mann-Whitney pairwise comparison) to determine which levels of the independent variable differ from each other level. To counteract the problem of multiple comparisons and reduce the probability of obtaining Type 1 errors (false positives), we use the Holm’s correction procedure, which allows adjusting the rejection criteria for each of the individual hypotheses tested.

In particular, we tested the following null hypothesis:
H01: There are no significant differences between the vulnerability-proneness levels of apps belonging to different Play store’s categories.

4 https://github.com/adisorbo/vulnerability proneness/wiki/Vulnerability-Types
5 https://github.com/adisorbo/vulnerability proneness
We also estimated the magnitude of the differences with statistical significance, through the Cliff’s delta (or d) effect-size measure (Grissom and Kim 2005). Following the guidelines in Grissom and Kim (2005), we interpret the effect-size as: small for |d| &lt; 0, medium for 0 ≤ |d| &lt; 0, and large for |d| ≥ 0. For graphically visualizing the distributions, we make use of boxplots. Moreover, to complement this quantitative analysis, we qualitatively investigated the different types of vulnerability warnings encountered in apps belonging to categories exhibiting higher/lower vulnerability-proneness levels.