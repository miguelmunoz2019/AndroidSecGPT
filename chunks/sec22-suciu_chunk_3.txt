most situations. Because the Temporal score is not updated in NVD, we collect it from two reputable sources: IBM X-Force Exchange  threat sharing platform and the Tenable Nessus  vulnerability scanner. Both scores are used as inputs to proprietary severity assessment solutions: the former is used by IBM in one of their cloud offerings , while the latter is used by Tenable as input to commercial vulnerability prioritization solutions . We use the labels "Functional" and "High" as evidence of exploitation, as defined by the official CVSS Specification , obtaining 28,009 exploited vulnerabilities. We further collect evidence of 2,547 exploited vulnerabilities available in three commercial exploitation tools: Metasploit , Canvas  and D2 . We also scrape the Bugtraq  exploit pages, and create NLP rules to extract evidence for 1,569 functional exploits. Examples of indicative phrases are: "A commercial exploit is available.", "A functional exploit was demonstrated by researchers.".

We also collect exploitation evidence that results from exploitation in the wild. Starting from a dataset collected from Symantec in our prior work , we update it by scraping Symantecâ€™s Attack Signatures  and Threat Explorer . We then aggregate labels extracted using NLP rules (matching e.g. "... was seen in the wild.") from scrapes of Bugtraq , Tenable , Skybox  and AlienVault OTX . In addition, we use the Contagio dump  which contains a curated list of exploits used by exploit kits. These sources were reported by prior work as reliable for information about exploits in the wild . Overall, 4,084 vulnerabilities are marked as exploited in the wild.

While exact development time for most exploits is not available, we drop evidence if we cannot confirm they were published within one year after vulnerability disclosure, simulating a historical setting. Our ground truth, consisting of 32,093 vulnerabilities known to have functional exploits, therefore reflects a lower bound for the number of exploits available, which translates to class-dependent label noise in classification, issue that we evaluate in Section 7.

# 4 Estimating Lifecycle Timestamps
Vulnerabilities are often published in NVD at a later date than their public disclosure . We estimate the public disclosure dates for the vulnerabilities in our dataset by selecting the minimum date among all write-ups in our collection and the publication date in NVD, in line with prior research . This represents the earliest date when expected exploitability can be evaluated. We validate our estimates for the disclosure dates by comparing them to two independent prior estimates , on the 67% of vulnerabilities which are also found in the other datasets. We find that the median date difference between the two estimates is 0 days, and our estimates are an average of 8 days earlier than prior assessments. Similarly, we estimate the time when PoCs are published as the minimum date among all sources that shared them, and we confirm the accuracy of these dates by verifying the commit history in exploit databases that use version control.

To assess whether EE can provide timely warnings, we need the dates for the emergence of functional exploits and attacks in the wild. Because the sources of exploit evidence do not share the dates when exploits were developed, we estimate these dates from ancillary data. For the exploit toolkits, we collect the earliest date when exploits are reported in the Metasploit and Canvas platforms. For exploits in the wild, we use the dates of first recorded attacks, from prior work . Across all exploited vulnerabilities, we also crawl VirusTotal , a popular threat sharing platform, for the timestamps when exploit files were first submitted. Finally, we estimate exploit availability as the earliest date among the different sources, excluding vulnerabilities with zero-day exploits. Overall, we discovered this date for 10% (3,119) of the exploits. These estimates could result in label noise, because exploits might sometimes be available earlier, e.g. PoCs that are easy to weaponize. In Section 7 we measure the impact of such label noise on the EE performance.

# 4 Datasets
We create three datasets that we use throughout the paper to evaluate EE. DS1 contains all 103,137 vulnerabilities in our collection that have at least one artifact published within one year after disclosure. We use this to evaluate the timeliness of various artifacts, compare the performance of EE with existing baselines, and measure the predictive power of different categories of features. The second dataset, DS2, contains 21,849 vulnerabilities that have artifacts across all different categories within one year. This is used to compare the predictive power of various feature categories, observe their improved utility over time, and to test their robustness to label noise. The third one, DS3 contains 924 out of the 3,119 vulnerabilities for which we estimated the exploit emergence date, and which are disclosed during our classifier deployment period described in Section 6. These are used to evaluate the ability of EE to distinguish imminent exploit.

# 5 Empirical Observations
We start our analysis with three empirical observations on DS1, which guide the design of our system for computing EE. Existing scores are poor predictors. First, we investigate the effectiveness of three vulnerability scoring systems, described in Section 2, for predicting exploitability. Because these scores are widely used, we will utilize them as baselines for our prediction performance; our goal for EE is to improve this performance substantially. As the three scores do not change over time, we utilize a threshold-based decision rule, which predicts that all vulnerabilities with scores greater or equal than the threshold are exploitable. By varying
USENIX Association 31st USENIX Security Symposium 381
# Performance of existing severity scores at capturing exploitability
When evaluating the Microsoft Exploitability Index on the 1,100 vulnerabilities for Microsoft products in our dataset disclosed since the score inception in 2008, we observe that the maximum precision achievable is 0. The recall is also lower because the score is only computed on a subset of vulnerabilities.

On the 3,030 vulnerabilities affecting RedHat products, we observe a similar trend for the proprietary severity metric, where precision does not exceed 0.

These results suggest that the three existing scores predict exploitability with > 50% false positives. This is compounded by the facts that (1) some scores are not computed for all vulnerabilities, owing to the manual effort required, which introduces false negative predictions; (2) the scores do not change, even if new information becomes available; and (3) not all the scores are available at the time of disclosure, meaning that the recall observed operationally soon after disclosure will be lower.

Artifacts provide early prediction opportunities. To assess the opportunities for early prediction, we look at the publication timing for certain artifacts from the vulnerability lifecycle.

Write-ups are the most widely available ones at the time of disclosure, suggesting that vendors prefer to disclose vulnerabilities through either advisories or third party databases. However, many PoCs are also published early: 71% of vulnerabilities have a PoC on the day of disclosure. In contrast, only 26% of vulnerabilities in our dataset are added to NVD on the day of disclosure, and surprisingly, only 9% of the CVSS scores are published at disclosure.

Overall, the availability of exploits is highly correlated with the emergence of other artifacts, indicating an opportunity to infer the existence of functional exploits as soon as, or before, they become available.

Exploit prediction is subject to feature-dependent label noise. Good predictions also require a judicious solution to the label noise challenge discussed in Section 3.

To test for such individual biases, we investigate the dependence.

# Functional Exploits
# Exploits in the Wild
between all sources of exploit evidence and various vulnerability characteristics. For each source and feature pair, we perform a Chi-squared test for independence, aiming to observe whether we can reject the null hypothesis H0 that the presence of an exploit within the source is independent of the presence of the feature for the vulnerabilities. Table 1 lists the results for all 12 sources of ground truth, across the most prevalent vulnerability types and affected products in our dataset. We utilize the Bonferroni correction for multiple tests  and a 0 significance level. We observe that the null hypothesis can be rejected for at least 4 features for each source. We observe that the null hypothesis can be rejected for at least 4 features for each source, indicating that all the sources for ground truth include biases caused by individual vulnerability features. These biases could be reflected in the aggregate ground truth, suggesting that exploit prediction is subject to class- and feature-dependent label noise.

# 6 Computing Expected Exploitability
In this section we describe the system for computing EE, starting from the design and implementation of our feature extractor, and presenting the classifier choice.

# 6 Feature Engineering
EE uses features extracted from all vulnerability and PoC artifacts in our datasets, which are summarized in Table 2.