# b) Release dates:
A second major drawback of the Play API is that it is not possible to query for release/upload dates of old app versions. In order to be able to map reviews to app versions by date, we follow the approach of related work to collect missing release dates from market analysis companies, such as appannie.com, apk4fun.com, and appbrain.com. In total, we were able to recover the upload dates of 81% of all app versions in our data set (51,225 of 62,838). For a set of 957 apps we were able to retrieve the complete version history. For the remaining 1,169 apps we have an incomplete set of upload dates, for whose majority (790 of 1,169) we miss the long tail of upload dates, i.e, we could not recover dates for early versions that were published before any of the market analysis services started to collect data.

# B. Review Classifier
A naïve way to identify SPR would be using keywords. However, this is not an easy task, since we cannot study millions of reviews to pick a representative keyword list. Besides, a review written by users can have multiple sentences. If we only use keywords to identify SPR, we may miss other information that comes from the nearby sentences that may contain interesting information but not the predefined keywords. Hence, by using machine learning techniques to learn not only the sentence with keywords but also the nearby ones we can expand our classifier’s knowledge. For instance, consider the following review: “Why do you need access to my location? Why on gods good green earth does your app need access to my location info? One star for the privacy steal.” If we would use keywords, we can only determine the first two sentences as security- or privacy-relevant. However, the last sentence is also an indicator that this app is perhaps doing something fishy. This is an important feature that we can put into a classifier without having to learn the phrase privacy steal. Later on, if our classifier encounters similar reviews, even without the presence of privacy-related keywords (here: location), it is still able to classify them as SPR (e.g., “This app steals your info”).

# a) Training set:
Given the large amount of reviews and the anticipated low portion of SPR, it is not feasible to manually label a representative set of reviews while simultaneously
balancing the number of SPR and non-SPR. Therefore, we first look at reviews that mention Android permissions or resources that are by default protected by an Android permission. We then manually examine some SPR to pick further keywords mentioned in such SPR and visit the Android documentation regarding the mentioned permissions to further complement our keyword list with the information from the documentation. We strongly focus on permission-protected resources, because this is the only interaction that end-users can usually observe when they interact with the apps, e.g., install-time permission dialogs (prior to Android 6) or intercepting dialogs for runtime permission requests (Android 6 or later). It is rather uncommon to see layman users that are not security experts using extra analysis tools (e.g., Xposed modules ) to track data flows within applications for privacy violations or to detect insecure network connections of apps.

# Table I
shows the list of compiled keywords we use in our analysis. This list results in approx. 1M reviews that are potentially security and privacy related. We randomly picked 4,000 reviews to manually label them. We consider a review as SPR if the user mentions the app’s requested permissions, keywords related to accessed resources, or other general SPR keywords (see Table I); otherwise we consider the review as non-SPR. After removing some malformed reviews (e.g., we were unable to determine what the reviews meant), our training set contained 3,891 reviews (SPR: 586, non-SPR: 3,305). To account for imbalanced data (SPR vs. non-SPR), we apply SMOTE  to over-sample the SPR class.

# b) Features extraction:
Characters of n-grams are commonly used features in text classification , , . Character n-gram features for a review are all n consecutive letters in that review. For instance, the 5-grams for the review ”Why does this app need access to my location” are why d, hy do, y doe, does, oes t, es th, [...] , locat, ocati, cation. We use n-grams of characters instead of words, because reviews written by users often contain typos, and by using n-grams of characters, we can reduce the influence of typos onto the classification. Prior work of McNamee et al.  showed that n = 4 (characters) is a good choice for European language text retrieval, while Dave et al.  reported that unigrams (n = 1) of words outperform bigrams when conducting text classification of movie reviews. Inspired by their findings, we choose n = 3, 4, 5 as our n-gram models, which also yielded the best results during experiments with our training data. Before extracting n-grams of the reviews, we apply different text pre-processing techniques to obtain a better quality data set since user reviews are often written on smart phones, hence they tend to be very short and usually contain grammatical mistakes or typos , , :
- Remove stopwords: remove articles from the user reviews (e.g ”a”, ”an”, ”the”)
- Stemming: reduce inflectional forms to a common base form of a word (e.g. am, are is → be)
# c) Machine learning model:
Classifying reviews belongs to the task of natural language processing (NLP) and the most common NLP approach for using machine learning to classify text documents is using Bag of Words . With bag of words, each text document is represented as the bag of its words regardless of its grammar forms and its orders. Occurrences of each word is used as feature for training classifiers. We use a Support Vector Machine (SVM) Linear kernel for our classifier as it has been shown to be effective for text classification , , especially for short documents . We form bag of words by splitting the reviews at spaces and punctuation marks, and use n-gram model to extract features for our classification task.

# d) Validation:
To validate our approach, we use k-Fold cross validation with k = 10, as prior work of Kohavi  has shown that this is the best method for cross validation. Besides, we choose AUC (area under the ROC curve ) as our classifier evaluation metric because it is not sensitive to imbalanced class distribution (SPR vs. non-SPR) and was widely used in prior work as the metric for imbalanced data classification , . Figure 4 shows the AUC values for our 10-Fold cross validation. Our classifier has an AOC’s mean value of 0 as its accuracy in classifying SPR (a classifier with perfect accuracy would have an AUC of 1).

# C. Static App Analysis
So far we have built the data model that allows us to map SPR to the enclosing set of app versions by using both app version release dates and the date of the review. In order to measure the effect of an SPR on app security and privacy, we conduct static analysis on the version immediately preceding the SPR and the updated versions after the SPR to find potential SPU. For the majority of end-users the install-time permission list and runtime permission requests are the only information to assess whether the advertised functionality (e.g., via the app description, app category, etc.) seems legitimate. To determine the change of permissions and usage of APIs that require permissions across versions, we leverage the permission lists of the axplorer project . Its authors provide mappings of Android SDK APIs to required permissions up to Android version 7. In a first step, we
# Mapping SPR to SPU
The final step in our work-flow (see Figure 1) is to correlate the SPR for an app with the security and privacy related changes of an app. First, we identify potential candidate app versions that might contain relevant app changes in connection with an SPR, afterwards we analyze the candidate versions for security and privacy relevant updates (e.g., in the app manifest or code).

# Identifying candidate app versions:
# SPR to SPU version distance:
While in ideal scenarios, we would expect SPU right after SPR, there are other factors which may contribute to the reasons why the next update may not be an SPU. For instance, developers are working on a particular feature of the app or they may only read user reviews irregularly (e.g., reviews come in large number ). We therefore take the distance between an SPR and an SPU release into account. In particular, if there is an SPR for version1 but an SPU is found at version4, then the distance is 3. The longer the distance is, the less likely the SPU is triggered by the SPR.