# Functionality First
A common theme across eight interviews (P3, P10, P15, P18, P20, P24, P25) is the prioritization of features and functionality – to the detriment of usable security. This functionality first principle is also an indicator of the low awareness of usable security’s importance: “against security, against usable security as well. Yes. The resources, the priorities were just mostly to new features” (P15). Participants described that this is related to time or budget:
“There’s a point where you will sacrifice a usability or security feature. [. . .] You cannot touch the main functionality. But you can remove a security feature or a UI part or whatever to remain inside a budget.” (P18)
Following that principle, some participants (P3, P10, P15, P25) told us that the features, including their security, only have to be functional regardless of how usable they are: “I absolutely don’t care what the function looks like. The main thing is that the function is in there, and the customer can use it.” (P3). Furthermore, those participants reported that improving usable security is something they plan to attend to sometime later. But since budgets are always too tight, the resources are never there. While functionality is the key selling point for software and the deciding factor when competing with other companies (P24), the fact that its usable security is never attended to before budgets run out should raise concerns in development companies and their customers.

# 2) Misconceptions
The analysis of the interviews revealed several misconceptions about usable security and also about usability itself. If software developers do not understand what usable security is, we cannot expect them to carry out the steps needed to deliver it. P9, for example, mentioned a usable authentication method where users overlooked the security but the participant himself did not realize that this is an aspect of usable security: “But otherwise, I think we really don’t have [usable security]. Because the login happens [. . .] [transparently for users] and what we do there in terms of security things has no influence on how the normal user uses it.” (P9). However, while this would allow accidental or unconscious usable security, other misconceptions would hinder usable security adoption. For example, P21 blamed users for being unable to cope with the security technology and forgetting usernames and passwords: “[This is] not related to usability, mostly it’s related to lack of technology skills. [. . .] we can’t do anything about [authentication]” (P21).

Another participant did not realize that even well-educated users are more likely to make mistakes when presented with a complex mechanism: “Then security is my first priority because they are already educated and they know exactly what’s going on in the system.” (P1).

As we have already seen, usability may be assigned a low priority because it is not taken seriously: “So it’s kind of, I would say, a healthy mix of best practice and common sense.” (P5). A designer (P19) told us that her expertise is not perceived as such:
“But there is a totem pole, and that’s you don’t always get people to trust your expertise, I guess. There are some people. . . they trust their opinion over the expertise in the field. What are you gonna do about that?” (P19)
This was evidenced in P21’s opinion that a UX expert was not needed on their team:
“We didn’t need any expert to know that. . . that this type of. . . of usability. . . is needed, actually we don’t put usability on the. . . on standards. Usability has to be like this or we can identify as that. . . we don’t know this type. . . or this. . . we don’t take it from the academic view. We. . . do deal with usability as a sense. I don’t know, as a fifth, fourth, seventh sense, I don’t know.” (P21)
Similar to other research , , , we found that participants (P1, P6, P21, P22, P24) believed in the trade-off myth for usable security. Around half of our participants stated that usability and security have to be weighed against each other: “I think the gain of security is worth the slight dip in usability.” (P24) or “I mean if you want to create something easy to use, you must pay for it at some point. [. . .] it puts you on a risk of being vulnerable on the security level” (P21).

Usable security research has provided ample evidence that and how security systems that are difficult to use end up being circumvented or compromised by users. Our results indicate that developers do not yet understand this. P22, P4, and P7 told us that they consciously traded security for more usability: “In order to make the application more usable, we had to remove all of that. . . all of that security-related code [. . .]” (P4).

In summary, we found fundamental misconceptions about usable security, not least a complete lack of awareness that being usable is a necessary condition for a security mechanism to be effective.

# 3) Communication Barriers:
Implementing usable security requires knowledge and skills in both security and usability. Individuals rarely have knowledge and skills in both, so development organizations have to take steps to ensure a development project has access to those skills. We found several contexts in which expertise was available but not accessible when it was needed. Incorporating other teams or experts in the SDP creates an overhead within organizations. P5 described that developers may have access to experts but do not consult them because it would cost valuable time. Instead, they would only resort to doing so in case of severe problems. This mental barrier may be heightened due to problems in understanding the respective other profession resulting from fractured knowledge. P10, P13, and P23 highlighted that especially designers lack knowledge and understanding of security concepts: “I think they really put a lot of effort into it already. But what you wonder is if the designer was even able to grasp the front-end developer” (P10). P22 surmised that designers might not be interested in technical aspects, such as security: “I didn’t meet any designers that like to be involved in technical decisions. I think they feel bored or something.” (P22).

P13 and P23 both told us about conflicts with the security team while fighting for better usability. For example, the usability expert (P13) said:
“We were constantly at odds with the [cryptography team] because [. . .] we had to basically consistently push back against that cryptography. Well, in their mind, it was against their cryptography because we had to tell them that, like, ‘Listen, and you can’t do it this way because, like, users will not accept, this is unacceptable, from the user perspective’.” (P13)
However, it is essential to combine both in the SDP to actually achieve usable security. Conflicts, therefore, have to be overcome by respecting and understanding each other.

# 4) Requirements, Guidelines, Compliance:
Usability requirements in our participants’ organizations were vague and rarely, if ever, written down. In contrast, we noticed that specific security requirements had their origin either in compliance standards or they emerged naturally, as P2, P6, P11, and P15 described: “Actually, they came pretty naturally.” (P6).