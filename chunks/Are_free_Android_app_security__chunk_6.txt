# 4 Ensuring Fairness
Consider only supported versions of Android:                   FixDroid was not evaluated on secure apps in Ghera because Android Studio version 3 was required to build the secure apps in Ghera and FixDroid was available as a plugin only to Android Studio version 2. Further, since we added benchmarks C4, I13, I15, and S4 after Ghera migrated to Android Studio version 3, FixDroid was not evaluated on these benchmarks; hence, we evaluated FixDroid on only 38 Ghera benchmarks.

Provide inputs as required by tools:                COVERT  and DIALDroid  detect vulnerabilities stemming from inter-app communications, e.g.,
10 Refer to Appendix A for a brief description of these benchmarks.

# Empirical Software Engineering (2020) 25:178–219
“?” denotes unknown information. S and D denote use of static analysis and dynamic analysis, respectively. L and R denote the tool runs locally and remotely, respectively. A and N denote academic tool and non-academic tool, respectively. H and E denote the tool uses shallow analysis and deep analysis, respectively. Empty cell denotes non-applicable cases.

1 We refer to Marvin Static Analyzer as Marvin-SA
# Empirical Software Engineering (2020) 25:178–219
collusion, compositional vulnerabilities. So, we applied each of these tools in its default configuration to 33 Ghera benchmarks that included malicious apps. For each benchmark, we provided the malicious app as input together with the benign app and the secure app.

# Consider multiple operational modes:
JAADAS  operates in two modes: fast mode in which only intra-procedural analysis is performed (JAADAS H) and full mode in which both intra- and inter-procedural analyses are performed (JAADAS E). Since the modes can be selected easily, we evaluated JAADAS in both modes.

QARK (LinkedIn 2015) can analyze the source code and the APK of an app. It decompiles the DEX bytecodes in an APK into source form. Since the structure of reverse engineered source code may differ from the original source code and we did not know if this could affect the accuracy of QARK’s verdicts, we evaluated QARK with both APKs (QARK A) and source code (QARK S).

# Consider only supported API levels:
Since the inception and evolution of tools are independent of the evolution of Android API levels, a tool may not support an API level (e.g., the API level is released after the tool was last developed/updated) and, hence, it may fail to detect vulnerabilities stemming from APIs introduced in such unsupported API levels. To control for this effect, we identified the API levels supported by tools.

Of the 14 tools, only three tools provide some information about the supported API levels (Android platform versions). Specifically, JAADAS documentation states that JAADAS will work for all Android platforms (as long as the platform version is provided as input). Amandroid was successfully used in Wei et al. (2018) to process ICC-Bench benchmarks  that target API level 25; hence, we inferred Amandroid supports API level 25 and below. DIALDroid tool repository contains versions of Android platform corresponding to API levels 3 thru 25 that are to be provided as input to the tool; hence, we inferred DIALDroid supports API levels 19 thru 25.

In the absence of such information, we conservatively assumed the tools supported API levels 19 thru 25, and this assumption is fair because 1) API level 19 and 25 were released in October 2013 and December 2016, respectively, (see API Levels column in Table 3), 2) all tools were last updated in 2013 or later (see Updated column in Table 2), and 3) every Ghera benchmark APK used in the evaluation were built to run on every API level 19 thru 25.

# Consider only applicable categories of vulnerabilities:
While each tool could be evaluated against all of 42 considered known vulnerabilities such an evaluation would be unfair as all tools are not geared to detect all kinds of vulnerabilities, e.g., cryptography related vulnerability vs. storage related vulnerability. Further, during app development, tools are often selected based on their capabilities pertaining to platform features/APIs used in the app.

To control for this effect, for each tool, we identified the categories of vulnerabilities that it was applicable to and evaluated it against only the vulnerabilities from these categories. Even within categories, we ignored vulnerabilities if a tool was definitively not designed to detect them, e.g., MalloDroid tool was evaluated only against SSL/TLS related vulnerabilities from Web category as the tool focuses on detecting SSL/TLS related vulnerabilities; see entry in Web column for MalloDroid in Table 5. For each tool, Table 4 reports the applicable Ghera benchmark categories.

# Consider the existence of vulnerabilities:
Expecting a tool to detect vulnerabilities that did not exist when the tool was developed/updated would be unfair. In terms of the purpose
# Empirical Software Engineering (2020) 25:178–219
Cumulative number of considered vulnerabilities discovered until a specific year (inclusive). Tools column is cumulative number of evaluated tools published until a specific year (inclusive). API Levels column lists the API levels released in a specific year
In 2018, the total number of tools is 13 instead of 14 as the last updated date for AppCritique tool is unknown.

Of this evaluation, this is not a concern as the evaluation is less focused on individual tools and more focused on assessing the effectiveness of the existing set of vulnerability detection tools against considered known vulnerabilities. In terms of the execution of this evaluation, this is not a concern as almost all of the considered vulnerabilities (39 out of 42) were discovered before 2016 (see Total column in Table 3) and almost all of the evaluated tools (at least 10 out of 14) were updated in or after 2016 (see # Tools column in Table 3).

# 4 Observations and Open Questions
This section lists interesting observations along with interesting open questions that were uncovered but not answered in this evaluation. While the data from this experiment supports the validity of the questions, the answers to these questions require more experimentation.

# 4 Tools Selection
Open Questions 1 & 2 Of the considered 64 solutions, 17 tools (including Amandroid) were intended to enable security analysis of Android apps; see Section 4. In other words, 26% of security analysis tools considered in this evaluation enable security analysis of Android apps. Further, we have found these tools be useful in our research workflow, e.g., Drozer (MWR Labs 2012), MobSF . Hence, we believe that studying these tools may be useful. Specifically, exploring two mutually related questions: 1) how expressive, effective, and easy-to-use are tools that enable security analysis? and 2) are Android app developers and security analysts willing to invest effort in such tools? may help both tool users and tool developers.

Observation 1 We rejected 39% of tools (9 out of 23) considered in deep selection; see Section 4. Even considering the number of instances where the evaluated tools failed to process certain benchmarks (see numbers in square brackets in Table 5), such a low rejection rate is rather impressive and suggests tool developers are putting in effort to release.

# Empirical Software Engineering (2020) 25:178–219
# Applicability of vulnerability detection tools to various benchmark categories in Ghera
“  ” denotes the tool is applicable to the vulnerability category in Ghera. Empty cell denotes non-applicable cases. “*” identifies non-academic tools
robust security analysis tools. This number can be further improved by distributing executables (where applicable), providing complete and accurate build instructions (e.g., versions of required dependencies) for local tools, providing complete and accurate information about execution environment (e.g., versions of target Android platforms), and publishing estimated turn around times for remote tools.

# Observation 2
If the sample of tools included in this evaluation is representative of the population of Android app security analysis tools, then almost every vulnerability detection tool for Android apps relies on static analysis, i.e., 13 out of 14; see S/D column in Table 2.

# Observation 3
Every vulnerability detection tool publicly discloses the category of vulnerabilities it tries to detect. Also, almost all vulnerability detection tools are available as locally executable tools, i.e., 13 out of 14; see L/R column in Table 2. So, vulnerability detection tools are open with regards to their vulnerability detection capabilities. The likely reason is to inform app developers how the security of apps improves by using a vulnerability detection tool and encourage the use of appropriate vulnerability detection tools.

# Observation 4
Ignoring tools with unknown update dates (“?” in column 3 of Table 2) and considering that we conducted the evaluation between June 2017 and May 2018, 9 out of 13 tools are less than 1 years old (2017 or later) and 12 out of 13 are less than or about 3 years old (2015 or later). Hence, the selected tools can be considered as current. Consequently, the resulting observations are highly likely to be representative of the current state of the freely available Android app security analysis tools.