LLR = − (1/N) ∑i=1N[y log(pθ(x)) − λKL(˜||ˆθ)]
The loss function complements the log-likelihood loss over the positive examples with a label regularizer, which is the KL divergence between a noise prior ˜ and the classifier’s output distribution over the negative examples ˆθ:
pθ = (1/N) ∑i=1N[(1 − y) log(1 − pθ(x))]
Intuitively, the label regularizer aims to push the classifier predictions on the noisy class towards the expected noise prior ˜, while the λ hyper-parameter controls the regularization strength. We use this loss to observe the extent to which existing noise correction approaches for related security tasks apply to our problem. However, this function was not designed to address (ii) and, as our results will reveal, yields poor performance in our problem.

FC: The Forward Correction loss has been shown to significantly improve robustness to class-dependent label noise in various computer vision tasks . The loss requires a pre-defined noise transition matrix T ∈ 2x2, where each element represents the probability of observing a noisy label ˜j for a true label y: Tij = p(˜ |y). For an instance x, the log-likelihood is then defined as:
lc(x) = −log(T0c(1 − pθ(x)) + T1c pθ(x))
In our case, because we assume that the probability of falsely labeling non-exploited vulnerabilities as exploited is negligible, the noise matrix can be defined as:
T = (1 0)
p 1 − ˜p
and the loss reduces to:
LFC = − (1/N) ∑i=1N[y log((1 − ˜)pθ(x)) + (1 − y) log(1 − (1 − ˜)pθ(x))]
On the negative class, the loss reduces the penalty for confident positive predictions, allowing the classifier to output a higher score for predictions which might have noisy labels.

Classifier deployment. We evaluate the historic performance of our classifier by partitioning the dataset into temporal splits, assuming that the classifier is re-trained periodically, on all the historical data available at that time. At the time the classifier is trained, we do not include the vulnerabilities disclosed within the last year because the positive labels from exploitation evidence might not be available until later on. We discovered that the classifier needs to be retrained every six months, as less frequent retraining would affect performance due to a larger time delay between the disclosure of training and testing instances. During testing, the system operates in a streaming environment in which it continuously collects the data published about vulnerabilities then recomputes their feature vectors over time and predicts their updated EE score.

The prediction for each test-time instance is performed with the most recently trained classifier. To observe how our classifier performs over time, we train the classifier using the various loss functions and test its performance on all vulnerabilities disclosed between January 2010, when 65% of our...

# Feature
All techniques require priors about the probability of noise. The LR and FC approaches require a prior ˜ over the entire negative class. To evaluate an upper bound of their capabilities, we assume perfect prior and set ˜ to match the fraction of training-time instances that are mislabeled. The FFC approach assumes knowledge of the noisy feature f. This assumption is realistic, as it is often possible to enumerate the features that are most likely noisy (e.g. prior work identified linux as a noise-inducing feature due to the fact that the vendor collecting exploit evidence does not have a product for the platform ). Besides, FFC requires estimates of the feature-specific priors ˜. We assume an operational scenario where ˜ f is estimated once, by manually labeling a subset of instances collected after training. We use the vulnerabilities disclosed in the first 6 months after training for estimating ˜ f and require that these vulnerabilities are correctly labeled.

We report the precision at a 0 recall (P) and the precision-recall AUC. The pristine BCE classifier performance is 0 and 0 respectively.

# 7 Evaluation
We evaluate our approach of predicting expected exploitability by testing EE on real-world vulnerabilities and answering the following questions, which are designed to address the third challenge identified in Section 3: How effective is EE at addressing label noise? How well does EE perform compared to baselines? How well do various artifacts predict exploitability? How does EE performance evolve over time? Can EE anticipate imminent exploits? Does EE have practicality for vulnerability prioritization?
# 7 Feature-dependent Noise Remediation
To observe the potential effect of feature-dependent label noise on our classifier, we simulate a worst-case scenario in which our training-time ground truth is missing all the exploits for certain features. The simulation involves training the classifier on dataset DS2, on a ground truth where all the vulnerabilities with a specific feature f are considered not exploited. At testing time, we evaluate the classifier on the original ground truth labels. Table 4 describes the setup for our experiments. We investigate 8 vulnerability features, part of which are CWE-79, CWE-94, CWE-89, CWE-119, CWE-20, CWE-22, Windows, and Linux.

On features where BCE is not substantially affected by noise, we observe that FC performs similarly well. However, for LR, after performing a grid search for the optimal λ parameter which we set to 1, we were unable to match the BCE performance on the pristine classifier. Indeed, we observe that the loss is unable to correct the effect of noise on any of the features, suggesting that it is not a suitable choice for our classifier as it does not address any of the two requirements of our classifier.

# Precision
We observe that none of the static exploitability metrics exceed 0 precision, while EE significantly outperforms all the baselines. The performance gap is especially apparent for the 60% of exploited vulnerabilities, where EE achieves 86% precision, whereas the SMC, the second-best performing classifier, obtains only 49%. We also observe that for around 10% of vulnerabilities, the artifacts available within 30 days have limited predictive utility, which affects the performance of these classifiers.

EE uses the most informative features. To understand why EE is able to outperform these baselines, in Figure 5b we plot the performance of EE trained and evaluated on individual categories of features (i.e. only considering instances which have artifacts within these categories). We observe that the handcrafted features are the worst performing category, perhaps due to the fact that the 53 features are not sufficient to capture the large diversity of vulnerabilities in our dataset. These features encode the existence of public PoCs, which is often used by practitioners as a heuristic rule for determining which vulnerabilities must be patched urgently. Our results suggest that this heuristic provides a weak signal for the emergence of functional exploits, in line with prior work predicting exploits , which concluded that PoCs "are not a reliable source of information for exploits in the wild" . Nevertheless, we can achieve a much higher precision at predicting exploitability by extracting deeper features from the PoCs. The PoC Code features provide a 0 precision for half of the exploited vulnerabilities, outperforming all other categories. This suggests that code complexity can be a good indicator for the likelihood of functional exploits, although not on all instances, as indicated by the sharp drop in precision beyond the 0 recall. A major reason for this drop is the existence of post-exploit mitigation techniques: even if a PoC is complex and contains advanced functionality, defenses might impede successful exploitation beyond denial of service. This highlights how our feature extractor is able to represent PoC descriptions and code characteristics which reflect exploitation efforts. Both the PoC and Write-up features, which EE capitalizes on, perform significantly better than other categories.

# 7 Effectiveness of Exploitability Prediction
Next, we evaluate the effectiveness of our system compared to the three static metrics described in Section 5, as well as two state-of-the-art classifiers from prior work. These two predictors, EPSS , and the Social Media Classifier (SMC) , were proposed for exploits in the wild and we re-implement and re-train them for our task. EPSS trains an ElasticNet regression model on the set of 53 hand-crafted features extracted from vulnerability descriptors. SMC combines the social media features with vulnerability information features from NVD to learn a linear SVM classifier. For both baselines, we perform hyper-parameter tuning and report the highest performance across all experiments, obtained using λ = 0 for EPSS and C = 0 for SMC. SMC is trained starting from 2015, as our tweets collection does not begin earlier.

In Figure 5a we plot the precision-recall trade-off of the classifiers trained on dataset DS1, evaluated 30 days after the disclosure of test-time instances. EE improves when combining artifacts. Next we look at the interaction among features on dataset DS2. In Figure 6a.