# Assessing the Effectiveness of LLMs in Android Application Vulnerability Analysis
Vasileios Kouliaridis1, Georgios Karopoulos1, and Georgios Kambourakis2
1European Commission, Joint Research Centre (JRC) 21027 Ispra, Italy
2Department of Information and Communication Systems Engineering, University of the Aegean, Karlovasi, 83200, Samos, Greece
# Abstract
The increasing frequency of attacks on Android applications coupled with the recent popularity of large language models (LLMs) necessitates a comprehensive understanding of the capabilities of the latter in identifying potential vulnerabilities, which is key to mitigate the overall risk. To this end, the work at hand compares the ability of nine state-of-the-art LLMs to detect Android code vulnerabilities listed in the latest Open Worldwide Application Security Project (OWASP) Mobile Top 10. Each LLM was evaluated against an open dataset of over 100 vulnerable code samples, including obfuscated ones, assessing each model’s ability to identify key vulnerabilities. Our analysis reveals the strengths and weaknesses of each LLM, identifying important factors that contribute to their performance. Additionally, we offer insights into context augmentation with retrieval-augmented generation (RAG) for detecting Android code vulnerabilities, which in turn may propel secure application development. Finally, while the reported findings regarding code vulnerability analysis show promise, they also reveal significant discrepancies among the different LLMs.

# Keywords
Large Language Models, Vulnerability analysis, Code analysis, OWASP, Mobile security, Android, Retrieval-Augmented Generation.

# 1 Introduction
As mobile devices continue to proliferate, the need for secure software development practices remains still of high priority. The predominant Android platform has become a prime target for attackers and malware writers, seeking to exploit vulnerabilities in the vast cosmos of mobile applications . The importance and volume of mobile vulnerabilities has led the Open Web Application Security Project (OWASP) to periodically publish a current, reputable list of the most prevalent vulnerabilities detected in mobile applications, namely OWASP Mobile Top 10 . This list can serve as a key benchmark in assessing the performance of any tool in finding software vulnerabilities.

An emerging approach to detecting Android code vulnerabilities is the use of large language models (LLMs) for code analysis. Actually, the use of LLMs for
# Vasileios Kouliaridis, Georgios Karopoulos, and Georgios Kambourakis
Code analysis is traced back to the early 2010s. That is, in 2013, the introduction of Word2Vec , a shallow neural network, marked the beginning of deep learning-based language models. That algorithm was capable of learning word embeddings (an encoding of the meaning of the word) from large datasets. In 2018, Google introduced Word2Vec’s successor, a language model known as Bidirectional Encoder Representations from Transformers (BERT) . BERT was designed to be bidirectionally trained, meaning it can learn information from both the left and right sides of a given text during training, therefore obtaining a better understanding of the context.

In the realm of code analysis, LLMs began to gain traction around 2017. One of the early applications of LLMs in code analysis was code completion. Models like GPT-2 , fully released in Nov. 2019, were trained on a large corpus of source code data. By understanding the structure and context of the code, these models could predict the most likely code to follow a given input. In 2020, OpenAI  introduced GPT-3 , a significantly larger model with 175B parameters. This model showed improved capabilities in generating human-like text and was even able to generate code when given a task description. The ability of LLMs to analyze and understand code has also been demonstrated in recent studies . Nevertheless, to the best of our knowledge, the literature lacks a comprehensive comparison of the ability of these models to detect Android code vulnerabilities so far.

The present work aims to fill this gap by comparing the ability of nine state-of-the-art LLMs to detect Android code vulnerabilities listed in the OWASP Mobile Top 10. Specifically, each model is evaluated regarding its performance in identifying key vulnerabilities against a dataset comprising snippets of vulnerable Android code. The assessment of each model is done through a combination of manual and automated evaluation methods. We additionally pinpoint the strengths and weaknesses of each LLM and provide insights into the factors that conduce to their performance. Overall, this study provides valuable insights into the use of LLMs for detecting mobile code vulnerabilities, thus contributing to the development of effective methods for secure mobile coding. The contributions of the paper are summarized as follows.

- We present a thorough comparative analysis on the capabilities and performance of nine leading LLMs, i.e., GPT 3, GPT 4, GPT 4 Turbo, Llama 2, Zephyr Alpha, Zephyr Beta, Nous Hermes Mixtral, MistralOrca, and Code Llama in identifying vulnerabilities residing in Android applications. The experiments conducted provide concrete evidence of the LLMs’ capabilities for such tasks, also identifying the limitations per LLM. These insights are critical for anyone interested in understanding the trade-offs associated with each LLM.

- We provide a comparison between the code analysis results as given by the nine LLMs against two well-known, publicly available static application security testing (SAST) tools, namely, Bearer  and MobSFscan.

# Title Suppressed Due to Excessive Length
We examine the impact of context augmentation on LLMs and contribute a set of guidelines regarding the selection and fine-tuning of LLMs towards enhancing the security posture of Android code.

We offer an open dataset to the community for driving research in this field forward.

The remainder of this paper is structured as follows. The next section presents previous work on the use of LLM for code vulnerability analysis. Section 3 details our methodology, while the results per LLM are given in section 4. The last section concludes and proposes some lines for future research.

# 2 Previous work
In recent years, LLMs have gained significant attention in the field of cyber-security for their potential to provide assistance in various domains, including vulnerability detection, penetration testing, and security analysis. State-of-the-art surveys such as  and , as well as a more recent but not yet peer-reviewed study , provide comprehensive overviews of the current state and potential future applications of LLMs in cybersecurity. These works analyze the challenges, practical implications, and future research directions to exploit the full potential of these models in ensuring cyber resilience. The rest of this section will focus on literature dealing with software vulnerability analysis using LLMs. This includes works that have already been peer-reviewed, as well as more recent research that has been self-archived for the sake of completeness.

In , transformer-based LLMs are evaluated in the task of code vulnerability detection. The authors evaluate such LLMs, including BERT, DistilBERT, CodeBERT, GPT-2 and Megatron, against C/C++ source code snippets from two publicly available datasets. The results showed that LLMs perform well in software vulnerability tasks; indicatively, the best scoring model, GPT-2, had an F1-score above 95% in all tests. In the context of software engineering,  investigates the use of in-context learning to improve the ability of LLMs to detect software vulnerabilities, showcasing the adaptability of LLMs to learn from context-specific examples. The authors use code retrieval to search for code snippets that are similar to the examined code and feed them to the LLM together with the examined code and its analysis. Their experimental results show that this approach has better performance than the original GPT model.

Another set of works adds verification in the vulnerability detection process. An empirical study of using LLMs for vulnerability assessment in software was conducted in . The authors used four well-known pre-trained LLMs to identify vulnerabilities in two labeled datasets, namely code gadgets and CVE-fixes, and static analysis as a reference point. The used LLMs include GPT-3, Davinci and CodeGen, and the analysis was limited to two kinds of vulnerabilities: SQL injections and buffer overflows. The study concluded that LLMs do not perform well at detecting vulnerabilities, presenting high false-positive rates, but could complement and improve the traditional static analysis process. Concerns
# 4
Vasileios Kouliaridis, Georgios Karopoulos, and Georgios Kambourakis
About the safe use of code assistants are addressed in . In this case, LLMs are used to produce code which is then assessed manually and using static analysis. This study provides empirical insights into how developers interact with LLMs, underscoring the importance of user awareness to mitigate security risks associated with assisted code generation.

Moving to non peer-reviewed works, the work of  delves into the application of LLMs in static binary taint analysis, demonstrating how these models can assist in vulnerability inspection of binaries. A binary is first disassembled and decompiled, and an LLM is used to identify security sensitive functions that may contain vulnerabilities, as well as candidate dangerous flows. In the last phase, the LLM combines the previous results to produce a vulnerability report for the examined binary. The authors of  propose DefectHunter, a vulnerability detection mechanism that combines various technologies, including LLMs. Its architecture has three main building blocks: a tool for extracting structural information from code snippets, a pre-trained LLM for generating semantic information, and a Conformer mechanism to identify vulnerabilities from the previously extracted structural and semantic data.