Provide inputs as required by tools: COVERT [Bagheri et al., 2015] and DIALDroid [Bosu et al., 2017] detect vulnerabilities stemming from inter-app communications, e.g., collusion, compositional vulnerabilities. So, we applied each of these tools in its default configuration to 33 Ghera benchmarks that included malicious apps. For each benchmark, we provided the malicious app as input together with the benign app and the secure app.

Consider multiple operational modes: JAADAS [Keen, 2016] operates in two modes: fast mode in which only intra-procedural analysis is performed (JAADASH) and full mode in which both intra- and inter-procedural analyses are performed (JAADASE). Since the modes can be selected easily, we evaluated JAADAS in both modes.

10Refer to Section A for a brief description of these benchmarks.

aWe refer to Marvin Static Analyzer as Marvin-SA.

# Cumulative # of Considered Vulnerabilities
QARK [LinkedIn, 2015] can analyze the source code and the APK of an app. It decompiles the DEX bytecodes in an APK into source form. Since the structure of reverse engineered source code may differ from the original source code and we did not know if this could affect the accuracy of QARK’s verdicts, we evaluated QARK with both APKs (QARKA) and source code (QARKS).

Consider only supported API levels: Since the inception and evolution of tools are independent of the evolution of Android API levels, a tool may not support an API level (e.g., the API level is released after the tool was last developed/updated) and, hence, it may fail to detect vulnerabilities stemming from APIs introduced in such unsupported API levels. To control for this effect, we identified the API levels supported by tools.

Of the 14 tools, only three tools provide some information about the supported API levels (Android platform versions). Specifically, JAADAS documentation states that JAADAS will work for all Android platforms (as long as the platform version is provided as input). Amandroid was successfully used in [Wei et al., 2018] to process ICC-Bench benchmarks [Wei, 2017] that target API level 25; hence, we inferred Amandroid supports API level 25 and below. DIALDroid tool repository contains versions of Android platform corresponding to API levels 3 thru 25 that are to be provided as input to the tool; hence, we inferred DIALDroid supports API levels 19 thru 25.

In the absence of such information, we conservatively assumed the tools supported API levels 19 thru 25, and this assumption is fair because 1) API level 19 and 25 were released in October 2013 and December 2016, respectively, (see API Levels column in Table 3), 2) all tools were last updated in 2013 or later (see Updated column in Table 2), and 3) every Ghera benchmark APK used in the evaluation were built to run on every API level 19 thru 25.

Consider only applicable categories of vulnerabilities: While each tool could be evaluated against all of 42 considered known vulnerabilities such an evaluation would be unfair as all tools are not geared to detect all kinds of vulnerabilities, e.g., cryptography related vulnerability vs. storage related vulnerability. Further, during app development, tools are often selected based on their capabilities pertaining to platform features/APIs used in the app.

To control for this effect, for each tool, we identified the categories of vulnerabilities that it was applicable to and evaluated it against only the vulnerabilities from these categories. Even within categories, we ignored vulnerabilities if a tool was definitive not designed to detect them, e.g., MalloDroid tool was evaluated only against SSL/TLS related vulnerabilities from Web category as the tool focuses on detecting SSL/TLS related vulnerabilities; see entry in Web column for MalloDroid in Table 5. For each tool, Table 4 reports the applicable Ghera benchmark categories.

11In 2018, the total number of tools is 13 instead of 14 as the last updated date for AppCritique tool is unknown.

# API (Vulnerability) Categories
Consider the existence of vulnerabilities: Expecting a tool to detect vulnerabilities that did not exist when the tool was developed/updated would be unfair. In terms of the purpose of this evaluation, this is not a concern as the evaluation is less focused on individual tools and more focused on assessing the effectiveness of the existing set of vulnerability detection tools against considered known vulnerabilities. In terms of the execution of this evaluation, this is not a concern as almost all of the considered vulnerabilities (39 out of 42) were discovered before 2016 (see Total column in Table 3) and almost all of the evaluated tools (at least 10 out of 14) were updated in or after 2016 (see # Tools column in Table 3).

# 4 Observations and Open Questions
This section lists interesting observations along with interesting open questions that were uncovered but not answered in this evaluation. While the data from this experiment supports the validity of the questions, the answers to these questions require more experimentation.

# 4 Tools Selection
Open Questions 1 & 2 Of the considered 64 solutions, 17 tools (including Amandroid) were intended to enable security analysis of Android apps; see Section 4. In other words, 26% of security analysis tools considered in this evaluation enable security analysis of Android apps. Further, we have found these tools be useful in our research workflow, e.g., Drozer [MWR Labs, 2012], MobSF [Abraham et al., 2015]. Hence, we believe that studying these tools may be useful. Specifically, exploring two mutually related questions: 1) how expressive, effective, and easy-to-use are tools that enable security analysis? and 2) are Android app developers and security analysts willing to invest effort in such tools? may help both tool users and tool developers.

Observation 1 We rejected 39% of tools (9 out of 23) considered in deep selection; see Section 4. Even considering the number of instances where the evaluated tools failed to process certain benchmarks (see numbers in square brackets in Table 5), such a low rejection rate is rather impressive and suggests tool developers are putting in effort to release robust security analysis tools. This number can be further improved by distributing executables (where applicable), providing complete and accurate build instructions (e.g., versions of required dependencies) for local tools, providing complete and accurate information about
# 4 Vulnerability Detection Tools
Every Ghera benchmark is associated with exactly one unique vulnerability v, and its benign app exhibits v while its secure app does not exhibit v. So, for a tool, for each applicable benchmark, we classified the tool’s verdict for the benign app as either true positive (i.e., v was detected in the benign app) or false negative (i.e., v was not detected in the benign app). We classified the tool’s verdict for the secure app as either true negative (i.e., v was not detected in a secure app) or false positive (i.e., v was detected in a secure app). Columns TP, FN, and TN in Table 5 report true positives, false negatives, and true negatives, respectively. False positives are not reported in the table as none of the tools except DevKnox (observe the D for DevKnox under System benchmarks in Table 5) and data leakage extension of Amandroid (observe the {1} for Amandroid1 under Storage benchmarks in Table 5) provided false positive verdicts. Reported verdicts do not include cases in which a tool failed to process apps.

# Observations
Observation 2: If the sample of tools included in this evaluation is representative of the population of Android app security analysis tools, then almost every vulnerability detection tool for Android apps relies on static analysis, i.e., 13 out of 14; see S/D column in Table 2.

Observation 3: Every vulnerability detection tool publicly discloses the category of vulnerabilities it tries to detect. Also, almost all vulnerability detection tools are available as locally executable tools, i.e., 13 out of 14; see L/R column in Table 2. So, vulnerability detection tools are open with regards to their vulnerability detection capabilities. The likely reason is to inform app developers how the security of apps improves by using a vulnerability detection tool and encourage the use of appropriate vulnerability detection tools.

Observation 4: Ignoring tools with unknown update dates (“?” in column 3 of Table 2) and considering that we conducted the evaluation between June 2017 and May 2018, 9 out of 13 tools are less than 1 years old (2017 or later) and 12 out of 13 are less than or about 3 years old (2015 or later). Hence, the selected tools can be considered as current. Consequently, the resulting observations are highly likely to be representative of the current state of the freely available Android app security analysis tools.

Observation 5: Most of the tools (10 out of 14) were applicable to every Ghera benchmark; see # Applicable Benchmarks column in Table 5. Except for MalloDroid, the rest of the tools were applicable to 24 or more Ghera benchmarks. This observation is also true of Amandroid if the results of its pre-packaged extensions are considered together.