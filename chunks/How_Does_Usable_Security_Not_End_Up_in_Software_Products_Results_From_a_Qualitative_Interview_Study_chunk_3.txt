# Qualitative Coding
We used both inductive and deductive approaches for data analysis: The first coder developed a code list based on the phases of software engineering as described by Pressman and Maxim , our research questions, the interview guide, and insights from the interviews. The codes were then iteratively refined over the process of coding nine transcripts independently with two authors, then discussing and adapting the codebook until all authors agreed on the codebook. All codes were formally defined (“operationalized”) to ensure agreement by all coders. Two authors then coded all transcripts, re-coding those used for codebook creation, and merged their codes for analysis. The final codebook (cf. Table IV) addressed concepts like usability, security, usable security, concepts related to mental models (such as “misconceptions”), factors that influence the software development process (such as budget and resources), as well as the software development process itself. Throughout the coding process, we explored the relationship between the codes as well as patterns and themes. The authors regularly met and discussed the emerging concepts, which allowed the focus of this paper to develop: how software development processes influence usable security in software products.

# F. Ethics and Data Protection
Neither of our institutions had an institutional review board (IRB) nor an ethics review board (ERB) that applied to our study. We adhered to the strict German and EU privacy laws, and the main author’s institution’s data protection officer reviewed and approved our study and our data handling process. Our consent form is compliant with the European General Data Protection Regulation (GDPR) and covered all information usually required for US IRB approval. Participants were informed that they could terminate the study at any time. We clarified all of their questions regarding the consent form, data processing and storage, and privacy ahead of the interview. All participants consented to our data handling, collection, usage practice, and publication strategy. We clarified that we would only evaluate de-identified data, publish aggregate data and de-identified quotes. We deleted all video and voice recordings post transcription to minimize unnecessary storage of PII, including information that was accidentally revealed in the interviews, and de-identified during the transcription process. To further prevent re-identification, we report results linked to development context independently from participant IDs. A more detailed discussion of our measures to ensure protection of participants’ data can be found in Appendix A.

# G. Limitations
This paper describes results from a qualitative study with a purposive sample. Therefore, all counts reported in our results are used to convey weight but should not be taken as quantitative results, statistics, or as claims to generalizability. Our goal was to understand a wide range of processes and conceptions about usable security. We are confident that we reached theoretical saturation in participants’ answers for our purposive sampling strategy. Due to our advertising and the questions we asked, participants likely knew that we were interested in usability, security, and usable security – which could raise concerns about priming. However, most participants reported a lack of processes for usability, security, and usable security, as well as disinterest in the topic in their professional experience, so the concern is not warranted. This qualitative study aimed to explore current practices surrounding usable security and if/how it is supported. Possible follow-up work can build on this with quantitative methods and the goal of generalizability to verify our results. Additionally, while we aimed for a diverse sample, our sample skewed Western and male, aligning with human factors research studies with software professionals. While individual developers might tend to provide their personal opinions, our interviews prompted participants to report specific organizational experiences and context information. Individual recollections are, of course, subject to self-censorship and biases , most importantly self-reporting and social desirability bias, e. g., interviewees claiming that usability is important to them when it is actually not. However, for many aspects like usability, security, and others, our in-depth interviews and the fine-grained process explanations allow us to assess actual procedural focus on specific topics, as opposed to individual assessment of importance.

# IV. RESULTS
In this section, we present the results and themes that emerged from the analysis of the 25 semi-structured interviews. We first provide an overview of development contexts, i. e., companies and projects (for a detailed breakdown, see Table II). We assessed each company regarding usable security, identifying four different categories (cf. Figure 2): (1) Usable Security Awareness and Follow-through, (2) Usable Security Awareness, Little or no Follow-through, (3) No Usable Security Awareness, some User-centered Measures, and (4) No Usable Security Awareness, few or no User-centered Measures. We present the findings in each category within the four sections starting at Section IV-B. In this and the following sections, we report counts of interviews. However, as we conducted qualitative analysis, the counts should only give weight to the themes we found and therefore should not be interpreted as a quantitative result.

# A. Software Development Contexts
Companies and Projects. Here we report the contexts, i. e., the companies, organizations, and projects that participants related to in the interviews. 21 of our 25 interviews covered at least one company context in detail, resulting in a total of 23 contexts (see Table II). The companies sizes’ ranged from five companies being very small (1–9 employees), eight small (10–99), five medium sized (100–999), two large (1000–5000), to three companies being very large (> 5000). In the remaining four interviews, participants shared their professional experiences and impressions across several projects and organizations, rather than focusing on a single context or company. While this did not allow us to investigate specific
# TABLE II
# SUMMARY OF THE CONTEXTS IN OUR INTERVIEWS.

User-centered approach We also found that in all contexts of this group, at least contexts in-depth, these experienced participants (mean industry experience: 13 years) and how they compared different contexts they encountered in their professional career provided a cross-cutting perspective that helped comparing different contexts described in the other interviews.

Usable Security Status. As a first step, we analyzed all 23 company contexts regarding their focus on and their awareness of usable security. If participants were aware of usable security and reported its relevance for the product or the company, we classified those contexts as being aware of usable security. We made this assessment based on the parts of the interviews we had coded Interplay Usability & Security, Product, Importance, and Responsibility. Contexts were also classified in this category when usable security was not directly mentioned, but the underlying concept was understood. We were able to reconstruct each company’s SDP from the codes Communication & Modeling, Construction, Deployment, Staff & Team, Budget & Resources. We analyzed whether user-centered development measures were taken, whether (1) participants mentioned systematic usability testing, and whether (2) they conducted user research, or performed strong and active engagement with their user community. We found various forms of empirical user testing, expert evaluations, or field observations. To assess whether the reported development process was user-centered, we relied on the key principles for user-centered system design by Gulliksen et al. . Based on the classification along those two axes, we obtained the four categories shown in Figure 2. We provide an in-depth description of the categories and their characteristics below.