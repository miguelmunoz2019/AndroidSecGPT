# 1) Potential TPL Identification
Since there are over 3 million TPL files to be compared in our database for each candidate library, to speed up the entire detection process, we search the database in the following order: a) Search by package names. For each library candidate, we first use its package namespace (if not obfuscated) to narrow down the search space in our database. Note that we cannot directly use the package name to determine a TPL, because the same package namespace could include different third-party libraries. For example, the Android support group  includes 99 different TPLs. These TPLs have the same group ID “com.android.support” and the same package name prefix “android/support/”. If the package name has been obfuscated or a candidate TPL module is without a package name, we move to the next filtering strategy. Note that, even though it is a non-trivial problem to decide the obfuscated package name, in our work, the package name is only used as supplementary information to speed up the search process. No matter whether a candidate TPL can find a match in the TPL database by using the package names, we still continue to search the TPL database via other features. Thus, we only applied a simple rule to identify the obfuscated apps: if a package name is a hash value or a single letter, we consider it obfuscated, b) Search by the number of classes. We assume two TPLs are unlikely to be the same one if the number of classes within two TPLs has a big difference . If the number of the classes in a TPL only accounts for less than 40% of that in another TPL in the database, we will not further compare them, which can help us speed up the identification process, c) Search by coarse-grained features. To speed up, we first search the coarse-grained feature T1 in the TPL database; if we find the same one, ATVHunter will report this TPL and stop the search process. Otherwise, ATVHunter will compare the candidate TPL with TPLs in the database, if all the coarse features are the same, we consider find the TPL and the search process will stop. If over 70% of the coarse-grained features are the same (followed by previous research ), we consider it as a potential TPL. When we find the potential TPL, we will identify the exact version.

# 2) Version Identification
To identify the specific versions of the used TPLs, we utilize the fine-grained features and calculate the similarity ratio of two TPLs as the evaluation metric. To ensure the efficiency, we do not compare these matched methods in previous stage. ATVHunter can record the same method pair in the previous stage, therefore, we only need to compare less than 30% of the methods in this phase. Since some code obfuscation techniques (e.g., junk code insertion) would change the fingerprints of methods, causing two methods that were initially the same to be different. Therefore, we need to compare the method similarity and consider two methods matched only when their method similarity exceeds a threshold. Based on the number of matched methods, we then compute the TPL similarity. When the number of matched methods exceeds the threshold, we consider we find the correct TPL with its version.

# Method Similarity Comparison
We employ edit distance  to measure the similarity between two method fingerprints. The edit distance of two fingerprints is defined as the number of minimum edit operations (i.e., insertion, deletion, substitution).

# IV. EVALUATION
# A. Vulnerable TPL-V Identification
We first build a vulnerable TPL-V database, based on which we identify the vulnerable TPL-Vs used by the apps.

# 1) Database Construction:
The vulnerable TPL-V database construction process includes collection of known vulnerabilities in Android TPLs and security bugs from open-source software.

Known TPL Vulnerability Collection. To collect the vulnerable TPL versions, we convert the names of all TPL files (3,006,676 in total) in our feature database into Common Platform Enumeration (CPE) format  and exploit cve-cve search , a professional CVE search tool, to query the vulnerable TPLs from the public CVE (Common Vulnerabilities and Exposures) database by mapping the transformed TPL names. In this way, we can get the known vulnerabilities id, vulnerability type, description, severity score from Common Vulnerability Scoring System (CVSS) , vulnerable versions, etc. We use CVSS v3 to indicate the severity of the collected vulnerabilities in this paper. Finally, we collected 1,180 CVEs from 957 unique TPLs with 38,243 affected versions.

# B. Preparation
Ground-truth Dataset Construction. We build this dataset for three primary purposes: 1) verify the effectiveness of ATVHUNTER; 2) compare the performance with the state-of-the-art tools; 3) release the datasets to the community to promote follow-up research. Since it is difficult to know the specific TPL-Vs from commercial apps, we choose the open-source apps to compare ATVHUNTER with existing tools.

We first collect the latest versions of 500 open-source apps from F-Droid  that is the largest repository maintaining open-source Android apps.

# C. Implementation
ATVHUNTER is implemented in 2k+ lines of python code. We employ APKTOOL , a reverse engineering tool commonly used by much previous work  to decompile the Android apps and exploit Androguard  to obtain the class dependency relations in order to get the independent TPL candidates. We then employ SOOT  to generate CFG and also build on SOOT to get the opcode sequence in each basic block of a CFG. We use the ssdeep  fuzzy hash algorithm to generate the code feature and employ the edit distance  algorithm to find the in-app TPLs. Our approach can pinpoint the specific TPL versions. We maintain a library database containing more than 3 million TPL files and construct a vulnerable TPL database that includes 224 security bugs from open-source Java software on Github, and 1,180 CVEs from 910 Android TPLs in public CVE databases.

# D. Experimental Results
If MSS exceeds a certain threshold 0, we consider the two methods are matched. Based on our experimental result in § IV-A, we choose 0 = 0 as the threshold. Based on the number of matched methods, the similarity of two TPLs (t1 and t2) are defined as follows: TSS(t1,t2) = Min(M|t1|, M|t2|) / Max(M|t1|, M|t2|) (2) where t1 is a TPL candidate from the test app, t2 is a TPL from the database for comparison: M|t1| is the number of methods in t1. M|t2| is the number of matched methods of t1 and t2.

# 4%
X0"" s0* 4u" 20%0 30* 2070 1% 10%
# similarity threshold
# Method-level
# TPL-level
subjects since we can get the specific TPL information (in- cluding the version) in the configuration files and source code of apps, such a mapping relation between apps and TPLs is used as the ground-truth for performance evaluation. These apps are from 17 different categories with various sizes. For each app, we manually analyze it and get the in-app TPLs with their specific versions. According to our analysis, these apps contain the number of TPLs ranging from 2 to 37 and these TPLs also have different functions with diverse sizes. We then download these TPLs with their versions from the Maven repository . To ensure the evaluation results more reliable, we collect the complete versions of each TPL. We filter 144 apps out due to the incomplete versions of TPLs maintained in the Maven repository. Note that, based on our analysis, we find the previous published datasets have some biases. TPLs from LibScout and LibID are most independent ones, thus, we add some TPLs that depend on other TPLs in our dataset, such as “Retrofit” depends on “Guava”, to reveal the lib identification capability of different tools. Finally, we choose 356 apps and 189 unique TPLs with the complete 6,819 version files in these apps as the ground truth.

# Threshold Selection
To avoid bias, we randomly select three groups (3 x 200) of apps except the aforementioned dataset to decide appropriate thresholds for method similarity score 0, and TPL similarity score 8. We use method-level false positive rate (FPR) and false negative rate (FNR); and TPL-level FPR and FNR as the metrics to decide the similarity thresholds by varying 6 and 8 from different thresholds. We employ the three groups of apps to implement the same experiment three times and then decide the optimal thresholds.